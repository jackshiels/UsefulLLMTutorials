{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_rq8MP7-rhsv"
      ],
      "authorship_tag": "ABX9TyNy/72Q+YkP5MvTDUYqBoIK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackshiels/UsefulLLMTutorials/blob/main/1_EncoderDecoderModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder-Decoder Models\n",
        "\n",
        "The following tutorial takes a look at basic encoder-decoder and generates a model for machine translation. The recommended reading for this tutorial is Chapter 2 of Large Language Models: A Deep Dive. You can find it here for under $15: [purchase](https://link.springer.com/book/10.1007/978-3-031-65647-7)\n",
        "\n",
        "We will be implementing the torch Gated Recurrent Unit (GRU), which is a choice against using the traditional Long Short-Term Memory (LSTM) model. By the end of the softmax layer, we are implementing greedy searching for tokens. An alternative attention approach is provided, too."
      ],
      "metadata": {
        "id": "0neRFBDbmIZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "# Seed torch and random\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device {device}')"
      ],
      "metadata": {
        "id": "cALWYbNEFjVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "001f6ba5-254a-4d1e-c96e-cfa2f2e9f820"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Toy Dataset"
      ],
      "metadata": {
        "id": "_rq8MP7-rhsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_LANGUAGE = 'en'\n",
        "TGT_LANGUAGE = 'fr'\n",
        "\n",
        "raw_data_pairs = [\n",
        "    (\"hello world\", \"bonjour le monde\"),\n",
        "    (\"how are you\", \"comment allez vous\"),\n",
        "    (\"i am fine\", \"je vais bien\"),\n",
        "    (\"good morning\", \"bonjour\"),\n",
        "    (\"thank you\", \"merci\"),\n",
        "    (\"i love pytorch\", \"j aime pytorch\"),\n",
        "    (\"machine translation is cool\", \"la traduction automatique est cool\"),\n",
        "    (\"see you later\", \"a plus tard\"),\n",
        "    (\"what is your name\", \"quel est votre nom\"),\n",
        "    (\"my name is model\", \"mon nom est modele\"),\n",
        "    (\"nice to meet you\", \"ravi de vous rencontrer\"),\n",
        "    (\"good night\", \"bonne nuit\"),\n",
        "    (\"have a nice day\", \"bonne journee\"),\n",
        "    (\"where are you from\", \"d ou venez vous\"),\n",
        "    (\"i am from canada\", \"je viens du canada\"),\n",
        "    (\"do you speak english\", \"parlez vous anglais\"),\n",
        "    (\"yes i speak english\", \"oui je parle anglais\"),\n",
        "    (\"no i don't understand\", \"non je ne comprends pas\"),\n",
        "    (\"can you help me\", \"pouvez vous m aider\"),\n",
        "    (\"i need assistance\", \"j ai besoin d aide\"),\n",
        "    (\"what time is it\", \"quelle heure est il\"),\n",
        "    (\"it is five o'clock\", \"il est cinq heures\"),\n",
        "    (\"where is the station\", \"ou est la gare\"),\n",
        "    (\"i am learning french\", \"j apprends le francais\"),\n",
        "    (\"this is a cat\", \"c est un chat\"),\n",
        "    (\"that is a dog\", \"c est un chien\"),\n",
        "    (\"the weather is nice\", \"il fait beau\"),\n",
        "    (\"it is raining\", \"il pleut\"),\n",
        "    (\"i am hungry\", \"j ai faim\"),\n",
        "    (\"i am thirsty\", \"j ai soif\"),\n",
        "    (\"let's go\", \"allons y\"),\n",
        "    (\"come here\", \"viens ici\"),\n",
        "    (\"open the door\", \"ouvre la porte\"),\n",
        "    (\"close the window\", \"ferme la fenetre\"),\n",
        "    (\"i am tired\", \"je suis fatigue\"),\n",
        "    (\"i don't know\", \"je ne sais pas\"),\n",
        "    (\"i agree\", \"je suis d accord\"),\n",
        "    (\"i disagree\", \"je ne suis pas d accord\"),\n",
        "    (\"i like this song\", \"j aime cette chanson\"),\n",
        "    (\"this is my friend\", \"c est mon ami\"),\n",
        "    (\"do you like coffee\", \"aimes tu le cafe\"),\n",
        "    (\"yes i like coffee\", \"oui j aime le cafe\"),\n",
        "    (\"no i prefer tea\", \"non je prefere le the\"),\n",
        "    (\"what do you do\", \"que fais tu dans la vie\"),\n",
        "    (\"i am a student\", \"je suis etudiant\"),\n",
        "    (\"i am a teacher\", \"je suis professeur\"),\n",
        "    (\"deep learning is interesting\", \"l apprentissage profond est interessant\"),\n",
        "    (\"we are training a model\", \"nous entrainons un modele\"),\n",
        "    (\"this dataset is large\", \"ce jeu de donnees est grand\"),\n",
        "    (\"the model is overfitting\", \"le modele fait du surapprentissage\"),\n",
        "    (\"we need more data\", \"nous avons besoin de plus de donnees\"),\n",
        "    (\"good afternoon\", \"bon apres-midi\"),\n",
        "    (\"good evening\", \"bonsoir\"),\n",
        "    (\"excuse me\", \"excusez-moi\"),\n",
        "    (\"please\", \"s il vous plait\"),\n",
        "    (\"you're welcome\", \"de rien\"),\n",
        "    (\"how much does it cost\", \"combien ca coute\"),\n",
        "    (\"where is the bathroom\", \"ou sont les toilettes\"),\n",
        "    (\"i need a doctor\", \"j ai besoin d un medecin\"),\n",
        "    (\"i am lost\", \"je suis perdu\"),\n",
        "    (\"can you repeat that\", \"pouvez vous repeter ca\"),\n",
        "    (\"speak slower please\", \"parlez plus lentement s il vous plait\"),\n",
        "    (\"write it down\", \"ecrivez le\"),\n",
        "    (\"what is this\", \"qu est ce que c est\"),\n",
        "    (\"how do you say this in french\", \"comment dit on ca en francais\"),\n",
        "    (\"i understand\", \"je comprends\"),\n",
        "    (\"i don't understand french\", \"je ne comprends pas le francais\"),\n",
        "    (\"it's too expensive\", \"c est trop cher\"),\n",
        "    (\"i'll take it\", \"je le prends\"),\n",
        "    (\"where is the exit\", \"ou est la sortie\"),\n",
        "    (\"where is the entrance\", \"ou est l entree\"),\n",
        "    (\"is there a restaurant nearby\", \"y a t il un restaurant a proximite\"),\n",
        "    (\"i want to eat\", \"je veux manger\"),\n",
        "    (\"i want to drink\", \"je veux boire\"),\n",
        "    (\"the bill please\", \"l addition s il vous plait\"),\n",
        "    (\"it was delicious\", \"c etait delicieux\"),\n",
        "    (\"i would like water\", \"je voudrais de l eau\"),\n",
        "    (\"i would like a coffee\", \"je voudrais un cafe\"),\n",
        "    (\"i would like a beer\", \"je voudrais une biere\"),\n",
        "    (\"do you have a table for two\", \"avez vous une table pour deux\"),\n",
        "    (\"i have a reservation\", \"j ai une reservation\"),\n",
        "    (\"what is the weather like\", \"quel temps fait il\"),\n",
        "    (\"it is sunny\", \"il fait soleil\"),\n",
        "    (\"it is cloudy\", \"il fait nuageux\"),\n",
        "    (\"it is cold\", \"il fait froid\"),\n",
        "    (\"it is hot\", \"il fait chaud\"),\n",
        "    (\"it is windy\", \"il fait du vent\"),\n",
        "    (\"what day is it today\", \"quel jour sommes nous aujourd hui\"),\n",
        "    (\"today is monday\", \"aujourd hui c est lundi\"),\n",
        "    (\"tomorrow is tuesday\", \"demain c est mardi\"),\n",
        "    (\"yesterday was sunday\", \"hier c etait dimanche\"),\n",
        "    (\"see you soon\", \"a bientot\"),\n",
        "    (\"have a good trip\", \"bon voyage\"),\n",
        "    (\"be careful\", \"fais attention\"),\n",
        "    (\"no problem\", \"pas de probleme\"),\n",
        "    (\"i am busy\", \"je suis occupe\"),\n",
        "    (\"i am happy\", \"je suis content\"),\n",
        "    (\"i am sad\", \"je suis triste\"),\n",
        "    (\"i am bored\", \"je m ennuie\"),\n",
        "    (\"i am excited\", \"je suis excite\"),\n",
        "    (\"i am sick\", \"je suis malade\"),\n",
        "    (\"i have a headache\", \"j ai mal a la tete\"),\n",
        "    (\"i have a stomach ache\", \"j ai mal au ventre\"),\n",
        "    (\"i feel good\", \"je me sens bien\"),\n",
        "    (\"i feel bad\", \"je me sens mal\"),\n",
        "    (\"what time do you open\", \"a quelle heure ouvrez vous\"),\n",
        "    (\"what time do you close\", \"a quelle heure fermez vous\"),\n",
        "    (\"is it open\", \"est ce ouvert\"),\n",
        "    (\"is it closed\", \"est ce ferme\"),\n",
        "    (\"can i pay by card\", \"puis je payer par carte\"),\n",
        "    (\"can i pay cash\", \"puis je payer en especes\"),\n",
        "    (\"where is the bank\", \"ou est la banque\"),\n",
        "    (\"where is the post office\", \"ou est la poste\"),\n",
        "    (\"how far is it\", \"a quelle distance est ce\"),\n",
        "    (\"it is far\", \"c est loin\"),\n",
        "    (\"it is near\", \"c est pres\"),\n",
        "    (\"turn left\", \"tournez a gauche\"),\n",
        "    (\"turn right\", \"tournez a droite\"),\n",
        "    (\"go straight ahead\", \"allez tout droit\"),\n",
        "    (\"stop here\", \"arretez vous ici\"),\n",
        "    (\"take me to this address\", \"emmenez moi a cette adresse\"),\n",
        "    (\"i want a ticket to paris\", \"je veux un billet pour paris\"),\n",
        "    (\"one way or round trip\", \"aller simple ou aller retour\"),\n",
        "    (\"how long does it take\", \"combien de temps ca prend\"),\n",
        "    (\"when does the train leave\", \"quand part le train\"),\n",
        "    (\"when does the bus arrive\", \"quand arrive le bus\"),\n",
        "    (\"i am here on vacation\", \"je suis ici en vacances\"),\n",
        "    (\"i am here for work\", \"je suis ici pour le travail\"),\n",
        "    (\"i like france\", \"j aime la france\"),\n",
        "    (\"i don't like it\", \"je n aime pas ca\"),\n",
        "    (\"can i try it on\", \"puis je l essayer\"),\n",
        "    (\"what size is this\", \"quelle taille est ce\"),\n",
        "    (\"do you have a bigger size\", \"avez vous une taille plus grande\"),\n",
        "    (\"do you have a smaller size\", \"avez vous une taille plus petite\"),\n",
        "    (\"i need help with my luggage\", \"j ai besoin d aide avec mes bagages\"),\n",
        "    (\"where is the information desk\", \"ou est le bureau d information\"),\n",
        "    (\"what's your phone number\", \"quel est votre numero de telephone\"),\n",
        "    (\"what's your email address\", \"quelle est votre adresse e-mail\"),\n",
        "    (\"can i call you\", \"puis je vous appeler\"),\n",
        "    (\"please wait\", \"veuillez patienter\"),\n",
        "    (\"come in\", \"entrez\"),\n",
        "    (\"sit down\", \"asseyez vous\"),\n",
        "    (\"stand up\", \"levez vous\"),\n",
        "    (\"listen to me\", \"ecoutez moi\"),\n",
        "    (\"look at this\", \"regardez ca\"),\n",
        "    (\"i am learning a lot\", \"j apprends beaucoup\"),\n",
        "    (\"it is difficult\", \"c est difficile\"),\n",
        "    (\"it is easy\", \"c est facile\"),\n",
        "    (\"it is very interesting\", \"c est tres interessant\"),\n",
        "    (\"i need more practice\", \"j ai besoin de plus de pratique\"),\n",
        "    (\"what are you doing\", \"que faites vous\"),\n",
        "    (\"i am reading a book\", \"je lis un livre\"),\n",
        "    (\"i am watching tv\", \"je regarde la tele\"),\n",
        "    (\"i am listening to music\", \"j ecoute de la musique\"),\n",
        "    (\"i am cooking\", \"je cuisine\"),\n",
        "    (\"i am working\", \"je travaille\"),\n",
        "    (\"i am studying\", \"j etudie\"),\n",
        "    (\"i am going home\", \"je rentre a la maison\"),\n",
        "    (\"i am going to bed\", \"je vais me coucher\"),\n",
        "    (\"i am waking up\", \"je me reveille\"),\n",
        "    (\"have a good meal\", \"bon appetit\"),\n",
        "    (\"cheers\", \"sante\"),\n",
        "    (\"happy birthday\", \"joyeux anniversaire\"),\n",
        "    (\"merry christmas\", \"joyeux noel\"),\n",
        "    (\"happy new year\", \"bonne annee\"),\n",
        "    (\"congratulations\", \"felicitations\"),\n",
        "    (\"good luck\", \"bonne chance\"),\n",
        "    (\"i am sorry\", \"je suis desole\"),\n",
        "    (\"it's okay\", \"c est bon\"),\n",
        "    (\"never mind\", \"laisse tomber\"),\n",
        "    (\"i totally agree\", \"je suis entierement d accord\"),\n",
        "    (\"i think so\", \"je pense que oui\"),\n",
        "    (\"i don't think so\", \"je ne pense pas\"),\n",
        "    (\"it's important\", \"c est important\"),\n",
        "    (\"it's urgent\", \"c est urgent\"),\n",
        "    (\"i need help with this exercise\", \"j ai besoin d aide pour cet exercice\"),\n",
        "    (\"this is a challenging problem\", \"c est un probleme difficile\"),\n",
        "    (\"we need to optimize the code\", \"nous devons optimiser le code\"),\n",
        "    (\"the algorithm is complex\", \"l algorithme est complexe\"),\n",
        "    (\"what is a neural network\", \"qu est ce qu un reseau de neurones\"),\n",
        "    (\"how does backpropagation work\", \"comment fonctionne la retropropagation\"),\n",
        "    (\"we are collecting more data\", \"nous collectons plus de donnees\"),\n",
        "    (\"the training loss is decreasing\", \"la perte d entrainement diminue\"),\n",
        "    (\"the validation accuracy is stable\", \"la precision de validation est stable\"),\n",
        "    (\"we need to adjust the hyperparameters\", \"nous devons ajuster les hyperparametres\"),\n",
        "    (\"this model is production-ready\", \"ce modele est pret pour la production\"),\n",
        "    (\"data preprocessing is crucial\", \"le pre-traitement des donnees est crucial\"),\n",
        "    (\"feature engineering is important\", \"l ingenierie des caracteristiques est importante\"),\n",
        "    (\"we are debugging the script\", \"nous deboguons le script\"),\n",
        "    (\"what is the learning rate\", \"quel est le taux d apprentissage\"),\n",
        "    (\"gradient descent is an optimization algorithm\", \"la descente de gradient est un algorithme d optimisation\"),\n",
        "    (\"we use GPUs for faster training\", \"nous utilisons des GPU pour un entrainement plus rapide\"),\n",
        "    (\"this is a classification task\", \"c est une tache de classification\"),\n",
        "    (\"this is a regression task\", \"c est une tache de regression\"),\n",
        "    (\"we need to fine-tune the model\", \"nous devons affiner le modele\"),\n",
        "    (\"transfer learning is effective\", \"l apprentissage par transfert est efficace\"),\n",
        "    (\"explain the attention mechanism\", \"expliquez le mecanisme d attention\"),\n",
        "    (\"what are transformers in nlp\", \"que sont les transformers en tnl\"),\n",
        "    (\"the model predicts the next word\", \"le modele predit le mot suivant\"),\n",
        "    (\"it's an end-to-end system\", \"c est un systeme de bout en bout\"),\n",
        "    (\"we are evaluating the performance\", \"nous evaluons la performance\"),\n",
        "    (\"the results are promising\", \"les resultats sont prometteurs\"),\n",
        "    (\"we need to document the code\", \"nous devons documenter le code\"),\n",
        "    (\"version control is essential\", \"le controle de version est essentiel\"),\n",
        "    (\"what is your favorite programming language\", \"quel est votre langage de programmation prefere\"),\n",
        "    (\"i prefer python\", \"je prefere python\"),\n",
        "    (\"this is a good example\", \"c est un bon exemple\"),\n",
        "    (\"it's a difficult question\", \"c est une question difficile\"),\n",
        "    (\"i'm thinking about it\", \"j y reflechis\"),\n",
        "    (\"can you explain more\", \"pouvez vous expliquer plus\"),\n",
        "    (\"i agree with you\", \"je suis d accord avec vous\"),\n",
        "    (\"i understand what you mean\", \"je comprends ce que vous voulez dire\"),\n",
        "    (\"how was your day\", \"comment etait ta journee\"),\n",
        "    (\"it was good\", \"c etait bien\"),\n",
        "    (\"it was bad\", \"c etait mauvais\"),\n",
        "    (\"i had a busy day\", \"j ai eu une journee occupee\"),\n",
        "    (\"what are your hobbies\", \"quels sont vos loisirs\"),\n",
        "    (\"i like to travel\", \"j aime voyager\"),\n",
        "    (\"i like to read\", \"j aime lire\"),\n",
        "    (\"i like to cook\", \"j aime cuisiner\"),\n",
        "    (\"what is your favorite food\", \"quel est votre plat prefere\"),\n",
        "    (\"i like french food\", \"j aime la cuisine francaise\"),\n",
        "    (\"can you recommend a good book\", \"pouvez vous me recommander un bon livre\"),\n",
        "    (\"i will try my best\", \"je ferai de mon mieux\"),\n",
        "    (\"i hope so\", \"j espere que oui\"),\n",
        "    (\"i hope not\", \"j espere que non\"),\n",
        "    (\"it's a pleasure\", \"c est un plaisir\"),\n",
        "    (\"take care\", \"prends soin de toi\"),\n",
        "    (\"what's new\", \"quoi de neuf\"),\n",
        "    (\"nothing much\", \"pas grand chose\"),\n",
        "    (\"do you have any questions\", \"avez vous des questions\"),\n",
        "    (\"i have no questions\", \"je n ai pas de questions\"),\n",
        "    (\"thank you for your time\", \"merci pour votre temps\"),\n",
        "    (\"see you tomorrow\", \"a demain\"),\n",
        "    (\"have a good weekend\", \"bon weekend\"),\n",
        "    (\"i want to learn more\", \"je veux en savoir plus\"),\n",
        "    (\"this is very useful\", \"c est tres utile\"),\n",
        "    (\"can you show me\", \"pouvez vous me montrer\"),\n",
        "    (\"i am sure\", \"je suis sur\"),\n",
        "    (\"i am not sure\", \"je ne suis pas sur\"),\n",
        "    (\"it's not fair\", \"ce n est pas juste\"),\n",
        "    (\"it's wonderful\", \"c est merveilleux\"),\n",
        "    (\"it's terrible\", \"c est terrible\"),\n",
        "    (\"i need a break\", \"j ai besoin d une pause\"),\n",
        "    (\"let's take a break\", \"faisons une pause\"),\n",
        "    (\"what's the problem\", \"quel est le probleme\"),\n",
        "    (\"there is no problem\", \"il n y a pas de probleme\"),\n",
        "    (\"i can't find it\", \"je ne le trouve pas\"),\n",
        "    (\"i found it\", \"je l ai trouve\"),\n",
        "    (\"it's getting late\", \"il se fait tard\"),\n",
        "    (\"i must go now\", \"je dois partir maintenant\"),\n",
        "    (\"this is too much\", \"c est trop\"),\n",
        "    (\"this is not enough\", \"ce n est pas assez\"),\n",
        "    (\"i'm ready\", \"je suis pret\"),\n",
        "    (\"are you ready\", \"etes vous pret\"),\n",
        "    (\"i'm waiting for you\", \"je vous attends\"),\n",
        "    (\"don't worry\", \"ne t inquiete pas\"),\n",
        "    (\"it's going to be okay\", \"ca va aller\"),\n",
        "    (\"what's your opinion\", \"quel est votre avis\"),\n",
        "    (\"in my opinion\", \"a mon avis\"),\n",
        "    (\"i think that\", \"je pense que\"),\n",
        "    (\"it seems that\", \"il semble que\"),\n",
        "    (\"i would like to know\", \"je voudrais savoir\"),\n",
        "    (\"can you explain to me\", \"pouvez vous m expliquer\"),\n",
        "    (\"i'm trying to learn\", \"j essaie d apprendre\")\n",
        "]"
      ],
      "metadata": {
        "id": "QExmKJ7yrjRq"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(raw_data_pairs)\n",
        "split_idx = int(len(raw_data_pairs) * 0.9)\n",
        "train_data = raw_data_pairs[:split_idx]\n",
        "valid_data = raw_data_pairs[split_idx:]\n",
        "\n",
        "print(f\"Training examples: {len(train_data)}\")\n",
        "print(f\"Validation examples: {len(valid_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVv6pHJti0S5",
        "outputId": "dd967875-527f-4472-feb4-d3ba9ac05290"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training examples: 238\n",
            "Validation examples: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer\n",
        "We build a simple tokenizer to introduce the concept. This tokenizer does not reduce to lemmas or perform any vector distributions for the inputs. Instead, each unique word is given a numeric representation that counts upward as new words are added."
      ],
      "metadata": {
        "id": "HZiBElgSrvna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_TOKEN = \"<pad>\"\n",
        "SOS_TOKEN = \"<sos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "class CustomTokenizer:\n",
        "  def __init__(self, language_name):\n",
        "    self.language_name = language_name\n",
        "    self.word2index = {}\n",
        "    self.index2word = {}\n",
        "    self.n_count = 0\n",
        "    self.word_counts = Counter()\n",
        "    self.add_word(PAD_TOKEN)\n",
        "    self.add_word(SOS_TOKEN)\n",
        "    self.add_word(EOS_TOKEN)\n",
        "    self.add_word(UNK_TOKEN)\n",
        "\n",
        "    self.PAD_IDX = self.add_word(PAD_TOKEN)\n",
        "    self.SOS_IDX = self.add_word(SOS_TOKEN)\n",
        "    self.EOS_IDX = self.add_word(EOS_TOKEN)\n",
        "    self.UNK_IDX = self.add_word(UNK_TOKEN)\n",
        "\n",
        "  # a linear tokenizer (count -> index)\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_count\n",
        "      self.index2word[self.n_count] = word\n",
        "      self.n_count += 1\n",
        "    return self.word2index[word]\n",
        "\n",
        "  def add_sentence(self, sentence):\n",
        "    for word in sentence.lower().split(' '):\n",
        "      self.word_counts[word] += 1\n",
        "\n",
        "  def build_vocab(self, sentences):\n",
        "    # Build up a count for each word\n",
        "    for sentence in sentences:\n",
        "      self.add_sentence(sentence)\n",
        "\n",
        "    # Add each unique key (word) to the word2index / index2word dicts\n",
        "    for word in sorted(self.word_counts.keys()):\n",
        "      self.add_word(word)\n",
        "\n",
        "  def sentence_to_indices(self, sentence):\n",
        "    tokens = [SOS_TOKEN] + sentence.lower().split(' ') + [EOS_TOKEN]\n",
        "    indices = [self.word2index.get(token, self.UNK_IDX) for token in tokens]\n",
        "    return indices\n",
        "\n",
        "  def indices_to_sentence(self, indices):\n",
        "    if hasattr(indices, 'tolist'):\n",
        "      indices = indices.tolist()\n",
        "    return ' '.join(self.index2word.get(index, UNK_TOKEN) for index in indices\n",
        "                    if index not in [self.SOS_IDX, self.EOS_IDX, self.PAD_IDX])\n"
      ],
      "metadata": {
        "id": "6wkhVYHUrw-6"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the tokenizers and input vocabularies"
      ],
      "metadata": {
        "id": "EdxLLoTL8xN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_tokenizer = CustomTokenizer(SRC_LANGUAGE)\n",
        "tgt_tokenizer = CustomTokenizer(TGT_LANGUAGE)\n",
        "\n",
        "src_sentences = [pair[0] for pair in train_data]\n",
        "tgt_sentences = [pair[1] for pair in train_data]\n",
        "\n",
        "src_tokenizer.build_vocab(src_sentences)\n",
        "tgt_tokenizer.build_vocab(tgt_sentences)"
      ],
      "metadata": {
        "id": "2LV38c9080rP"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test their behaviour"
      ],
      "metadata": {
        "id": "ZjMJ73rz9aJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary\n",
        "print(\"\\nSource Vocabulary (EN):\")\n",
        "print(src_tokenizer.word2index)\n",
        "print(f\"PAD_IDX: {src_tokenizer.PAD_IDX}, SOS_IDX: {src_tokenizer.SOS_IDX},\"\n",
        "      f\"EOS_IDX: {src_tokenizer.EOS_IDX}, UNK_IDX: {src_tokenizer.UNK_IDX}\")\n",
        "\n",
        "print(f\"\\nTarget Vocabulary (FR)\")\n",
        "print(tgt_tokenizer.word2index)\n",
        "print(f\"PAD_IDX: {tgt_tokenizer.PAD_IDX}, SOS_IDX: {tgt_tokenizer.SOS_IDX}\"\n",
        "      f\"EOS_IDX: {tgt_tokenizer.EOS_IDX}, UNK_IDX: {tgt_tokenizer.UNK_IDX}\")\n",
        "\n",
        "# Test the tokenizer\n",
        "test_src_sent = \"hello world\"\n",
        "test_src_indices = src_tokenizer.sentence_to_indices(test_src_sent)\n",
        "print(f\"\\n'{test_src_sent}' -> {test_src_indices}\")\n",
        "print(f\"'{test_src_indices}' -> '{src_tokenizer.indices_to_sentence(test_src_indices)}'\\n\")\n",
        "\n",
        "test_tgt_sent = \"bonjour le monde\"\n",
        "test_tgt_indices = tgt_tokenizer.sentence_to_indices(test_tgt_sent)\n",
        "print(f\"'{test_tgt_sent}' -> {test_tgt_indices}\")\n",
        "print(f\"'{test_tgt_indices}' -> '{tgt_tokenizer.indices_to_sentence(test_tgt_indices)}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkGlpW339bXF",
        "outputId": "d2f7a5f6-0a38-4e51-e741-6ef87ef0c409"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Source Vocabulary (EN):\n",
            "{'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3, 'a': 4, 'about': 5, 'accuracy': 6, 'ache': 7, 'address': 8, 'adjust': 9, 'afternoon': 10, 'agree': 11, 'algorithm': 12, 'am': 13, 'an': 14, 'any': 15, 'are': 16, 'arrive': 17, 'assistance': 18, 'at': 19, 'attention': 20, 'backpropagation': 21, 'bad': 22, 'bank': 23, 'bathroom': 24, 'be': 25, 'bed': 26, 'beer': 27, 'best': 28, 'bigger': 29, 'bill': 30, 'birthday': 31, 'book': 32, 'bored': 33, 'break': 34, 'bus': 35, 'busy': 36, 'by': 37, 'call': 38, 'can': 39, \"can't\": 40, 'canada': 41, 'card': 42, 'care': 43, 'careful': 44, 'cash': 45, 'cat': 46, 'challenging': 47, 'cheers': 48, 'christmas': 49, 'classification': 50, 'close': 51, 'closed': 52, 'cloudy': 53, 'code': 54, 'coffee': 55, 'cold': 56, 'come': 57, 'complex': 58, 'control': 59, 'cook': 60, 'cooking': 61, 'cool': 62, 'cost': 63, 'crucial': 64, 'data': 65, 'dataset': 66, 'day': 67, 'debugging': 68, 'delicious': 69, 'descent': 70, 'desk': 71, 'difficult': 72, 'disagree': 73, 'do': 74, 'doctor': 75, 'document': 76, 'does': 77, 'dog': 78, 'doing': 79, \"don't\": 80, 'door': 81, 'down': 82, 'drink': 83, 'easy': 84, 'eat': 85, 'effective': 86, 'email': 87, 'end-to-end': 88, 'engineering': 89, 'english': 90, 'entrance': 91, 'essential': 92, 'evaluating': 93, 'evening': 94, 'example': 95, 'excited': 96, 'excuse': 97, 'exercise': 98, 'exit': 99, 'expensive': 100, 'explain': 101, 'fair': 102, 'far': 103, 'faster': 104, 'favorite': 105, 'feature': 106, 'feel': 107, 'find': 108, 'fine': 109, 'fine-tune': 110, 'five': 111, 'food': 112, 'for': 113, 'found': 114, 'france': 115, 'french': 116, 'friend': 117, 'from': 118, 'getting': 119, 'go': 120, 'going': 121, 'good': 122, 'gpus': 123, 'gradient': 124, 'had': 125, 'happy': 126, 'have': 127, 'headache': 128, 'hello': 129, 'help': 130, 'here': 131, 'hope': 132, 'hot': 133, 'how': 134, 'hungry': 135, 'hyperparameters': 136, 'i': 137, \"i'll\": 138, \"i'm\": 139, 'important': 140, 'in': 141, 'information': 142, 'interesting': 143, 'is': 144, 'it': 145, \"it's\": 146, 'know': 147, 'language': 148, 'large': 149, 'late': 150, 'learn': 151, 'learning': 152, 'leave': 153, 'left': 154, \"let's\": 155, 'like': 156, 'listen': 157, 'listening': 158, 'look': 159, 'lot': 160, 'love': 161, 'luck': 162, 'luggage': 163, 'machine': 164, 'me': 165, 'meal': 166, 'mean': 167, 'mechanism': 168, 'meet': 169, 'merry': 170, 'mind': 171, 'model': 172, 'more': 173, 'much': 174, 'music': 175, 'must': 176, 'my': 177, 'name': 178, 'near': 179, 'nearby': 180, 'need': 181, 'network': 182, 'neural': 183, 'never': 184, 'new': 185, 'next': 186, 'nice': 187, 'night': 188, 'nlp': 189, 'no': 190, 'not': 191, 'nothing': 192, 'now': 193, 'number': 194, \"o'clock\": 195, 'office': 196, 'okay': 197, 'on': 198, 'one': 199, 'open': 200, 'opinion': 201, 'optimization': 202, 'optimize': 203, 'or': 204, 'overfitting': 205, 'pay': 206, 'performance': 207, 'phone': 208, 'please': 209, 'pleasure': 210, 'post': 211, 'practice': 212, 'predicts': 213, 'prefer': 214, 'preprocessing': 215, 'problem': 216, 'production-ready': 217, 'programming': 218, 'python': 219, 'pytorch': 220, 'question': 221, 'questions': 222, 'raining': 223, 'rate': 224, 'read': 225, 'reading': 226, 'ready': 227, 'recommend': 228, 'regression': 229, 'repeat': 230, 'reservation': 231, 'restaurant': 232, 'right': 233, 'round': 234, 'sad': 235, 'say': 236, 'script': 237, 'see': 238, 'show': 239, 'sick': 240, 'sit': 241, 'size': 242, 'slower': 243, 'smaller': 244, 'so': 245, 'soon': 246, 'sorry': 247, 'speak': 248, 'stable': 249, 'stand': 250, 'station': 251, 'stomach': 252, 'stop': 253, 'student': 254, 'studying': 255, 'sunday': 256, 'sunny': 257, 'sure': 258, 'system': 259, 'table': 260, 'take': 261, 'task': 262, 'teacher': 263, 'terrible': 264, 'thank': 265, 'that': 266, 'the': 267, 'there': 268, 'think': 269, 'thinking': 270, 'this': 271, 'time': 272, 'tired': 273, 'to': 274, 'today': 275, 'tomorrow': 276, 'too': 277, 'totally': 278, 'train': 279, 'training': 280, 'transfer': 281, 'transformers': 282, 'translation': 283, 'travel': 284, 'trip': 285, 'try': 286, 'trying': 287, 'tuesday': 288, 'turn': 289, 'tv': 290, 'two': 291, 'understand': 292, 'up': 293, 'urgent': 294, 'use': 295, 'useful': 296, 'vacation': 297, 'validation': 298, 'version': 299, 'very': 300, 'wait': 301, 'waiting': 302, 'want': 303, 'was': 304, 'watching': 305, 'water': 306, 'way': 307, 'we': 308, 'weather': 309, 'weekend': 310, 'welcome': 311, 'what': 312, \"what's\": 313, 'when': 314, 'where': 315, 'will': 316, 'window': 317, 'windy': 318, 'with': 319, 'wonderful': 320, 'word': 321, 'work': 322, 'working': 323, 'world': 324, 'worry': 325, 'would': 326, 'write': 327, 'year': 328, 'yes': 329, 'yesterday': 330, 'you': 331, \"you're\": 332, 'your': 333}\n",
            "PAD_IDX: 0, SOS_IDX: 1,EOS_IDX: 2, UNK_IDX: 3\n",
            "\n",
            "Target Vocabulary (FR)\n",
            "{'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3, 'a': 4, 'accord': 5, 'addition': 6, 'adresse': 7, 'affiner': 8, 'ai': 9, 'aide': 10, 'aider': 11, 'aime': 12, 'aimes': 13, 'ajuster': 14, 'algorithme': 15, 'aller': 16, 'allez': 17, 'allons': 18, 'ami': 19, 'anglais': 20, 'annee': 21, 'anniversaire': 22, 'appeler': 23, 'appetit': 24, 'apprendre': 25, 'apprends': 26, 'apprentissage': 27, 'apres-midi': 28, 'arretez': 29, 'arrive': 30, 'asseyez': 31, 'attends': 32, 'attention': 33, 'au': 34, 'aujourd': 35, 'automatique': 36, 'avec': 37, 'avez': 38, 'avis': 39, 'bagages': 40, 'banque': 41, 'beau': 42, 'beaucoup': 43, 'besoin': 44, 'bien': 45, 'bientot': 46, 'biere': 47, 'boire': 48, 'bon': 49, 'bonjour': 50, 'bonne': 51, 'bonsoir': 52, 'bout': 53, 'bureau': 54, 'bus': 55, 'c': 56, 'ca': 57, 'cafe': 58, 'canada': 59, 'caracteristiques': 60, 'carte': 61, 'ce': 62, 'cet': 63, 'cette': 64, 'chance': 65, 'chat': 66, 'chaud': 67, 'cher': 68, 'chien': 69, 'chose': 70, 'cinq': 71, 'classification': 72, 'code': 73, 'combien': 74, 'comment': 75, 'complexe': 76, 'comprends': 77, 'content': 78, 'controle': 79, 'cool': 80, 'coucher': 81, 'coute': 82, 'crucial': 83, 'cuisine': 84, 'cuisiner': 85, 'd': 86, 'dans': 87, 'de': 88, 'deboguons': 89, 'delicieux': 90, 'demain': 91, 'des': 92, 'descente': 93, 'desole': 94, 'deux': 95, 'devons': 96, 'difficile': 97, 'dimanche': 98, 'dire': 99, 'distance': 100, 'dit': 101, 'documenter': 102, 'dois': 103, 'donnees': 104, 'droite': 105, 'du': 106, 'e-mail': 107, 'eau': 108, 'ecoute': 109, 'ecoutez': 110, 'ecrivez': 111, 'efficace': 112, 'emmenez': 113, 'en': 114, 'ennuie': 115, 'entierement': 116, 'entrainement': 117, 'entrainons': 118, 'entree': 119, 'entrez': 120, 'especes': 121, 'espere': 122, 'essaie': 123, 'essayer': 124, 'essentiel': 125, 'est': 126, 'etait': 127, 'etes': 128, 'etudiant': 129, 'etudie': 130, 'eu': 131, 'evaluons': 132, 'excite': 133, 'excusez-moi': 134, 'exemple': 135, 'exercice': 136, 'expliquer': 137, 'expliquez': 138, 'facile': 139, 'faim': 140, 'fais': 141, 'faisons': 142, 'fait': 143, 'faites': 144, 'fatigue': 145, 'fenetre': 146, 'ferai': 147, 'ferme': 148, 'fermez': 149, 'fonctionne': 150, 'francais': 151, 'francaise': 152, 'france': 153, 'froid': 154, 'gare': 155, 'gauche': 156, 'gpu': 157, 'gradient': 158, 'grand': 159, 'grande': 160, 'heure': 161, 'heures': 162, 'hier': 163, 'hui': 164, 'hyperparametres': 165, 'ici': 166, 'il': 167, 'important': 168, 'importante': 169, 'information': 170, 'ingenierie': 171, 'inquiete': 172, 'interessant': 173, 'j': 174, 'je': 175, 'jeu': 176, 'jour': 177, 'journee': 178, 'joyeux': 179, 'juste': 180, 'l': 181, 'la': 182, 'laisse': 183, 'langage': 184, 'le': 185, 'lentement': 186, 'les': 187, 'levez': 188, 'lire': 189, 'lis': 190, 'livre': 191, 'loin': 192, 'm': 193, 'maintenant': 194, 'mal': 195, 'malade': 196, 'manger': 197, 'mardi': 198, 'mauvais': 199, 'me': 200, 'mecanisme': 201, 'medecin': 202, 'merci': 203, 'merveilleux': 204, 'mes': 205, 'mieux': 206, 'modele': 207, 'moi': 208, 'mon': 209, 'monde': 210, 'montrer': 211, 'mot': 212, 'musique': 213, 'n': 214, 'ne': 215, 'neuf': 216, 'neurones': 217, 'noel': 218, 'nom': 219, 'nous': 220, 'nuageux': 221, 'nuit': 222, 'numero': 223, 'occupe': 224, 'occupee': 225, 'on': 226, 'optimisation': 227, 'optimiser': 228, 'ou': 229, 'oui': 230, 'ouvert': 231, 'ouvre': 232, 'ouvrez': 233, 'par': 234, 'parle': 235, 'parlez': 236, 'part': 237, 'partir': 238, 'pas': 239, 'patienter': 240, 'pause': 241, 'payer': 242, 'pense': 243, 'performance': 244, 'petite': 245, 'plaisir': 246, 'plait': 247, 'plat': 248, 'pleut': 249, 'plus': 250, 'porte': 251, 'poste': 252, 'pour': 253, 'pouvez': 254, 'pratique': 255, 'pre-traitement': 256, 'precision': 257, 'predit': 258, 'prefere': 259, 'prends': 260, 'pres': 261, 'pret': 262, 'probleme': 263, 'production': 264, 'professeur': 265, 'programmation': 266, 'proximite': 267, 'puis': 268, 'python': 269, 'pytorch': 270, 'qu': 271, 'quand': 272, 'que': 273, 'quel': 274, 'quelle': 275, 'question': 276, 'questions': 277, 'quoi': 278, 'rapide': 279, 'ravi': 280, 'recommander': 281, 'reflechis': 282, 'regarde': 283, 'regardez': 284, 'regression': 285, 'rencontrer': 286, 'repeter': 287, 'reseau': 288, 'reservation': 289, 'restaurant': 290, 'retour': 291, 'retropropagation': 292, 'rien': 293, 's': 294, 'sais': 295, 'sante': 296, 'savoir': 297, 'script': 298, 'se': 299, 'sens': 300, 'simple': 301, 'soin': 302, 'soleil': 303, 'sommes': 304, 'sont': 305, 'sortie': 306, 'stable': 307, 'suis': 308, 'suivant': 309, 'sur': 310, 'surapprentissage': 311, 'systeme': 312, 't': 313, 'ta': 314, 'table': 315, 'tache': 316, 'taille': 317, 'tard': 318, 'taux': 319, 'tele': 320, 'telephone': 321, 'temps': 322, 'terrible': 323, 'tete': 324, 'tnl': 325, 'toi': 326, 'toilettes': 327, 'tomber': 328, 'tournez': 329, 'traduction': 330, 'train': 331, 'transfert': 332, 'transformers': 333, 'travail': 334, 'travaille': 335, 'tres': 336, 'triste': 337, 'trop': 338, 'trouve': 339, 'tu': 340, 'un': 341, 'une': 342, 'urgent': 343, 'utile': 344, 'utilisons': 345, 'va': 346, 'vacances': 347, 'vais': 348, 'validation': 349, 'venez': 350, 'vent': 351, 'ventre': 352, 'version': 353, 'veuillez': 354, 'veux': 355, 'vie': 356, 'viens': 357, 'votre': 358, 'voudrais': 359, 'voulez': 360, 'vous': 361, 'voyage': 362, 'voyager': 363, 'weekend': 364, 'y': 365}\n",
            "PAD_IDX: 0, SOS_IDX: 1EOS_IDX: 2, UNK_IDX: 3\n",
            "\n",
            "'hello world' -> [1, 129, 324, 2]\n",
            "'[1, 129, 324, 2]' -> 'hello world'\n",
            "\n",
            "'bonjour le monde' -> [1, 50, 185, 210, 2]\n",
            "'[1, 50, 185, 210, 2]' -> 'bonjour le monde'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding\n",
        "We need to pad so that different length sentences can be accepted by the RNN in batches. We use padding tokens to achieve this."
      ],
      "metadata": {
        "id": "9MHZzvHqGAhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch, src_tokenizer, tgt_tokenizer, device):\n",
        "  src_batch, tgt_batch = [], []\n",
        "  src_lens, tgt_lens = [], []\n",
        "  for src_sample, tgt_sample in batch:\n",
        "    src_indices = src_tokenizer.sentence_to_indices(src_sample)\n",
        "    tgt_indices = tgt_tokenizer.sentence_to_indices(tgt_sample)\n",
        "\n",
        "    src_batch.append(torch.tensor(src_indices, dtype=torch.long))\n",
        "    tgt_batch.append(torch.tensor(tgt_indices, dtype=torch.long))\n",
        "\n",
        "    src_lens.append(len(src_indices))\n",
        "    tgt_lens.append(len(tgt_indices))\n",
        "\n",
        "  src_padded = nn.utils.rnn.pad_sequence(src_batch, padding_value=src_tokenizer.PAD_IDX, batch_first=False)\n",
        "  tgt_padded = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=tgt_tokenizer.PAD_IDX, batch_first=False)\n",
        "\n",
        "  return src_padded.to(device), tgt_padded.to(device), torch.tensor(src_lens), torch.tensor(tgt_lens)"
      ],
      "metadata": {
        "id": "oh2Jr7IbGJlp"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a sample dataloader"
      ],
      "metadata": {
        "id": "4WygYgAhmajh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2\n",
        "def get_data_iterator(data, src_tokenizer, tgt_tokenizer, batch_size, device, shuffle=True):\n",
        "  if shuffle:\n",
        "    data_copy = list(data)\n",
        "    random.shuffle(data_copy)\n",
        "  else:\n",
        "    data_copy = data\n",
        "\n",
        "  for i in range(0, len(data_copy), batch_size):\n",
        "    batch = data_copy[i:i+batch_size]\n",
        "    yield collate_fn(batch, src_tokenizer, tgt_tokenizer, device)"
      ],
      "metadata": {
        "id": "fEFHGnySmcO-"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTesting data iterator:\")\n",
        "data_iter = get_data_iterator(train_data,src_tokenizer, tgt_tokenizer, BATCH_SIZE, device)\n",
        "for i, (src_batch, tgt_batch, src_lens, tgt_lens) in enumerate(data_iter):\n",
        "  print(f\"Batch {i+1}:\")\n",
        "  print(\"Source batch shape: \", src_batch.shape)\n",
        "  print(\"target batch shape: \", tgt_batch.shape)\n",
        "  print(\"Source lengths: \", src_lens)\n",
        "  print(\"Target lengths: \", tgt_lens)\n",
        "  print(\"Source batch (first example):\\n\", src_batch[:, 0])\n",
        "  print(\"Target batch (first example):\\n\", tgt_batch[:, 0])\n",
        "  if i == 0: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mECBFLTvn6KC",
        "outputId": "c2f85508-fb97-47bf-cfa0-544a0d65212a"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing data iterator:\n",
            "Batch 1:\n",
            "Source batch shape:  torch.Size([6, 2])\n",
            "target batch shape:  torch.Size([8, 2])\n",
            "Source lengths:  tensor([6, 6])\n",
            "Target lengths:  tensor([8, 8])\n",
            "Source batch (first example):\n",
            " tensor([  1, 137, 181,   4,  34,   2])\n",
            "Target batch (first example):\n",
            " tensor([  1, 174,   9,  44,  86, 342, 241,   2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder\n",
        "\n",
        "We'll write the encoder using an nn.GRU unit. The encoder unit has several stages to its learning:\n",
        "\n",
        "* Timestep states *h* are computed as h$_{t}$ = f(h$_{t-1}$, x$_{t}$).\n",
        "* The final state h$_{t}$ is the context variable.\n",
        "* The context variable is given by some mapping *m* such that c = m(h$_{1}$, h$_{2}$, ... , h$_{t}$)\n",
        "* Encoders may be bidirectional, such that h$_{t}$ is a function of h$_{t-1}$ and h$_{t+1}$\n",
        "\n",
        "We build the unidirectional encoder here.\n",
        "\n",
        "Note the dimensionality of the various matrices."
      ],
      "metadata": {
        "id": "9VbBms5l_zYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout_p):\n",
        "    super().__init__()\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_layers = n_layers\n",
        "    # Here is the interesting bit: input dim is the size of your vocabulary.\n",
        "    # emb_dim is the arbitrary learning layer - you can select a size and experiment.\n",
        "    # We therefore train the rnn to work at a token level\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim,\n",
        "                      hid_dim,\n",
        "                      n_layers,\n",
        "                      dropout=dropout_p if n_layers > 1 else 0)\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self, src_seq):\n",
        "    embedded = self.dropout(self.embedding(src_seq))\n",
        "    outputs, hidden = self.rnn(embedded)\n",
        "    return outputs, hidden"
      ],
      "metadata": {
        "id": "9A57ZE1x_0np"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder takes the context variable from the encoder and creates its own hidden state. This hidden state is not only a function of the last hidden state, but also the previously decoded token.\n",
        "\n",
        "* s$_{t'}$ = g(s$_{t-1}$, y$_{t'-1}$, c)\n",
        "* y$_{t'}$ is a probability distribution of P(y$_{t'}$| y$_{t-1}$, ..., y$_{1}$, c) = softmax(s$_{t-1}$, y$_{t'-1}$, c).\n",
        "* In a sense, this means that the last N output tokens from the decoder influence the latest token and the current hidden state."
      ],
      "metadata": {
        "id": "A-pCsQ_QJpb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout_p):\n",
        "    super().__init__()\n",
        "    self.output_dim = output_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # Components\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim,\n",
        "                      hid_dim,\n",
        "                      n_layers,\n",
        "                      dropout=dropout_p if n_layers > 1 else 0)\n",
        "    self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self, input_token, hidden_state):\n",
        "    # turns [batch_size] into [1, batch_size]\n",
        "    input_token = input_token.unsqueeze(0)\n",
        "    embedded = self.dropout(self.embedding(input_token))\n",
        "\n",
        "    # Per token decoding\n",
        "    output, new_hidden_state = self.rnn(embedded, hidden_state)\n",
        "    prediction = self.fc_out(output.squeeze(0))\n",
        "\n",
        "    return prediction, new_hidden_state"
      ],
      "metadata": {
        "id": "8K5PmNiDN6yu"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq Implementation\n",
        "\n",
        "The Seq2Seq component implements and handles the encoder/decoder architecture."
      ],
      "metadata": {
        "id": "lY-91VeMfOcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "    # Sanity check for dimensionality\n",
        "    assert encoder.hid_dim == decoder.hid_dim, \"hidden dims must be equal\"\n",
        "    assert encoder.n_layers == decoder.n_layers, \"layers must be equal\"\n",
        "\n",
        "  def forward(self, src_seq, tgt_seq, teacher_forcing_ratio=0.5):\n",
        "    batch_size = src_seq.shape[1]\n",
        "    tgt_len = tgt_seq.shape[0]\n",
        "    tgt_vocab_size = self.decoder.output_dim\n",
        "    outputs = torch.zeros(tgt_len, batch_size, tgt_vocab_size)\n",
        "\n",
        "    # Encode\n",
        "    enc_out, hidden = self.encoder(src_seq)\n",
        "\n",
        "    # Decode\n",
        "    dec_in = tgt_seq[0, :]\n",
        "    for t in range(1, tgt_len):\n",
        "      dec_out, hidden = self.decoder(dec_in, hidden)\n",
        "      outputs[t] = dec_out\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "      top1 = dec_out.argmax(1)\n",
        "      dec_in = tgt_seq[t] if teacher_force else top1\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "-dy3R35PfXC_"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "QckUDH8kcOis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparams\n",
        "INPUT_DIM = src_tokenizer.n_count\n",
        "OUTPUT_DIM = tgt_tokenizer.n_count\n",
        "ENC_EMB_DIM = 128\n",
        "DEC_EMB_DIM = 128\n",
        "HID_DIM = 256\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "LEARNING_RATE = 0.001\n",
        "N_EPOCHS = 50\n",
        "CLIP = 1"
      ],
      "metadata": {
        "id": "cquq3cUbcP-M"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# components\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)\n",
        "model = Seq2Seq(enc, dec, device)"
      ],
      "metadata": {
        "id": "LDs8YgZlweKi"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwoiHd_0w1si",
        "outputId": "37f9db33-ebab-4849-baed-045d4832e953"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 1,566,062 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optim and learn\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = tgt_tokenizer.PAD_IDX)"
      ],
      "metadata": {
        "id": "AmF7TUbzxEcT"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "brGpMZ6AxQ22"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    batch_n = 0\n",
        "    for i, (src, tgt, _, _) in enumerate(iterator): # src_lens, tgt_lens not used directly here\n",
        "        batch_n += 1\n",
        "        optimizer.zero_grad()\n",
        "        # output = [tgt_len, batch_size, output_vocab_size]\n",
        "        output = model(src, tgt)\n",
        "        # get vocab length for next step\n",
        "        output_dim = output.shape[-1]\n",
        "        # remove <sos> tag by enforcing [1:]\n",
        "        # turn [tgt_len, batch, vocab] into [(tgt_len-1 * batch), vocab] so it fits into loss\n",
        "        output_flat = output[1:].view(-1, output_dim)\n",
        "        # since we know the vocab, this doesn't have the V dimension [tgt_len-1, batch]\n",
        "        tgt_flat = tgt[1:].view(-1)\n",
        "        # now that they are equal dim, compute the loss between out and tgt\n",
        "        loss = criterion(output_flat, tgt_flat)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / batch_n\n",
        "\n",
        "def evaluate_epoch(model, iterator, criterion):\n",
        "  # the same setup, but we loop without adam use\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  batch_n = 0\n",
        "  with torch.no_grad():\n",
        "    for i, (src, tgt, _, _) in enumerate(iterator):\n",
        "      batch_n += 1\n",
        "      output = model(src, tgt, 0)\n",
        "      output_dim = output.shape[-1]\n",
        "      output_flat = output[1:].view(-1, output_dim)\n",
        "      tgt_flat = tgt[1:].view(-1)\n",
        "      loss = criterion(output_flat, tgt_flat)\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss / batch_n"
      ],
      "metadata": {
        "id": "XRLTMnN8yG2p"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Execute"
      ],
      "metadata": {
        "id": "LDWtJKaM2hfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start_time = time.time()\n",
        "  train_iter = get_data_iterator(train_data,\n",
        "                               src_tokenizer,\n",
        "                               tgt_tokenizer,\n",
        "                               BATCH_SIZE,\n",
        "                               device,\n",
        "                               shuffle=True)\n",
        "  valid_iter = get_data_iterator(valid_data,\n",
        "                               src_tokenizer,\n",
        "                               tgt_tokenizer,\n",
        "                               BATCH_SIZE,\n",
        "                               device,\n",
        "                               shuffle=False)\n",
        "  train_loss = train_epoch(model, train_iter, optimizer, criterion, CLIP)\n",
        "  valid_loss = evaluate_epoch(model, valid_iter, criterion)\n",
        "\n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "torch.save(model.state_dict(), 'language_enc_dec.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8PSyI3EJ2jNs",
        "outputId": "a923f0e3-5698-4460-86b8-1c368d2f233e"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch: 01 | Time: 0m 6s\n",
            "\tTrain Loss: 3.913 | Train PPL:  50.027\n",
            "\t Val. Loss: 4.653 |  Val. PPL: 104.940\n",
            "Epoch: 02 | Time: 0m 5s\n",
            "\tTrain Loss: 3.452 | Train PPL:  31.569\n",
            "\t Val. Loss: 4.705 |  Val. PPL: 110.491\n",
            "Epoch: 03 | Time: 0m 6s\n",
            "\tTrain Loss: 3.077 | Train PPL:  21.688\n",
            "\t Val. Loss: 4.772 |  Val. PPL: 118.131\n",
            "Epoch: 04 | Time: 0m 5s\n",
            "\tTrain Loss: 2.700 | Train PPL:  14.874\n",
            "\t Val. Loss: 5.028 |  Val. PPL: 152.648\n",
            "Epoch: 05 | Time: 0m 6s\n",
            "\tTrain Loss: 2.386 | Train PPL:  10.873\n",
            "\t Val. Loss: 5.068 |  Val. PPL: 158.802\n",
            "Epoch: 06 | Time: 0m 5s\n",
            "\tTrain Loss: 2.021 | Train PPL:   7.542\n",
            "\t Val. Loss: 5.041 |  Val. PPL: 154.594\n",
            "Epoch: 07 | Time: 0m 6s\n",
            "\tTrain Loss: 1.741 | Train PPL:   5.703\n",
            "\t Val. Loss: 5.064 |  Val. PPL: 158.144\n",
            "Epoch: 08 | Time: 0m 5s\n",
            "\tTrain Loss: 1.437 | Train PPL:   4.208\n",
            "\t Val. Loss: 5.376 |  Val. PPL: 216.053\n",
            "Epoch: 09 | Time: 0m 6s\n",
            "\tTrain Loss: 1.164 | Train PPL:   3.203\n",
            "\t Val. Loss: 5.276 |  Val. PPL: 195.637\n",
            "Epoch: 10 | Time: 0m 5s\n",
            "\tTrain Loss: 0.910 | Train PPL:   2.484\n",
            "\t Val. Loss: 5.448 |  Val. PPL: 232.235\n",
            "Epoch: 11 | Time: 0m 5s\n",
            "\tTrain Loss: 0.721 | Train PPL:   2.057\n",
            "\t Val. Loss: 5.453 |  Val. PPL: 233.487\n",
            "Epoch: 12 | Time: 0m 5s\n",
            "\tTrain Loss: 0.598 | Train PPL:   1.818\n",
            "\t Val. Loss: 5.920 |  Val. PPL: 372.407\n",
            "Epoch: 13 | Time: 0m 5s\n",
            "\tTrain Loss: 0.448 | Train PPL:   1.565\n",
            "\t Val. Loss: 6.157 |  Val. PPL: 472.122\n",
            "Epoch: 14 | Time: 0m 5s\n",
            "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
            "\t Val. Loss: 5.922 |  Val. PPL: 373.216\n",
            "Epoch: 15 | Time: 0m 5s\n",
            "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
            "\t Val. Loss: 6.329 |  Val. PPL: 560.548\n",
            "Epoch: 16 | Time: 0m 5s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
            "\t Val. Loss: 6.169 |  Val. PPL: 477.885\n",
            "Epoch: 17 | Time: 0m 5s\n",
            "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
            "\t Val. Loss: 6.491 |  Val. PPL: 659.300\n",
            "Epoch: 18 | Time: 0m 5s\n",
            "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
            "\t Val. Loss: 6.926 |  Val. PPL: 1018.278\n",
            "Epoch: 19 | Time: 0m 5s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 6.789 |  Val. PPL: 888.170\n",
            "Epoch: 20 | Time: 0m 5s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 6.829 |  Val. PPL: 924.370\n",
            "Epoch: 21 | Time: 0m 5s\n",
            "\tTrain Loss: 0.031 | Train PPL:   1.032\n",
            "\t Val. Loss: 7.060 |  Val. PPL: 1164.476\n",
            "Epoch: 22 | Time: 0m 5s\n",
            "\tTrain Loss: 0.019 | Train PPL:   1.019\n",
            "\t Val. Loss: 7.470 |  Val. PPL: 1754.280\n",
            "Epoch: 23 | Time: 0m 5s\n",
            "\tTrain Loss: 0.017 | Train PPL:   1.017\n",
            "\t Val. Loss: 7.250 |  Val. PPL: 1407.564\n",
            "Epoch: 24 | Time: 0m 5s\n",
            "\tTrain Loss: 0.011 | Train PPL:   1.011\n",
            "\t Val. Loss: 7.576 |  Val. PPL: 1951.769\n",
            "Epoch: 25 | Time: 0m 6s\n",
            "\tTrain Loss: 0.009 | Train PPL:   1.009\n",
            "\t Val. Loss: 7.698 |  Val. PPL: 2204.314\n",
            "Epoch: 26 | Time: 0m 5s\n",
            "\tTrain Loss: 0.008 | Train PPL:   1.008\n",
            "\t Val. Loss: 7.695 |  Val. PPL: 2198.204\n",
            "Epoch: 27 | Time: 0m 6s\n",
            "\tTrain Loss: 0.008 | Train PPL:   1.008\n",
            "\t Val. Loss: 7.731 |  Val. PPL: 2278.537\n",
            "Epoch: 28 | Time: 0m 5s\n",
            "\tTrain Loss: 0.007 | Train PPL:   1.007\n",
            "\t Val. Loss: 7.826 |  Val. PPL: 2505.898\n",
            "Epoch: 29 | Time: 0m 5s\n",
            "\tTrain Loss: 0.006 | Train PPL:   1.006\n",
            "\t Val. Loss: 7.812 |  Val. PPL: 2470.280\n",
            "Epoch: 30 | Time: 0m 5s\n",
            "\tTrain Loss: 0.004 | Train PPL:   1.004\n",
            "\t Val. Loss: 7.870 |  Val. PPL: 2618.549\n",
            "Epoch: 31 | Time: 0m 5s\n",
            "\tTrain Loss: 0.004 | Train PPL:   1.004\n",
            "\t Val. Loss: 7.903 |  Val. PPL: 2704.501\n",
            "Epoch: 32 | Time: 0m 5s\n",
            "\tTrain Loss: 0.004 | Train PPL:   1.004\n",
            "\t Val. Loss: 7.912 |  Val. PPL: 2729.946\n",
            "Epoch: 33 | Time: 0m 5s\n",
            "\tTrain Loss: 0.003 | Train PPL:   1.003\n",
            "\t Val. Loss: 8.049 |  Val. PPL: 3129.378\n",
            "Epoch: 34 | Time: 0m 5s\n",
            "\tTrain Loss: 0.003 | Train PPL:   1.003\n",
            "\t Val. Loss: 8.082 |  Val. PPL: 3235.706\n",
            "Epoch: 35 | Time: 0m 5s\n",
            "\tTrain Loss: 0.003 | Train PPL:   1.003\n",
            "\t Val. Loss: 8.186 |  Val. PPL: 3589.604\n",
            "Epoch: 36 | Time: 0m 6s\n",
            "\tTrain Loss: 0.003 | Train PPL:   1.003\n",
            "\t Val. Loss: 8.255 |  Val. PPL: 3848.624\n",
            "Epoch: 37 | Time: 0m 5s\n",
            "\tTrain Loss: 0.002 | Train PPL:   1.002\n",
            "\t Val. Loss: 8.246 |  Val. PPL: 3813.844\n",
            "Epoch: 38 | Time: 0m 5s\n",
            "\tTrain Loss: 0.002 | Train PPL:   1.002\n",
            "\t Val. Loss: 8.329 |  Val. PPL: 4140.432\n",
            "Epoch: 39 | Time: 0m 5s\n",
            "\tTrain Loss: 0.002 | Train PPL:   1.002\n",
            "\t Val. Loss: 8.262 |  Val. PPL: 3875.425\n",
            "Epoch: 40 | Time: 0m 6s\n",
            "\tTrain Loss: 0.003 | Train PPL:   1.003\n",
            "\t Val. Loss: 8.421 |  Val. PPL: 4542.569\n",
            "Epoch: 41 | Time: 0m 5s\n",
            "\tTrain Loss: 0.017 | Train PPL:   1.017\n",
            "\t Val. Loss: 8.340 |  Val. PPL: 4188.542\n",
            "Epoch: 42 | Time: 0m 5s\n",
            "\tTrain Loss: 0.024 | Train PPL:   1.024\n",
            "\t Val. Loss: 8.031 |  Val. PPL: 3073.535\n",
            "Epoch: 43 | Time: 0m 5s\n",
            "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
            "\t Val. Loss: 7.649 |  Val. PPL: 2099.274\n",
            "Epoch: 44 | Time: 0m 5s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
            "\t Val. Loss: 8.000 |  Val. PPL: 2980.491\n",
            "Epoch: 45 | Time: 0m 5s\n",
            "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
            "\t Val. Loss: 8.005 |  Val. PPL: 2995.400\n",
            "Epoch: 46 | Time: 0m 5s\n",
            "\tTrain Loss: 0.046 | Train PPL:   1.047\n",
            "\t Val. Loss: 8.310 |  Val. PPL: 4064.159\n",
            "Epoch: 47 | Time: 0m 5s\n",
            "\tTrain Loss: 0.035 | Train PPL:   1.036\n",
            "\t Val. Loss: 8.486 |  Val. PPL: 4846.031\n",
            "Epoch: 48 | Time: 0m 5s\n",
            "\tTrain Loss: 0.013 | Train PPL:   1.013\n",
            "\t Val. Loss: 8.388 |  Val. PPL: 4392.270\n",
            "Epoch: 49 | Time: 0m 5s\n",
            "\tTrain Loss: 0.018 | Train PPL:   1.018\n",
            "\t Val. Loss: 8.401 |  Val. PPL: 4450.065\n",
            "Epoch: 50 | Time: 0m 5s\n",
            "\tTrain Loss: 0.006 | Train PPL:   1.006\n",
            "\t Val. Loss: 8.546 |  Val. PPL: 5143.677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Model"
      ],
      "metadata": {
        "id": "inpbzvO6-Qzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take an input sentence and translate it\n",
        "def translate_sentence(sentence, src_tokenizer, tgt_tokenizer, model, device, max_len=50):\n",
        "  model.eval()\n",
        "\n",
        "  if isinstance(sentence, str):\n",
        "    tokens = src_tokenizer.sentence_to_indices(sentence)\n",
        "  else:\n",
        "    tokens = sentence\n",
        "\n",
        "  src_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(1).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    encoder_outputs, hidden = model.encoder(src_tensor)\n",
        "\n",
        "  tgt_indices = [tgt_tokenizer.SOS_IDX]\n",
        "  for _ in range(max_len):\n",
        "    tgt_tensor = torch.tensor([tgt_indices[-1]], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output, hidden = model.decoder(tgt_tensor, hidden)\n",
        "\n",
        "    pred_token = output.argmax(1).item()\n",
        "    tgt_indices.append(pred_token)\n",
        "    if pred_token == tgt_tokenizer.EOS_IDX:\n",
        "      break\n",
        "\n",
        "  translated_sentence = tgt_tokenizer.indices_to_sentence(tgt_indices)\n",
        "  return translated_sentence\n",
        "\n",
        "example_sentences = [\n",
        "    \"hello world\",\n",
        "    \"i am hungry\",\n",
        "    \"machine translation is cool\",\n",
        "    \"this is a new sentence with unknown words\",\n",
        "    \"do you speak english\"\n",
        "]\n",
        "\n",
        "# Add some examples from validation set too\n",
        "if len(valid_data) > 0:\n",
        "    for i in range(min(3, len(valid_data))):\n",
        "        example_sentences.append(valid_data[i][0])\n",
        "\n",
        "for sentence in example_sentences:\n",
        "    translation = translate_sentence(sentence, src_tokenizer, tgt_tokenizer, model, device)\n",
        "    print(f\"Original (EN): {sentence}\")\n",
        "    # Find the ground truth if available\n",
        "    gt_fr = \"N/A\"\n",
        "    for en_s, fr_s in raw_data_pairs: # search in all raw data\n",
        "        if en_s == sentence:\n",
        "            gt_fr = fr_s\n",
        "            break\n",
        "    print(f\"Ground Truth (FR): {gt_fr}\")\n",
        "    print(f\"Translated (FR): {translation}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47pMjo7a8mAD",
        "outputId": "e97dfea0-3a83-40b1-f8b6-4f985df1486b"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original (EN): hello world\n",
            "Ground Truth (FR): bonjour le monde\n",
            "Translated (FR): bonjour le monde\n",
            "\n",
            "Original (EN): i am hungry\n",
            "Ground Truth (FR): j ai faim\n",
            "Translated (FR): j ai faim\n",
            "\n",
            "Original (EN): machine translation is cool\n",
            "Ground Truth (FR): la traduction automatique est cool\n",
            "Translated (FR): la traduction automatique est cool\n",
            "\n",
            "Original (EN): this is a new sentence with unknown words\n",
            "Ground Truth (FR): N/A\n",
            "Translated (FR): c est un bon exemple\n",
            "\n",
            "Original (EN): do you speak english\n",
            "Ground Truth (FR): parlez vous anglais\n",
            "Translated (FR): parlez vous anglais\n",
            "\n",
            "Original (EN): i would like to know\n",
            "Ground Truth (FR): je voudrais savoir\n",
            "Translated (FR): je voudrais en cafe\n",
            "\n",
            "Original (EN): what are your hobbies\n",
            "Ground Truth (FR): quels sont vos loisirs\n",
            "Translated (FR): que sont les transformers en tnl\n",
            "\n",
            "Original (EN): i am thirsty\n",
            "Ground Truth (FR): j ai soif\n",
            "Translated (FR): je suis fatigue\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
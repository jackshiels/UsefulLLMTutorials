{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackshiels/UsefulLLMTutorials/blob/main/1_EncoderDecoderModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0neRFBDbmIZs"
      },
      "source": [
        "# Encoder-Decoder Models\n",
        "\n",
        "The following tutorial explores basic encoder-decoder models and walks through building one for machine translation. For more foundational understanding, see Chapter 2 of *Large Language Models: A Deep Dive*. You can find it for under $15 here: [purchase](https://link.springer.com/book/10.1007/978-3-031-65647-7)\n",
        "\n",
        "### How Encoder-Decoder Models Work\n",
        "\n",
        "Encoder-decoder models are designed to handle **variable-length inputs and outputs**, a challenge in many natural language processing tasks. Unlike standard neural networks with fixed input/output sizes (e.g., classifiers), encoder-decoder models dynamically generate sequences of arbitrary length by learning token-by-token outputs.\n",
        "\n",
        "The **encoder** transforms an input sequence into a fixed-size (or variable-length) vector representation, called a **context vector**. During training, the model learns to encode meaningful features of the input sequence into this representation.\n",
        "\n",
        "The **decoder** takes the context vector and generates the output sequence, one token at a time, typically using **autoregression** (feeding its previous output back in as input). This process relies on a hidden state that evolves as more tokens are generated.\n",
        "\n",
        "The architecture involves two sets of hidden states:\n",
        "- One in the encoder, which processes the input tokens.\n",
        "- One in the decoder, which generates output tokens based on both the encoder's context and previously generated tokens.\n",
        "\n",
        "The predictive capability of the decoder comes from probabilistically selecting the next token, based on the formula:\n",
        "\n",
        "$\\text{softmax}(s_{t-1}, y'_{t-1}, c)$\n",
        "\n",
        "Where:\n",
        "- $\\text s_{t-1}$ is the decoder's previous hidden state  \n",
        "- $\\text y'_{t-1}$ is the previously generated token  \n",
        "- $\\text c$ is the context vector from the encoder\n",
        "\n",
        "**Softmax** transforms a vector of logits (raw model scores) into probabilities that sum to 1, enabling the model to sample or choose the most likely next token.\n",
        "\n",
        "### What this Means / TL:DR\n",
        "\n",
        "TL:DR you can train a model to learn sequence probabilities such that it can output sequences that vary in length. Here are few examples of what you could do with such a network:\n",
        "\n",
        "- Translate one language to another (a very common. use case)\n",
        "- Extract data from a sentence, such as highlighting named entities (e.g., names, nouns, locations)\n",
        "- Predict time series data\n",
        "\n",
        "The crux of an ancoder decoder model is: we have a sequence, can we predict another sequence that depends on the data in this sequence?\n",
        "\n",
        "### What this Tutorial Does\n",
        "\n",
        "In this tutorial we'll be building an English to French translator model. We'll use a very small toy dataset, so don't expect accuracy. Remember to write it out yourself, line by line. We'll start by grabbing the data from hugging face. DON'T WORRY ABOUT THE COMPLEXITY OF THIS FUNCTION, just focus on the fact that we have some training data.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upgrade the Datasets package to avoid a known bug"
      ],
      "metadata": {
        "id": "j3a46emXjBlm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sJwTmfU3Xu_o"
      },
      "outputs": [],
      "source": [
        " !pip install --upgrade datasets fsspec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Standard Imports"
      ],
      "metadata": {
        "id": "BVpDXhI7jFix"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cALWYbNEFjVV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "# Seed torch and random\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Import Hugging Face Datasets\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    HF_DATASETS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    HF_DATASETS_AVAILABLE = False\n",
        "    print(\"Hugging Face datasets library not found. Please install it: pip install datasets\")\n",
        "    print(\"Falling back to the toy dataset.\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rq8MP7-rhsv"
      },
      "source": [
        "# Toy Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QExmKJ7yrjRq"
      },
      "outputs": [],
      "source": [
        "# Define the tags for our source (SRC) and target (TGT) sequences\n",
        "SRC_LANGUAGE = 'en'\n",
        "TGT_LANGUAGE = 'fr'\n",
        "\n",
        "MAX_DATASET_SIZE_TRAIN = 2000 # Keeping it small for quick demo\n",
        "MAX_DATASET_SIZE_VALID = 200\n",
        "\n",
        "raw_data_pairs = []\n",
        "\n",
        "# Don't bother learning or recreating this function\n",
        "if HF_DATASETS_AVAILABLE:\n",
        "    try:\n",
        "        print(\"Attempting to load 'opus_books' dataset for en-fr...\")\n",
        "        hf_dataset = load_dataset(\"opus_books\", \"en-fr\", split='train')\n",
        "        print(f\"Successfully loaded 'opus_books' dataset. Original size: {len(hf_dataset)}\")\n",
        "\n",
        "        # Define the indices where we'll cut out the train and valid data\n",
        "        train_end_idx = 0\n",
        "        if MAX_DATASET_SIZE_TRAIN is not None:\n",
        "            train_end_idx = min(MAX_DATASET_SIZE_TRAIN, len(hf_dataset))\n",
        "            dataset_subset_train = hf_dataset.select(range(train_end_idx))\n",
        "        else:\n",
        "            dataset_subset_train = hf_dataset\n",
        "\n",
        "        # Validation starts where training ends\n",
        "        valid_start_idx = train_end_idx\n",
        "        valid_end_idx = valid_start_idx\n",
        "        if MAX_DATASET_SIZE_VALID is not None:\n",
        "            valid_end_idx = min(valid_start_idx + MAX_DATASET_SIZE_VALID, len(hf_dataset))\n",
        "            if valid_start_idx < valid_end_idx: # Ensure there's data left for validation\n",
        "                 dataset_subset_valid = hf_dataset.select(range(valid_start_idx, valid_end_idx))\n",
        "            else:\n",
        "                dataset_subset_valid = None\n",
        "                print(\"Not enough data for a separate validation set with current MAX_DATASET_SIZE settings.\")\n",
        "        else:\n",
        "            dataset_subset_valid = None\n",
        "\n",
        "        # Process training data\n",
        "        train_data_list = []\n",
        "        for example in dataset_subset_train:\n",
        "            src_text = example['translation'][SRC_LANGUAGE]\n",
        "            tgt_text = example['translation'][TGT_LANGUAGE]\n",
        "            if src_text and tgt_text:\n",
        "                train_data_list.append((src_text, tgt_text))\n",
        "\n",
        "        # Process validation data\n",
        "        valid_data_list = []\n",
        "        if dataset_subset_valid:\n",
        "            for example in dataset_subset_valid:\n",
        "                src_text = example['translation'][SRC_LANGUAGE]\n",
        "                tgt_text = example['translation'][TGT_LANGUAGE]\n",
        "                if src_text and tgt_text:\n",
        "                    valid_data_list.append((src_text, tgt_text))\n",
        "\n",
        "        # We apply some randomness to the data\n",
        "        random.shuffle(train_data_list)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset from Hugging Face: {e}\")\n",
        "        print(\"Falling back to the toy dataset.\")\n",
        "        HF_DATASETS_AVAILABLE = False\n",
        "\n",
        "print(f\"Total training examples: {len(train_data_list)}\")\n",
        "print(f\"Total validation examples: {len(valid_data_list)}\")\n",
        "\n",
        "if train_data_list:\n",
        "    print(\"\\nSample training data point:\")\n",
        "    print(f\"Source: {train_data_list[0][0]}\")\n",
        "    print(f\"Target: {train_data_list[0][1]}\")\n",
        "else:\n",
        "    print(\"Error: No training data available!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZiBElgSrvna"
      },
      "source": [
        "# Tokenizer\n",
        "\n",
        "What is a tokenizer? Tokenizers determine how to transform your input data into vector-ready data for the machine learning process. One way to tokenize a sentence is to simply assign unique numbers to each individual word. You could do this by counting up from 1 every time you encounter a new word and storing these values in a dictionary.\n",
        "\n",
        "Other, more complex and performant tokenizers exist. Some tokenizers will often reduce words into their 'lemmas' (base words) or 'stems' (base strings) to avoid producing overly large vocabularies. However, these tokenizer are becoming less common as models become larger.\n",
        "\n",
        "We build a simple tokenizer to introduce the concept. This tokenizer does not reduce to lemmas or stems on inputs. Instead, each unique word is given a numeric representation that counts upward as new words are added. This tokenizer will also be used to decode the model output back into french.\n",
        "\n",
        "### Token Types\n",
        "\n",
        "It is important to note *how* we deal with variable length sequences in this model. We already described how encoder-decoder models can handle variable length outputs, making them excellent for translation. However, we still need strategies around how to signal the start and end of a sentence, and how we ultimately fit these variable length sequences into the training data.\n",
        "\n",
        "We can do this with four tokens. These tokens are:\n",
        "\n",
        "- PAD: the $\\text <pad>$ token is added N times to the end of a sequence to prevent a sequence from being too short.\n",
        "- SOS: the Start of Sentence $\\text <sos>$ token signals a sequence has started.\n",
        "- EOS: the End of Sentence $\\text <eos>$ token signals a sequence has ended.\n",
        "-UNK: the unknown $\\text <unk>$ token is used for inputs that the tokenizer does not have in its vocabulary.\n",
        "\n",
        "Since our model ultimately has a maximum sequence length, we adjust the tokenizer to support a padding token, which we'll add to training data later on. For now, our tokenizer should support these four unique tokens, alongside all the token values for our training dataset.\n",
        "\n",
        "### TL:DR\n",
        "\n",
        "The tokenizer converts words to numbers, and we have some special tokens to handle padding (ensuring all inputs are the same length), start and ends of sentences (so that when we decode, we know when to stop), and an unknown token for any out-of-vocabulary inputs that are provided to our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wkhVYHUrw-6"
      },
      "outputs": [],
      "source": [
        "# Define the special tokens\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "SOS_TOKEN = \"<sos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "class CustomTokenizer:\n",
        "  def __init__(self, language_name):\n",
        "    self.language_name = language_name\n",
        "\n",
        "    # these two dictionaries are super important. We need them to get an index (token)\n",
        "    # from a word (word2index) and we need to later turn the token index back into\n",
        "    # a word again (index2word).\n",
        "    self.word2index = {}\n",
        "    self.index2word = {}\n",
        "    self.n_count = 0\n",
        "    self.word_counts = Counter()\n",
        "\n",
        "    # initialise the special tokens into the vocabulary\n",
        "    self.add_word(PAD_TOKEN)\n",
        "    self.add_word(SOS_TOKEN)\n",
        "    self.add_word(EOS_TOKEN)\n",
        "    self.add_word(UNK_TOKEN)\n",
        "\n",
        "    # add the numeric token values as class attributes for later\n",
        "    self.PAD_IDX = self.add_word(PAD_TOKEN)\n",
        "    self.SOS_IDX = self.add_word(SOS_TOKEN)\n",
        "    self.EOS_IDX = self.add_word(EOS_TOKEN)\n",
        "    self.UNK_IDX = self.add_word(UNK_TOKEN)\n",
        "\n",
        "  # a linear tokenizer (count -> index)\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_count\n",
        "      self.index2word[self.n_count] = word\n",
        "      self.n_count += 1\n",
        "    return self.word2index[word]\n",
        "\n",
        "  # clean the sentence before tokenization\n",
        "  def add_sentence(self, sentence):\n",
        "    cleaned_sentence = re.sub(r'[^a-z\\s\\']', '', sentence.lower())\n",
        "    for word in cleaned_sentence.lower().split(' '):\n",
        "      self.word_counts[word] += 1\n",
        "\n",
        "  def build_vocab(self, sentences):\n",
        "    # Build up a count for each word\n",
        "    for sentence in sentences:\n",
        "      self.add_sentence(sentence)\n",
        "\n",
        "    # Add each unique key (word) to the word2index / index2word dicts\n",
        "    for word in sorted(self.word_counts.keys()):\n",
        "      self.add_word(word)\n",
        "\n",
        "  # our outward facing methods getting both indices and sentences\n",
        "  def sentence_to_indices(self, sentence):\n",
        "    cleaned_sentence = re.sub(r'[^a-z\\s\\']', '', sentence.lower())\n",
        "    tokens = [SOS_TOKEN] + cleaned_sentence.lower().split(' ') + [EOS_TOKEN]\n",
        "    indices = [self.word2index.get(token, self.UNK_IDX) for token in tokens]\n",
        "    return indices\n",
        "\n",
        "  def indices_to_sentence(self, indices):\n",
        "    if hasattr(indices, 'tolist'):\n",
        "      indices = indices.tolist()\n",
        "    return ' '.join(self.index2word.get(index, UNK_TOKEN) for index in indices\n",
        "                    if index not in [self.SOS_IDX, self.EOS_IDX, self.PAD_IDX])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdxLLoTL8xN-"
      },
      "source": [
        "Create the tokenizers and input vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LV38c9080rP"
      },
      "outputs": [],
      "source": [
        "src_tokenizer = CustomTokenizer(SRC_LANGUAGE)\n",
        "tgt_tokenizer = CustomTokenizer(TGT_LANGUAGE)\n",
        "\n",
        "src_sentences = [pair[0] for pair in train_data]\n",
        "tgt_sentences = [pair[1] for pair in train_data]\n",
        "\n",
        "src_tokenizer.build_vocab(src_sentences)\n",
        "tgt_tokenizer.build_vocab(tgt_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjMJ73rz9aJR"
      },
      "source": [
        "Test their behaviour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkGlpW339bXF"
      },
      "outputs": [],
      "source": [
        "# Vocabulary\n",
        "print(\"\\nSource Vocabulary (EN):\")\n",
        "print(src_tokenizer.word2index)\n",
        "print(f\"PAD_IDX: {src_tokenizer.PAD_IDX}, SOS_IDX: {src_tokenizer.SOS_IDX},\"\n",
        "      f\"EOS_IDX: {src_tokenizer.EOS_IDX}, UNK_IDX: {src_tokenizer.UNK_IDX}\")\n",
        "\n",
        "print(f\"\\nTarget Vocabulary (FR)\")\n",
        "print(tgt_tokenizer.word2index)\n",
        "print(f\"PAD_IDX: {tgt_tokenizer.PAD_IDX}, SOS_IDX: {tgt_tokenizer.SOS_IDX}\"\n",
        "      f\"EOS_IDX: {tgt_tokenizer.EOS_IDX}, UNK_IDX: {tgt_tokenizer.UNK_IDX}\")\n",
        "\n",
        "# Test the tokenizer\n",
        "test_src_sent = \"hello world\"\n",
        "test_src_indices = src_tokenizer.sentence_to_indices(test_src_sent)\n",
        "print(f\"\\n'{test_src_sent}' -> {test_src_indices}\")\n",
        "print(f\"'{test_src_indices}' -> '{src_tokenizer.indices_to_sentence(test_src_indices)}'\\n\")\n",
        "\n",
        "test_tgt_sent = \"bonjour le monde\"\n",
        "test_tgt_indices = tgt_tokenizer.sentence_to_indices(test_tgt_sent)\n",
        "print(f\"'{test_tgt_sent}' -> {test_tgt_indices}\")\n",
        "print(f\"'{test_tgt_indices}' -> '{tgt_tokenizer.indices_to_sentence(test_tgt_indices)}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MHZzvHqGAhg"
      },
      "source": [
        "### Padding\n",
        "As discussed above, we need to pad the training data to ensure each entry is of equal length. We do this with the collate_fn function, which we then apply to our data in the get_data_iterator function below that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh2Jr7IbGJlp"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, src_tokenizer, tgt_tokenizer, device):\n",
        "  src_batch, tgt_batch = [], []\n",
        "  src_lens, tgt_lens = [], []\n",
        "  for src_sample, tgt_sample in batch:\n",
        "    src_indices = src_tokenizer.sentence_to_indices(src_sample)\n",
        "    tgt_indices = tgt_tokenizer.sentence_to_indices(tgt_sample)\n",
        "\n",
        "    # take the indices of this batch and create a tensor\n",
        "    # (a matrix structure used in pytorch for training / inference)\n",
        "    src_batch.append(torch.tensor(src_indices, dtype=torch.long))\n",
        "    tgt_batch.append(torch.tensor(tgt_indices, dtype=torch.long))\n",
        "\n",
        "    src_lens.append(len(src_indices))\n",
        "    tgt_lens.append(len(tgt_indices))\n",
        "\n",
        "  # pad the tensors using a utility method from torch.nn. Note the padding_value\n",
        "  # batch_first=False is a complex topic, which we'll cover later on. Note that\n",
        "  # typically, tensor dimensions START with the batch B. Here we do not structure\n",
        "  # our tensors in this way.\n",
        "  src_padded = nn.utils.rnn.pad_sequence(src_batch, padding_value=src_tokenizer.PAD_IDX, batch_first=False)\n",
        "  tgt_padded = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=tgt_tokenizer.PAD_IDX, batch_first=False)\n",
        "  src_lens = torch.tensor(src_lens)\n",
        "  tgt_lens = torch.tensor(tgt_lens)\n",
        "\n",
        "  # return on the computing device. Tensors must generally be moved onto the hardware first\n",
        "  return src_padded.to(device), tgt_padded.to(device), src_lens.to(device), tgt_lens.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WygYgAhmajh"
      },
      "source": [
        "Create a sample dataloader. A dataloader is a function that iteratively returns the training and validation data during the training loop. Note the use of a yield pattern, which returns a padded batch of data each time the training loop will call it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEFHGnySmcO-"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "def get_data_iterator(data, src_tokenizer, tgt_tokenizer, batch_size, device, shuffle=True):\n",
        "  if shuffle:\n",
        "    data_copy = list(data)\n",
        "    random.shuffle(data_copy)\n",
        "  else:\n",
        "    data_copy = data\n",
        "\n",
        "  for i in range(0, len(data_copy), batch_size):\n",
        "    batch = data_copy[i:i+batch_size]\n",
        "    yield collate_fn(batch, src_tokenizer, tgt_tokenizer, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mECBFLTvn6KC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "print(\"\\nTesting data iterator:\")\n",
        "data_iter = get_data_iterator(train_data,src_tokenizer, tgt_tokenizer, BATCH_SIZE, device)\n",
        "for i, (src_batch, tgt_batch, src_lens, tgt_lens) in enumerate(data_iter):\n",
        "  print(f\"Batch {i+1}:\")\n",
        "  print(\"Source batch shape: \", src_batch.shape)\n",
        "  print(\"target batch shape: \", tgt_batch.shape)\n",
        "  print(\"Source lengths: \", src_lens)\n",
        "  print(\"Target lengths: \", tgt_lens)\n",
        "  print(\"Source batch (first example):\\n\", src_batch[:, 0])\n",
        "  print(\"Target batch (first example):\\n\", tgt_batch[:, 0])\n",
        "  if i == 0: break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Model Components\n",
        "\n",
        "The following four code blocks build the components that make up this model. We start by defining a standard Encoder block. We'll use nn.GRU to perform the hidden state and context training. Gated recurrent unit (GRU) involves some fairly complex mathematics, which we'll implement manually at a later point. You can read more about it here: https://en.wikipedia.org/wiki/Gated_recurrent_unit\n",
        "\n",
        "The second component we'll write is the attention mechanism. Typically, a normal Encoder-Decoder (sometimes called Seq2Seq) model will just rely on its own context and hidden states to perform training. However, we can improve the accuracy of the model by implementing attention. Attention in this instance is the measure of how well the encoder output matches the decoder hidden state. The results from this measure are then added to the context vector from the encoder to feed into the decoder module. More information is provided below.\n",
        "\n",
        "Lastly, the decoder performs its sequential output with the attention-scored context from above. These three components compose the Seq2Seq class, which is the final code block. Try to pay attention to the tensor matrix shapes - how do they mutate and change as they pass through the model components. Comments indicate each mutation."
      ],
      "metadata": {
        "id": "KLSSlGp204K9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VbBms5l_zYc"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "We'll write the encoder using an nn.GRU unit. The encoder unit has several stages to its learning:\n",
        "\n",
        "* Timestep states *h* are computed as h$_{t}$ = f(h$_{t-1}$, x$_{t}$).\n",
        "* The final state h$_{t}$ is the context variable.\n",
        "* The context variable is given by some mapping *m* such that c = m(h$_{1}$, h$_{2}$, ... , h$_{t}$)\n",
        "* Encoders may be bidirectional, such that h$_{t}$ is a function of h$_{t-1}$ and h$_{t+1}$\n",
        "\n",
        "## How Does the GRU Work?\n",
        "\n",
        "The Gated Recurrent Unit (GRU) works by performing several key matrix calculations and holding 'gates' in memory. These are the following:\n",
        "\n",
        "- Update gate: decides how much of the hidden state to keep from the last refresh.\n",
        "- Reset gate: determines how much of the past to forget when computing the new candidate state.\n",
        "- Candidate hidden state: the proposed new hidden state.\n",
        "- Final hidden state: a mixture of the old hidden state and the candidate.\n",
        "\n",
        "Each of these steps is determined by using a hidden state. This hidden state is continuously updated as the model learns.\n",
        "\n",
        "The update gate is trained by taking the input of size [B, E] (where E is the embedding layer learned previously), and outputting [B, H]. All outputs in the GRU follow this shape.\n",
        "\n",
        "The reset gate is similarly shaped, and is essentially the same computation (just applied differently to the candidate hidden state).\n",
        "\n",
        "The candidate hidden state applies both of the previous gates in the identity tanh(). Some of the new data is kept, and some of it is forgotten. This helps to minimise issues where irrelevant past learned information is retained into the final hidden state.\n",
        "\n",
        "Lastly, the final hidden state is updated with some of the candidate hidden state, allowing the update to be moderated by past and new learnings.\n",
        "\n",
        "## TL:DR\n",
        "\n",
        "TL:DR: the GRU is an intelligent way of learning how embeddings should be transformed into a hidden state. These hidden states then feed into the next stage of the model, the attention decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A57ZE1x_0np"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout_p):\n",
        "    super().__init__()\n",
        "\n",
        "    # This is arbitrary, you can make it as big or small as you like (but test results)\n",
        "    self.hid_dim = hid_dim\n",
        "    # also arbitrary. 4 is a good start\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # Here is the interesting bit: input dim is the size of your vocabulary.\n",
        "    # emb_dim is the arbitrary learning layer - you can select a size and experiment.\n",
        "    # We therefore train the recurrent unit to work at a token level\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim,\n",
        "                      hid_dim,\n",
        "                      n_layers,\n",
        "                      bidirectional=True,\n",
        "                      dropout=dropout_p if n_layers > 1 else 0)\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self, src_seq):\n",
        "    embedded = self.dropout(self.embedding(src_seq))\n",
        "\n",
        "    # we get outputs, which is the hidden state at every token step, and the hidden,\n",
        "    # which is the final state of the encoder.\n",
        "    outputs, hidden = self.rnn(embedded)\n",
        "    return outputs, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZbkoyEU16bt"
      },
      "source": [
        "### Attention Module\n",
        "\n",
        "We implement an optional attention model that takes the attention weight values of the current decoder outputs in relation to the hidden layer of the encoder. This allows greater context to be embedded in the model in the subsequent attention weights, which are placed into the embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_TNW8d32dmr"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "    # enc_hid_dim_effective is the final hidden layer dimension, e.g. H * 2 for bidir\n",
        "    # dec_hid_dim is the decoder hidden dimensions\n",
        "    self.attn = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)\n",
        "    self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "  def forward(self, decoder_hidden_top_layer, encoder_outputs):\n",
        "    # decoder_hidden_top_layer = [batch_size, dec_hid_dim] as we take current only\n",
        "    # encoder outputs = [src_len, batch_size, enc_hid_dim]\n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "    hidden_repeated = decoder_hidden_top_layer.unsqueeze(1).repeat(1, src_len, 1)\n",
        "    encoder_outputs_permuted = encoder_outputs.permute(1, 0, 2)\n",
        "    energy = torch.tanh(self.attn(torch.cat((hidden_repeated, encoder_outputs_permuted), dim=2)))\n",
        "    # This doesn't damage attention [batch, src_len, 1] because it just removes the dim\n",
        "    attention_scores = self.v(energy).squeeze(2)\n",
        "    return F.softmax(attention_scores, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-pCsQ_QJpb7"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder takes the context variable from the encoder and creates its own hidden state. This hidden state is not only a function of the last hidden state, but also the previously decoded token.\n",
        "\n",
        "* s$_{t'}$ = g(s$_{t-1}$, y$_{t'-1}$, c)\n",
        "* y$_{t'}$ is a probability distribution of P(y$_{t'}$| y$_{t-1}$, ..., y$_{1}$, c) = softmax(s$_{t-1}$, y$_{t'-1}$, c).\n",
        "* In a sense, this means that the last N output tokens from the decoder influence the latest token and the current hidden state.\n",
        "\n",
        "When we compute attention, we feed the hidden states at every timestep. this gives us a very comprehensive mathematical view of the input sequence when training, allowing contextual knowledge about each individual sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K5PmNiDN6yu"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout_p, enc_hid_dim):\n",
        "    super().__init__()\n",
        "    self.output_dim = output_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_layers = n_layers\n",
        "    self.enc_hid_dim = enc_hid_dim\n",
        "\n",
        "    # Components\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.attention = Attention(self.enc_hid_dim, self.hid_dim)\n",
        "    self.rnn = nn.GRU(emb_dim * 3,\n",
        "                      hid_dim,\n",
        "                      n_layers,\n",
        "                      dropout=dropout_p if n_layers > 1 else 0)\n",
        "    self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self, input_token, hidden_state, encoder_outputs):\n",
        "    # turns [batch_size] into [1, batch_size]\n",
        "    input_token = input_token.unsqueeze(0)\n",
        "    embedded = self.dropout(self.embedding(input_token))\n",
        "\n",
        "    # decoder top hidden\n",
        "    decoder_top_hidden = hidden_state[-1]\n",
        "    a = self.attention(decoder_top_hidden, encoder_outputs)\n",
        "    a = a.unsqueeze(1)\n",
        "\n",
        "    # here we create the context from the encoder's hidden state.\n",
        "    encoder_outputs_permutated = encoder_outputs.permute(1, 0, 2)\n",
        "    context = torch.bmm(a, encoder_outputs_permutated)\n",
        "    context = context.permute(1, 0, 2)\n",
        "    rnn_input = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "    # Per token decoding\n",
        "    output, new_hidden_state = self.rnn(rnn_input, hidden_state)\n",
        "\n",
        "    prediction = self.fc_out(output.squeeze(0))\n",
        "\n",
        "    return prediction, new_hidden_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY-91VeMfOcM"
      },
      "source": [
        "### Seq2Seq Implementation\n",
        "\n",
        "The Seq2Seq component implements and handles the encoder/decoder architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dy3R35PfXC_"
      },
      "outputs": [],
      "source": [
        "def resize_bidirectional_hidden(n_layers, batch_size, hid, enc_hidden, bridge):\n",
        "  enc_hidden = enc_hidden.view(n_layers, 2, batch_size, hid)\n",
        "  cat = torch.cat((enc_hidden[:, 0, :, :], enc_hidden[:, 1, :, :]), dim=2)\n",
        "  return torch.tanh(bridge(cat))\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "    enc_bridge_input_dim = self.encoder.hid_dim * 2\n",
        "    self.bridge = nn.Linear(enc_bridge_input_dim, self.decoder.hid_dim)\n",
        "\n",
        "    # Sanity check for dimensionality\n",
        "    assert encoder.hid_dim == decoder.hid_dim, \"hidden dims must be equal\"\n",
        "    assert encoder.n_layers == decoder.n_layers, \"layers must be equal\"\n",
        "\n",
        "  def _init_decoder_hidden(self, enc_hidden):\n",
        "    n_layers, batch_size, hid = self.encoder.n_layers, enc_hidden.size(1), self.encoder.hid_dim\n",
        "    return resize_bidirectional_hidden(n_layers, batch_size, hid, enc_hidden, self.bridge)\n",
        "\n",
        "  def forward(self, src_seq, tgt_seq, teacher_forcing_ratio=0.5):\n",
        "    batch_size = src_seq.shape[1]\n",
        "    tgt_len = tgt_seq.shape[0]\n",
        "    tgt_vocab_size = self.decoder.output_dim\n",
        "    outputs = torch.zeros(tgt_len, batch_size, tgt_vocab_size).to(self.device)\n",
        "\n",
        "    # Encode\n",
        "    enc_out, hidden = self.encoder(src_seq)\n",
        "    hidden = self._init_decoder_hidden(hidden)\n",
        "\n",
        "    # Decode\n",
        "    dec_in = tgt_seq[0, :]\n",
        "    for t in range(1, tgt_len):\n",
        "      dec_out, hidden = self.decoder(dec_in, hidden, enc_out)\n",
        "      outputs[t] = dec_out\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "      top1 = dec_out.argmax(1)\n",
        "      dec_in = tgt_seq[t] if teacher_force else top1\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QckUDH8kcOis"
      },
      "source": [
        "# Training\n",
        "\n",
        "Now, we perform the standard training process. By now, this format should be familiar to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cquq3cUbcP-M"
      },
      "outputs": [],
      "source": [
        "# hyperparams\n",
        "INPUT_DIM = src_tokenizer.n_count\n",
        "OUTPUT_DIM = tgt_tokenizer.n_count\n",
        "ENC_EMB_DIM = 32\n",
        "DEC_EMB_DIM = 32\n",
        "HID_DIM = 32\n",
        "EFFECTIVE_ENC_HID_DIM = HID_DIM * 2\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "LEARNING_RATE = 0.001\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "BIDIR_MODEL_NAME = \"language_enc_dec_bidir_attn.pt\"\n",
        "UNIDIR_MODEL_NAME = \"language_enc_dec.pt\"\n",
        "MODEL_NAME = BIDIR_MODEL_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDs8YgZlweKi"
      },
      "outputs": [],
      "source": [
        "# components\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, EFFECTIVE_ENC_HID_DIM).to(device)\n",
        "model_bidir = Seq2Seq(enc, dec, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwoiHd_0w1si"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model_bidir):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmF7TUbzxEcT"
      },
      "outputs": [],
      "source": [
        "# optim and learn\n",
        "optimizer = optim.Adam(model_bidir.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = tgt_tokenizer.PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brGpMZ6AxQ22"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRLTMnN8yG2p"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    batch_n = 0\n",
        "    for i, (src, tgt, _, _) in enumerate(iterator): # src_lens, tgt_lens not used directly here\n",
        "        batch_n += 1\n",
        "        optimizer.zero_grad()\n",
        "        # output = [tgt_len, batch_size, output_vocab_size]\n",
        "        output = model(src, tgt)\n",
        "        # get vocab length for next step\n",
        "        output_dim = output.shape[-1]\n",
        "        # remove <sos> tag by enforcing [1:]\n",
        "        # turn [tgt_len, batch, vocab] into [(tgt_len-1 * batch), vocab] so it fits into loss\n",
        "        output_flat = output[1:].view(-1, output_dim)\n",
        "        # since we know the vocab, this doesn't have the V dimension [tgt_len-1, batch]\n",
        "        tgt_flat = tgt[1:].view(-1)\n",
        "        # now that they are equal dim, compute the loss between out and tgt\n",
        "        loss = criterion(output_flat, tgt_flat)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / batch_n\n",
        "\n",
        "def evaluate_epoch(model, iterator, criterion):\n",
        "  # the same setup, but we loop without adam use\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  batch_n = 0\n",
        "  with torch.no_grad():\n",
        "    for i, (src, tgt, _, _) in enumerate(iterator):\n",
        "      batch_n += 1\n",
        "      output = model(src, tgt, 0)\n",
        "      output_dim = output.shape[-1]\n",
        "      output_flat = output[1:].view(-1, output_dim)\n",
        "      tgt_flat = tgt[1:].view(-1)\n",
        "      loss = criterion(output_flat, tgt_flat)\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss / batch_n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDWtJKaM2hfr"
      },
      "source": [
        "### Train Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8PSyI3EJ2jNs",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start_time = time.time()\n",
        "  train_iter = get_data_iterator(train_data,\n",
        "                               src_tokenizer,\n",
        "                               tgt_tokenizer,\n",
        "                               BATCH_SIZE,\n",
        "                               device,\n",
        "                               shuffle=True)\n",
        "  valid_iter = get_data_iterator(valid_data,\n",
        "                               src_tokenizer,\n",
        "                               tgt_tokenizer,\n",
        "                               BATCH_SIZE,\n",
        "                               device,\n",
        "                               shuffle=False)\n",
        "  train_loss = train_epoch(model_bidir, train_iter, optimizer, criterion, CLIP)\n",
        "  valid_loss = evaluate_epoch(model_bidir, valid_iter, criterion)\n",
        "\n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "torch.save(model_bidir.state_dict(), MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inpbzvO6-Qzs"
      },
      "source": [
        "### Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9V4cY_4e4Izk"
      },
      "outputs": [],
      "source": [
        "model_bidir.load_state_dict(torch.load(MODEL_NAME, weights_only=True))\n",
        "model_bidir.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvbxTxaHDb_r"
      },
      "source": [
        "Create the beam search component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "47pMjo7a8mAD"
      },
      "outputs": [],
      "source": [
        "class BeamHypothesis:\n",
        "  def __init__(self, tokens, log_prob, hidden_state):\n",
        "    self.tokens = tokens\n",
        "    self.log_prob = log_prob\n",
        "    self.hidden_state = hidden_state\n",
        "\n",
        "  def extend(self, token_idx, log_prob_token, new_hidden_state):\n",
        "    return BeamHypothesis(\n",
        "        self.tokens + [token_idx],\n",
        "        self.log_prob + log_prob_token,\n",
        "        new_hidden_state\n",
        "    )\n",
        "\n",
        "  @property\n",
        "  def latest_token(self):\n",
        "    return self.tokens[-1]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens)\n",
        "\n",
        "  def __lt__(self, other):\n",
        "    return self.log_prob < other.log_prob\n",
        "\n",
        "  def normalized_score(self, alpha=0.6):\n",
        "    if not self.tokens or len(self.tokens) == 0:\n",
        "      return float('inf')\n",
        "\n",
        "    penalty = ((5 + len(self.tokens)) / (5 + 1)) ** alpha\n",
        "    return self.log_prob / penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xko9aBRK_tH"
      },
      "source": [
        "Build the translate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OpzalzzeLC67"
      },
      "outputs": [],
      "source": [
        "def beam_search_translate(sentence,\n",
        "                          src_tokenizer,\n",
        "                          tgt_tokenizer,\n",
        "                          model,\n",
        "                          device,\n",
        "                          beam_width=3,\n",
        "                          max_len=50,\n",
        "                          len_penalty_alpha=0.6):\n",
        "  model.eval()\n",
        "\n",
        "  # check inputs\n",
        "  if isinstance(sentence, str):\n",
        "    src_tokens = src_tokenizer.sentence_to_indices(sentence)\n",
        "  else:\n",
        "    src_tokens = sentence\n",
        "\n",
        "  # unsqueeze, since the model expects [src_len, batch_size] as input.\n",
        "  # this produces [[n], [n], [n]], where n = 1 since this is 1 sentence.\n",
        "  src_tensor = torch.tensor(src_tokens, dtype=torch.long).unsqueeze(1).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    enc_outputs, hidden = model.encoder(src_tensor)\n",
        "    decoder_hidden = model._init_decoder_hidden(hidden)\n",
        "\n",
        "  # initialize the beams\n",
        "  initial_beam = BeamHypothesis(\n",
        "      tokens=[tgt_tokenizer.SOS_IDX],\n",
        "      log_prob=0.0,\n",
        "      hidden_state=decoder_hidden\n",
        "  )\n",
        "  active_beams = [initial_beam]\n",
        "  completed_hypotheses = []\n",
        "\n",
        "  # beam search loop\n",
        "  for step in range(max_len):\n",
        "    if not active_beams:\n",
        "      break\n",
        "\n",
        "    new_potential_beams = []\n",
        "    temp_active_beams_after_eos_check = []\n",
        "\n",
        "    for beam in active_beams:\n",
        "      # when a sentence ends, add it to the completions\n",
        "      if beam.latest_token == tgt_tokenizer.EOS_IDX:\n",
        "        completed_hypotheses.append(beam)\n",
        "      else:\n",
        "        temp_active_beams_after_eos_check.append(beam)\n",
        "\n",
        "    active_beams = temp_active_beams_after_eos_check\n",
        "    if not active_beams:\n",
        "      break\n",
        "\n",
        "    for beam in active_beams:\n",
        "      tgt_input_tokens = torch.tensor([beam.latest_token], dtype=torch.long).to(device)\n",
        "\n",
        "      # decode next token\n",
        "      with torch.no_grad():\n",
        "        decoder_output_logits, new_hidden_state = model.decoder(\n",
        "            tgt_input_tokens,\n",
        "            beam.hidden_state,\n",
        "            enc_outputs\n",
        "        )\n",
        "\n",
        "        # now get the top k for this decoded output\n",
        "        logits = decoder_output_logits[0]                   # → shape [vocab_size]\n",
        "        log_probs_next_token = F.log_softmax(logits, dim=-1)\n",
        "        top_k_log_probs, top_k_indices = torch.topk(log_probs_next_token, beam_width)\n",
        "\n",
        "        # create a flat list with all potential k tokens for active beams\n",
        "        for i in range(beam_width):\n",
        "          next_token_idx = top_k_indices[i].item()\n",
        "          log_prob_of_next = top_k_log_probs[i].item()\n",
        "\n",
        "          # create the extended hypothesis\n",
        "          hidden_clone = new_hidden_state.clone()\n",
        "          extended_beam = beam.extend(next_token_idx, log_prob_of_next, hidden_clone)\n",
        "          new_potential_beams.append(extended_beam)\n",
        "\n",
        "    # will be empty if all ended with EOS\n",
        "    if not new_potential_beams:\n",
        "        break\n",
        "\n",
        "    # take beams with top k\n",
        "    active_beams = sorted(new_potential_beams, key=lambda x: x.normalized_score(len_penalty_alpha), reverse=True)[:beam_width]\n",
        "\n",
        "  completed_hypotheses.extend(active_beams)\n",
        "  if not completed_hypotheses:\n",
        "    return \"error\"\n",
        "\n",
        "  # perform the penalty and select\n",
        "  completed_hypotheses.sort(key=lambda x: x.normalized_score(len_penalty_alpha), reverse=True)\n",
        "  best_hypothesis = completed_hypotheses[0]\n",
        "\n",
        "  translated_tokens = best_hypothesis.tokens\n",
        "  if translated_tokens and translated_tokens[0] == tgt_tokenizer.SOS_IDX:\n",
        "    translated_tokens = translated_tokens[1:]\n",
        "  if translated_tokens and translated_tokens[-1] == tgt_tokenizer.EOS_IDX:\n",
        "    translated_tokens = translated_tokens[:-1]\n",
        "\n",
        "  translated_sentence = tgt_tokenizer.indices_to_sentence(translated_tokens)\n",
        "  return translated_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv-7s6AO0lC-"
      },
      "source": [
        "Implement BLEU to test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DZkPxmbxwQSq"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "\n",
        "def ngrams(seq: List[str], n: int) -> List[tuple[str, ...]]:\n",
        "  return [tuple(seq[i:i+n]) for i in range(len(seq) - n + 1)]\n",
        "\n",
        "def modified_precision(candidate: List[str],\n",
        "                       references: List[List[str]],\n",
        "                       n: int):\n",
        "  cand_ngrams = Counter(ngrams(candidate, n))\n",
        "  max_reference_counts = Counter()\n",
        "\n",
        "  for ref in references:\n",
        "    ref_counts = Counter(ngrams(ref, n))\n",
        "    for ngram, count in ref_counts.items():\n",
        "      max_reference_counts[ngram] = max(max_reference_counts[ngram], count)\n",
        "\n",
        "  clipped_counts = {ngram: min(count, max_reference_counts[ngram])\n",
        "    for ngram, count in cand_ngrams.items()}\n",
        "\n",
        "  numerator = sum(clipped_counts.values())\n",
        "  denominator = sum(cand_ngrams.values())\n",
        "\n",
        "  return numerator, denominator\n",
        "\n",
        "def brevity_penalty(c: int, r: int) -> float:\n",
        "  return 1.0 if c > r else math.exp(1 - r / c)\n",
        "\n",
        "def closest_ref_len(c: int, ref_lens: List[int]) -> int:\n",
        "  return min(ref_lens, key=lambda rl: (abs(rl - c), rl))\n",
        "\n",
        "def bleu(candidate: List[str],\n",
        "         references: List[List[str]],\n",
        "         max_n: int = 4) -> float:\n",
        "  weights = [1/max_n] * max_n\n",
        "  precisions = []\n",
        "\n",
        "  for n in range(1, max_n+1):\n",
        "    num, den = modified_precision(candidate, references, n)\n",
        "    if num == 0:\n",
        "      num, den = 1, 2\n",
        "    precisions.append((num, den))\n",
        "\n",
        "  geo_mean = math.exp(sum(w * math.log(num/den)\n",
        "    for (num, den), w in zip(precisions, weights)))\n",
        "\n",
        "  c = len(candidate)\n",
        "  r = closest_ref_len(c, [len(r) for r in references])\n",
        "  bp = brevity_penalty(c, r)\n",
        "  return bp * geo_mean\n",
        "\n",
        "def tokenize_sequence(sequence):\n",
        "  return sequence.lower().split(' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vLl4yNhd9XzS"
      },
      "outputs": [],
      "source": [
        "# get BLEU candidates / references\n",
        "references_bidir = [tokenize_sequence(text_pair[1]) for text_pair in valid_data]\n",
        "candidates_bidir = [tokenize_sequence(beam_search_translate(text_pair[0], src_tokenizer, tgt_tokenizer, model_bidir, device, beam_width=3, max_len=100)) for text_pair in valid_data]\n",
        "print(references_bidir)\n",
        "print(candidates_bidir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oOna1djQDWzd"
      },
      "outputs": [],
      "source": [
        "bleu_results = []\n",
        "for candidate in candidates_bidir:\n",
        "  bleu_results.append(bleu(candidate, references_bidir))\n",
        "\n",
        "print(bleu_results)\n",
        "sum_of_bleu = sum(bleu_results)\n",
        "len_of_bleu = len(bleu_results)\n",
        "print(f'The average bidir BLEU is: {sum_of_bleu / len_of_bleu}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "QaeIMp03wPfe"
      },
      "outputs": [],
      "source": [
        "for text_valid in valid_data:\n",
        "    translation = translate_sentence(text_valid[0], src_tokenizer, tgt_tokenizer, model_bidir, device)\n",
        "    print(f\"Original (EN): {text_valid}\")\n",
        "    # Find the ground truth if available\n",
        "    gt_fr = \"N/A\"\n",
        "    for en_s, fr_s in raw_data_pairs: # search in all raw data\n",
        "        if en_s == text_valid[0]:\n",
        "            gt_fr = fr_s\n",
        "            break\n",
        "    print(f\"Ground Truth (FR): {gt_fr}\")\n",
        "    print(f\"Translated (FR): {translation}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAWdVOfH64RyEr8dCyjynL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
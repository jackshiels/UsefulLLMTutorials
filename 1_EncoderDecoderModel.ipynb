{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_rq8MP7-rhsv"
      ],
      "authorship_tag": "ABX9TyPJN+RXznK86DlXFRYYu7k2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackshiels/UsefulLLMTutorials/blob/main/1_EncoderDecoderModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder-Decoder Models\n",
        "\n",
        "The following tutorial takes a look at basic encoder-decoder and generates a model for machine translation. The recommended reading for this tutorial is Chapter 2 of Large Language Models: A Deep Dive. You can find it here for under $15: [purchase](https://link.springer.com/book/10.1007/978-3-031-65647-7)\n",
        "\n",
        "We will be implementing the torch Gated Recurrent Unit (GRU), which is a choice against using the traditional Long Short-Term Memory (LSTM) model. By the end of the softmax layer, we are implementing greedy searching for tokens. An alternative attention approach is provided, too."
      ],
      "metadata": {
        "id": "0neRFBDbmIZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "# Seed torch and random\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device {device}')"
      ],
      "metadata": {
        "id": "cALWYbNEFjVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a65c57d-e21f-43b8-85aa-8176530595e0"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Options for training\n",
        "bidirectional_gru = True"
      ],
      "metadata": {
        "id": "l-qXgnZQJ8gR"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Toy Dataset"
      ],
      "metadata": {
        "id": "_rq8MP7-rhsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_LANGUAGE = 'en'\n",
        "TGT_LANGUAGE = 'fr'\n",
        "\n",
        "raw_data_pairs = [\n",
        "    (\"hello world\", \"bonjour le monde\"),\n",
        "    (\"how are you\", \"comment allez vous\"),\n",
        "    (\"i am fine\", \"je vais bien\"),\n",
        "    (\"good morning\", \"bonjour\"),\n",
        "    (\"thank you\", \"merci\"),\n",
        "    (\"i love pytorch\", \"j aime pytorch\"),\n",
        "    (\"machine translation is cool\", \"la traduction automatique est cool\"),\n",
        "    (\"see you later\", \"a plus tard\"),\n",
        "    (\"what is your name\", \"quel est votre nom\"),\n",
        "    (\"my name is model\", \"mon nom est modele\"),\n",
        "    (\"nice to meet you\", \"ravi de vous rencontrer\"),\n",
        "    (\"good night\", \"bonne nuit\"),\n",
        "    (\"have a nice day\", \"bonne journee\"),\n",
        "    (\"where are you from\", \"d ou venez vous\"),\n",
        "    (\"i am from canada\", \"je viens du canada\"),\n",
        "    (\"do you speak english\", \"parlez vous anglais\"),\n",
        "    (\"yes i speak english\", \"oui je parle anglais\"),\n",
        "    (\"no i don't understand\", \"non je ne comprends pas\"),\n",
        "    (\"can you help me\", \"pouvez vous m aider\"),\n",
        "    (\"i need assistance\", \"j ai besoin d aide\"),\n",
        "    (\"what time is it\", \"quelle heure est il\"),\n",
        "    (\"it is five o'clock\", \"il est cinq heures\"),\n",
        "    (\"where is the station\", \"ou est la gare\"),\n",
        "    (\"i am learning french\", \"j apprends le francais\"),\n",
        "    (\"this is a cat\", \"c est un chat\"),\n",
        "    (\"that is a dog\", \"c est un chien\"),\n",
        "    (\"the weather is nice\", \"il fait beau\"),\n",
        "    (\"it is raining\", \"il pleut\"),\n",
        "    (\"i am hungry\", \"j ai faim\"),\n",
        "    (\"i am thirsty\", \"j ai soif\"),\n",
        "    (\"let's go\", \"allons y\"),\n",
        "    (\"come here\", \"viens ici\"),\n",
        "    (\"open the door\", \"ouvre la porte\"),\n",
        "    (\"close the window\", \"ferme la fenetre\"),\n",
        "    (\"i am tired\", \"je suis fatigue\"),\n",
        "    (\"i don't know\", \"je ne sais pas\"),\n",
        "    (\"i agree\", \"je suis d accord\"),\n",
        "    (\"i disagree\", \"je ne suis pas d accord\"),\n",
        "    (\"i like this song\", \"j aime cette chanson\"),\n",
        "    (\"this is my friend\", \"c est mon ami\"),\n",
        "    (\"do you like coffee\", \"aimes tu le cafe\"),\n",
        "    (\"yes i like coffee\", \"oui j aime le cafe\"),\n",
        "    (\"no i prefer tea\", \"non je prefere le the\"),\n",
        "    (\"what do you do\", \"que fais tu dans la vie\"),\n",
        "    (\"i am a student\", \"je suis etudiant\"),\n",
        "    (\"i am a teacher\", \"je suis professeur\"),\n",
        "    (\"deep learning is interesting\", \"l apprentissage profond est interessant\"),\n",
        "    (\"we are training a model\", \"nous entrainons un modele\"),\n",
        "    (\"this dataset is large\", \"ce jeu de donnees est grand\"),\n",
        "    (\"the model is overfitting\", \"le modele fait du surapprentissage\"),\n",
        "    (\"we need more data\", \"nous avons besoin de plus de donnees\"),\n",
        "    (\"good afternoon\", \"bon apres-midi\"),\n",
        "    (\"good evening\", \"bonsoir\"),\n",
        "    (\"excuse me\", \"excusez-moi\"),\n",
        "    (\"please\", \"s il vous plait\"),\n",
        "    (\"you're welcome\", \"de rien\"),\n",
        "    (\"how much does it cost\", \"combien ca coute\"),\n",
        "    (\"where is the bathroom\", \"ou sont les toilettes\"),\n",
        "    (\"i need a doctor\", \"j ai besoin d un medecin\"),\n",
        "    (\"i am lost\", \"je suis perdu\"),\n",
        "    (\"can you repeat that\", \"pouvez vous repeter ca\"),\n",
        "    (\"speak slower please\", \"parlez plus lentement s il vous plait\"),\n",
        "    (\"write it down\", \"ecrivez le\"),\n",
        "    (\"what is this\", \"qu est ce que c est\"),\n",
        "    (\"how do you say this in french\", \"comment dit on ca en francais\"),\n",
        "    (\"i understand\", \"je comprends\"),\n",
        "    (\"i don't understand french\", \"je ne comprends pas le francais\"),\n",
        "    (\"it's too expensive\", \"c est trop cher\"),\n",
        "    (\"i'll take it\", \"je le prends\"),\n",
        "    (\"where is the exit\", \"ou est la sortie\"),\n",
        "    (\"where is the entrance\", \"ou est l entree\"),\n",
        "    (\"is there a restaurant nearby\", \"y a t il un restaurant a proximite\"),\n",
        "    (\"i want to eat\", \"je veux manger\"),\n",
        "    (\"i want to drink\", \"je veux boire\"),\n",
        "    (\"the bill please\", \"l addition s il vous plait\"),\n",
        "    (\"it was delicious\", \"c etait delicieux\"),\n",
        "    (\"i would like water\", \"je voudrais de l eau\"),\n",
        "    (\"i would like a coffee\", \"je voudrais un cafe\"),\n",
        "    (\"i would like a beer\", \"je voudrais une biere\"),\n",
        "    (\"do you have a table for two\", \"avez vous une table pour deux\"),\n",
        "    (\"i have a reservation\", \"j ai une reservation\"),\n",
        "    (\"what is the weather like\", \"quel temps fait il\"),\n",
        "    (\"it is sunny\", \"il fait soleil\"),\n",
        "    (\"it is cloudy\", \"il fait nuageux\"),\n",
        "    (\"it is cold\", \"il fait froid\"),\n",
        "    (\"it is hot\", \"il fait chaud\"),\n",
        "    (\"it is windy\", \"il fait du vent\"),\n",
        "    (\"what day is it today\", \"quel jour sommes nous aujourd hui\"),\n",
        "    (\"today is monday\", \"aujourd hui c est lundi\"),\n",
        "    (\"tomorrow is tuesday\", \"demain c est mardi\"),\n",
        "    (\"yesterday was sunday\", \"hier c etait dimanche\"),\n",
        "    (\"see you soon\", \"a bientot\"),\n",
        "    (\"have a good trip\", \"bon voyage\"),\n",
        "    (\"be careful\", \"fais attention\"),\n",
        "    (\"no problem\", \"pas de probleme\"),\n",
        "    (\"i am busy\", \"je suis occupe\"),\n",
        "    (\"i am happy\", \"je suis content\"),\n",
        "    (\"i am sad\", \"je suis triste\"),\n",
        "    (\"i am bored\", \"je m ennuie\"),\n",
        "    (\"i am excited\", \"je suis excite\"),\n",
        "    (\"i am sick\", \"je suis malade\"),\n",
        "    (\"i have a headache\", \"j ai mal a la tete\"),\n",
        "    (\"i have a stomach ache\", \"j ai mal au ventre\"),\n",
        "    (\"i feel good\", \"je me sens bien\"),\n",
        "    (\"i feel bad\", \"je me sens mal\"),\n",
        "    (\"what time do you open\", \"a quelle heure ouvrez vous\"),\n",
        "    (\"what time do you close\", \"a quelle heure fermez vous\"),\n",
        "    (\"is it open\", \"est ce ouvert\"),\n",
        "    (\"is it closed\", \"est ce ferme\"),\n",
        "    (\"can i pay by card\", \"puis je payer par carte\"),\n",
        "    (\"can i pay cash\", \"puis je payer en especes\"),\n",
        "    (\"where is the bank\", \"ou est la banque\"),\n",
        "    (\"where is the post office\", \"ou est la poste\"),\n",
        "    (\"how far is it\", \"a quelle distance est ce\"),\n",
        "    (\"it is far\", \"c est loin\"),\n",
        "    (\"it is near\", \"c est pres\"),\n",
        "    (\"turn left\", \"tournez a gauche\"),\n",
        "    (\"turn right\", \"tournez a droite\"),\n",
        "    (\"go straight ahead\", \"allez tout droit\"),\n",
        "    (\"stop here\", \"arretez vous ici\"),\n",
        "    (\"take me to this address\", \"emmenez moi a cette adresse\"),\n",
        "    (\"i want a ticket to paris\", \"je veux un billet pour paris\"),\n",
        "    (\"one way or round trip\", \"aller simple ou aller retour\"),\n",
        "    (\"how long does it take\", \"combien de temps ca prend\"),\n",
        "    (\"when does the train leave\", \"quand part le train\"),\n",
        "    (\"when does the bus arrive\", \"quand arrive le bus\"),\n",
        "    (\"i am here on vacation\", \"je suis ici en vacances\"),\n",
        "    (\"i am here for work\", \"je suis ici pour le travail\"),\n",
        "    (\"i like france\", \"j aime la france\"),\n",
        "    (\"i don't like it\", \"je n aime pas ca\"),\n",
        "    (\"can i try it on\", \"puis je l essayer\"),\n",
        "    (\"what size is this\", \"quelle taille est ce\"),\n",
        "    (\"do you have a bigger size\", \"avez vous une taille plus grande\"),\n",
        "    (\"do you have a smaller size\", \"avez vous une taille plus petite\"),\n",
        "    (\"i need help with my luggage\", \"j ai besoin d aide avec mes bagages\"),\n",
        "    (\"where is the information desk\", \"ou est le bureau d information\"),\n",
        "    (\"what's your phone number\", \"quel est votre numero de telephone\"),\n",
        "    (\"what's your email address\", \"quelle est votre adresse e-mail\"),\n",
        "    (\"can i call you\", \"puis je vous appeler\"),\n",
        "    (\"please wait\", \"veuillez patienter\"),\n",
        "    (\"come in\", \"entrez\"),\n",
        "    (\"sit down\", \"asseyez vous\"),\n",
        "    (\"stand up\", \"levez vous\"),\n",
        "    (\"listen to me\", \"ecoutez moi\"),\n",
        "    (\"look at this\", \"regardez ca\"),\n",
        "    (\"i am learning a lot\", \"j apprends beaucoup\"),\n",
        "    (\"it is difficult\", \"c est difficile\"),\n",
        "    (\"it is easy\", \"c est facile\"),\n",
        "    (\"it is very interesting\", \"c est tres interessant\"),\n",
        "    (\"i need more practice\", \"j ai besoin de plus de pratique\"),\n",
        "    (\"what are you doing\", \"que faites vous\"),\n",
        "    (\"i am reading a book\", \"je lis un livre\"),\n",
        "    (\"i am watching tv\", \"je regarde la tele\"),\n",
        "    (\"i am listening to music\", \"j ecoute de la musique\"),\n",
        "    (\"i am cooking\", \"je cuisine\"),\n",
        "    (\"i am working\", \"je travaille\"),\n",
        "    (\"i am studying\", \"j etudie\"),\n",
        "    (\"i am going home\", \"je rentre a la maison\"),\n",
        "    (\"i am going to bed\", \"je vais me coucher\"),\n",
        "    (\"i am waking up\", \"je me reveille\"),\n",
        "    (\"have a good meal\", \"bon appetit\"),\n",
        "    (\"cheers\", \"sante\"),\n",
        "    (\"happy birthday\", \"joyeux anniversaire\"),\n",
        "    (\"merry christmas\", \"joyeux noel\"),\n",
        "    (\"happy new year\", \"bonne annee\"),\n",
        "    (\"congratulations\", \"felicitations\"),\n",
        "    (\"good luck\", \"bonne chance\"),\n",
        "    (\"i am sorry\", \"je suis desole\"),\n",
        "    (\"it's okay\", \"c est bon\"),\n",
        "    (\"never mind\", \"laisse tomber\"),\n",
        "    (\"i totally agree\", \"je suis entierement d accord\"),\n",
        "    (\"i think so\", \"je pense que oui\"),\n",
        "    (\"i don't think so\", \"je ne pense pas\"),\n",
        "    (\"it's important\", \"c est important\"),\n",
        "    (\"it's urgent\", \"c est urgent\"),\n",
        "    (\"i need help with this exercise\", \"j ai besoin d aide pour cet exercice\"),\n",
        "    (\"this is a challenging problem\", \"c est un probleme difficile\"),\n",
        "    (\"we need to optimize the code\", \"nous devons optimiser le code\"),\n",
        "    (\"the algorithm is complex\", \"l algorithme est complexe\"),\n",
        "    (\"what is a neural network\", \"qu est ce qu un reseau de neurones\"),\n",
        "    (\"how does backpropagation work\", \"comment fonctionne la retropropagation\"),\n",
        "    (\"we are collecting more data\", \"nous collectons plus de donnees\"),\n",
        "    (\"the training loss is decreasing\", \"la perte d entrainement diminue\"),\n",
        "    (\"the validation accuracy is stable\", \"la precision de validation est stable\"),\n",
        "    (\"we need to adjust the hyperparameters\", \"nous devons ajuster les hyperparametres\"),\n",
        "    (\"this model is production-ready\", \"ce modele est pret pour la production\"),\n",
        "    (\"data preprocessing is crucial\", \"le pre-traitement des donnees est crucial\"),\n",
        "    (\"feature engineering is important\", \"l ingenierie des caracteristiques est importante\"),\n",
        "    (\"we are debugging the script\", \"nous deboguons le script\"),\n",
        "    (\"what is the learning rate\", \"quel est le taux d apprentissage\"),\n",
        "    (\"gradient descent is an optimization algorithm\", \"la descente de gradient est un algorithme d optimisation\"),\n",
        "    (\"we use GPUs for faster training\", \"nous utilisons des GPU pour un entrainement plus rapide\"),\n",
        "    (\"this is a classification task\", \"c est une tache de classification\"),\n",
        "    (\"this is a regression task\", \"c est une tache de regression\"),\n",
        "    (\"we need to fine-tune the model\", \"nous devons affiner le modele\"),\n",
        "    (\"transfer learning is effective\", \"l apprentissage par transfert est efficace\"),\n",
        "    (\"explain the attention mechanism\", \"expliquez le mecanisme d attention\"),\n",
        "    (\"what are transformers in nlp\", \"que sont les transformers en tnl\"),\n",
        "    (\"the model predicts the next word\", \"le modele predit le mot suivant\"),\n",
        "    (\"it's an end-to-end system\", \"c est un systeme de bout en bout\"),\n",
        "    (\"we are evaluating the performance\", \"nous evaluons la performance\"),\n",
        "    (\"the results are promising\", \"les resultats sont prometteurs\"),\n",
        "    (\"we need to document the code\", \"nous devons documenter le code\"),\n",
        "    (\"version control is essential\", \"le controle de version est essentiel\"),\n",
        "    (\"what is your favorite programming language\", \"quel est votre langage de programmation prefere\"),\n",
        "    (\"i prefer python\", \"je prefere python\"),\n",
        "    (\"this is a good example\", \"c est un bon exemple\"),\n",
        "    (\"it's a difficult question\", \"c est une question difficile\"),\n",
        "    (\"i'm thinking about it\", \"j y reflechis\"),\n",
        "    (\"can you explain more\", \"pouvez vous expliquer plus\"),\n",
        "    (\"i agree with you\", \"je suis d accord avec vous\"),\n",
        "    (\"i understand what you mean\", \"je comprends ce que vous voulez dire\"),\n",
        "    (\"how was your day\", \"comment etait ta journee\"),\n",
        "    (\"it was good\", \"c etait bien\"),\n",
        "    (\"it was bad\", \"c etait mauvais\"),\n",
        "    (\"i had a busy day\", \"j ai eu une journee occupee\"),\n",
        "    (\"what are your hobbies\", \"quels sont vos loisirs\"),\n",
        "    (\"i like to travel\", \"j aime voyager\"),\n",
        "    (\"i like to read\", \"j aime lire\"),\n",
        "    (\"i like to cook\", \"j aime cuisiner\"),\n",
        "    (\"what is your favorite food\", \"quel est votre plat prefere\"),\n",
        "    (\"i like french food\", \"j aime la cuisine francaise\"),\n",
        "    (\"can you recommend a good book\", \"pouvez vous me recommander un bon livre\"),\n",
        "    (\"i will try my best\", \"je ferai de mon mieux\"),\n",
        "    (\"i hope so\", \"j espere que oui\"),\n",
        "    (\"i hope not\", \"j espere que non\"),\n",
        "    (\"it's a pleasure\", \"c est un plaisir\"),\n",
        "    (\"take care\", \"prends soin de toi\"),\n",
        "    (\"what's new\", \"quoi de neuf\"),\n",
        "    (\"nothing much\", \"pas grand chose\"),\n",
        "    (\"do you have any questions\", \"avez vous des questions\"),\n",
        "    (\"i have no questions\", \"je n ai pas de questions\"),\n",
        "    (\"thank you for your time\", \"merci pour votre temps\"),\n",
        "    (\"see you tomorrow\", \"a demain\"),\n",
        "    (\"have a good weekend\", \"bon weekend\"),\n",
        "    (\"i want to learn more\", \"je veux en savoir plus\"),\n",
        "    (\"this is very useful\", \"c est tres utile\"),\n",
        "    (\"can you show me\", \"pouvez vous me montrer\"),\n",
        "    (\"i am sure\", \"je suis sur\"),\n",
        "    (\"i am not sure\", \"je ne suis pas sur\"),\n",
        "    (\"it's not fair\", \"ce n est pas juste\"),\n",
        "    (\"it's wonderful\", \"c est merveilleux\"),\n",
        "    (\"it's terrible\", \"c est terrible\"),\n",
        "    (\"i need a break\", \"j ai besoin d une pause\"),\n",
        "    (\"let's take a break\", \"faisons une pause\"),\n",
        "    (\"what's the problem\", \"quel est le probleme\"),\n",
        "    (\"there is no problem\", \"il n y a pas de probleme\"),\n",
        "    (\"i can't find it\", \"je ne le trouve pas\"),\n",
        "    (\"i found it\", \"je l ai trouve\"),\n",
        "    (\"it's getting late\", \"il se fait tard\"),\n",
        "    (\"i must go now\", \"je dois partir maintenant\"),\n",
        "    (\"this is too much\", \"c est trop\"),\n",
        "    (\"this is not enough\", \"ce n est pas assez\"),\n",
        "    (\"i'm ready\", \"je suis pret\"),\n",
        "    (\"are you ready\", \"etes vous pret\"),\n",
        "    (\"i'm waiting for you\", \"je vous attends\"),\n",
        "    (\"don't worry\", \"ne t inquiete pas\"),\n",
        "    (\"it's going to be okay\", \"ca va aller\"),\n",
        "    (\"what's your opinion\", \"quel est votre avis\"),\n",
        "    (\"in my opinion\", \"a mon avis\"),\n",
        "    (\"i think that\", \"je pense que\"),\n",
        "    (\"it seems that\", \"il semble que\"),\n",
        "    (\"i would like to know\", \"je voudrais savoir\"),\n",
        "    (\"can you explain to me\", \"pouvez vous m expliquer\"),\n",
        "    (\"i'm trying to learn\", \"j essaie d apprendre\")\n",
        "]"
      ],
      "metadata": {
        "id": "QExmKJ7yrjRq"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(raw_data_pairs)\n",
        "split_idx = int(len(raw_data_pairs) * 0.9)\n",
        "train_data = raw_data_pairs[:split_idx]\n",
        "valid_data = raw_data_pairs[split_idx:]\n",
        "\n",
        "print(f\"Training examples: {len(train_data)}\")\n",
        "print(f\"Validation examples: {len(valid_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVv6pHJti0S5",
        "outputId": "ec65bece-0fb8-4406-f844-b43e0e19095c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training examples: 238\n",
            "Validation examples: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer\n",
        "We build a simple tokenizer to introduce the concept. This tokenizer does not reduce to lemmas or perform any vector distributions for the inputs. Instead, each unique word is given a numeric representation that counts upward as new words are added."
      ],
      "metadata": {
        "id": "HZiBElgSrvna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_TOKEN = \"<pad>\"\n",
        "SOS_TOKEN = \"<sos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "class CustomTokenizer:\n",
        "  def __init__(self, language_name):\n",
        "    self.language_name = language_name\n",
        "    self.word2index = {}\n",
        "    self.index2word = {}\n",
        "    self.n_count = 0\n",
        "    self.word_counts = Counter()\n",
        "    self.add_word(PAD_TOKEN)\n",
        "    self.add_word(SOS_TOKEN)\n",
        "    self.add_word(EOS_TOKEN)\n",
        "    self.add_word(UNK_TOKEN)\n",
        "\n",
        "    self.PAD_IDX = self.add_word(PAD_TOKEN)\n",
        "    self.SOS_IDX = self.add_word(SOS_TOKEN)\n",
        "    self.EOS_IDX = self.add_word(EOS_TOKEN)\n",
        "    self.UNK_IDX = self.add_word(UNK_TOKEN)\n",
        "\n",
        "  # a linear tokenizer (count -> index)\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_count\n",
        "      self.index2word[self.n_count] = word\n",
        "      self.n_count += 1\n",
        "    return self.word2index[word]\n",
        "\n",
        "  def add_sentence(self, sentence):\n",
        "    for word in sentence.lower().split(' '):\n",
        "      self.word_counts[word] += 1\n",
        "\n",
        "  def build_vocab(self, sentences):\n",
        "    # Build up a count for each word\n",
        "    for sentence in sentences:\n",
        "      self.add_sentence(sentence)\n",
        "\n",
        "    # Add each unique key (word) to the word2index / index2word dicts\n",
        "    for word in sorted(self.word_counts.keys()):\n",
        "      self.add_word(word)\n",
        "\n",
        "  def sentence_to_indices(self, sentence):\n",
        "    tokens = [SOS_TOKEN] + sentence.lower().split(' ') + [EOS_TOKEN]\n",
        "    indices = [self.word2index.get(token, self.UNK_IDX) for token in tokens]\n",
        "    return indices\n",
        "\n",
        "  def indices_to_sentence(self, indices):\n",
        "    if hasattr(indices, 'tolist'):\n",
        "      indices = indices.tolist()\n",
        "    return ' '.join(self.index2word.get(index, UNK_TOKEN) for index in indices\n",
        "                    if index not in [self.SOS_IDX, self.EOS_IDX, self.PAD_IDX])\n"
      ],
      "metadata": {
        "id": "6wkhVYHUrw-6"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the tokenizers and input vocabularies"
      ],
      "metadata": {
        "id": "EdxLLoTL8xN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_tokenizer = CustomTokenizer(SRC_LANGUAGE)\n",
        "tgt_tokenizer = CustomTokenizer(TGT_LANGUAGE)\n",
        "\n",
        "src_sentences = [pair[0] for pair in train_data]\n",
        "tgt_sentences = [pair[1] for pair in train_data]\n",
        "\n",
        "src_tokenizer.build_vocab(src_sentences)\n",
        "tgt_tokenizer.build_vocab(tgt_sentences)"
      ],
      "metadata": {
        "id": "2LV38c9080rP"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test their behaviour"
      ],
      "metadata": {
        "id": "ZjMJ73rz9aJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary\n",
        "print(\"\\nSource Vocabulary (EN):\")\n",
        "print(src_tokenizer.word2index)\n",
        "print(f\"PAD_IDX: {src_tokenizer.PAD_IDX}, SOS_IDX: {src_tokenizer.SOS_IDX},\"\n",
        "      f\"EOS_IDX: {src_tokenizer.EOS_IDX}, UNK_IDX: {src_tokenizer.UNK_IDX}\")\n",
        "\n",
        "print(f\"\\nTarget Vocabulary (FR)\")\n",
        "print(tgt_tokenizer.word2index)\n",
        "print(f\"PAD_IDX: {tgt_tokenizer.PAD_IDX}, SOS_IDX: {tgt_tokenizer.SOS_IDX}\"\n",
        "      f\"EOS_IDX: {tgt_tokenizer.EOS_IDX}, UNK_IDX: {tgt_tokenizer.UNK_IDX}\")\n",
        "\n",
        "# Test the tokenizer\n",
        "test_src_sent = \"hello world\"\n",
        "test_src_indices = src_tokenizer.sentence_to_indices(test_src_sent)\n",
        "print(f\"\\n'{test_src_sent}' -> {test_src_indices}\")\n",
        "print(f\"'{test_src_indices}' -> '{src_tokenizer.indices_to_sentence(test_src_indices)}'\\n\")\n",
        "\n",
        "test_tgt_sent = \"bonjour le monde\"\n",
        "test_tgt_indices = tgt_tokenizer.sentence_to_indices(test_tgt_sent)\n",
        "print(f\"'{test_tgt_sent}' -> {test_tgt_indices}\")\n",
        "print(f\"'{test_tgt_indices}' -> '{tgt_tokenizer.indices_to_sentence(test_tgt_indices)}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkGlpW339bXF",
        "outputId": "a02276a9-7bab-45a3-b484-5ac948ae037f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Source Vocabulary (EN):\n",
            "{'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3, 'a': 4, 'about': 5, 'accuracy': 6, 'ache': 7, 'address': 8, 'adjust': 9, 'afternoon': 10, 'agree': 11, 'algorithm': 12, 'am': 13, 'an': 14, 'any': 15, 'are': 16, 'arrive': 17, 'assistance': 18, 'at': 19, 'attention': 20, 'backpropagation': 21, 'bad': 22, 'bank': 23, 'bathroom': 24, 'be': 25, 'bed': 26, 'beer': 27, 'best': 28, 'bigger': 29, 'bill': 30, 'birthday': 31, 'book': 32, 'bored': 33, 'break': 34, 'bus': 35, 'busy': 36, 'by': 37, 'call': 38, 'can': 39, \"can't\": 40, 'canada': 41, 'card': 42, 'care': 43, 'careful': 44, 'cash': 45, 'cat': 46, 'challenging': 47, 'cheers': 48, 'christmas': 49, 'classification': 50, 'close': 51, 'closed': 52, 'cloudy': 53, 'code': 54, 'coffee': 55, 'cold': 56, 'come': 57, 'complex': 58, 'control': 59, 'cook': 60, 'cooking': 61, 'cool': 62, 'cost': 63, 'crucial': 64, 'data': 65, 'dataset': 66, 'day': 67, 'debugging': 68, 'delicious': 69, 'descent': 70, 'desk': 71, 'difficult': 72, 'disagree': 73, 'do': 74, 'doctor': 75, 'document': 76, 'does': 77, 'dog': 78, 'doing': 79, \"don't\": 80, 'door': 81, 'down': 82, 'drink': 83, 'easy': 84, 'eat': 85, 'effective': 86, 'email': 87, 'end-to-end': 88, 'engineering': 89, 'english': 90, 'entrance': 91, 'essential': 92, 'evaluating': 93, 'evening': 94, 'example': 95, 'excited': 96, 'excuse': 97, 'exercise': 98, 'exit': 99, 'expensive': 100, 'explain': 101, 'fair': 102, 'far': 103, 'faster': 104, 'favorite': 105, 'feature': 106, 'feel': 107, 'find': 108, 'fine': 109, 'fine-tune': 110, 'five': 111, 'food': 112, 'for': 113, 'found': 114, 'france': 115, 'french': 116, 'friend': 117, 'from': 118, 'getting': 119, 'go': 120, 'going': 121, 'good': 122, 'gpus': 123, 'gradient': 124, 'had': 125, 'happy': 126, 'have': 127, 'headache': 128, 'hello': 129, 'help': 130, 'here': 131, 'hope': 132, 'hot': 133, 'how': 134, 'hungry': 135, 'hyperparameters': 136, 'i': 137, \"i'll\": 138, \"i'm\": 139, 'important': 140, 'in': 141, 'information': 142, 'interesting': 143, 'is': 144, 'it': 145, \"it's\": 146, 'know': 147, 'language': 148, 'large': 149, 'late': 150, 'learn': 151, 'learning': 152, 'leave': 153, 'left': 154, \"let's\": 155, 'like': 156, 'listen': 157, 'listening': 158, 'look': 159, 'lot': 160, 'love': 161, 'luck': 162, 'luggage': 163, 'machine': 164, 'me': 165, 'meal': 166, 'mean': 167, 'mechanism': 168, 'meet': 169, 'merry': 170, 'mind': 171, 'model': 172, 'more': 173, 'much': 174, 'music': 175, 'must': 176, 'my': 177, 'name': 178, 'near': 179, 'nearby': 180, 'need': 181, 'network': 182, 'neural': 183, 'never': 184, 'new': 185, 'next': 186, 'nice': 187, 'night': 188, 'nlp': 189, 'no': 190, 'not': 191, 'nothing': 192, 'now': 193, 'number': 194, \"o'clock\": 195, 'office': 196, 'okay': 197, 'on': 198, 'one': 199, 'open': 200, 'opinion': 201, 'optimization': 202, 'optimize': 203, 'or': 204, 'overfitting': 205, 'pay': 206, 'performance': 207, 'phone': 208, 'please': 209, 'pleasure': 210, 'post': 211, 'practice': 212, 'predicts': 213, 'prefer': 214, 'preprocessing': 215, 'problem': 216, 'production-ready': 217, 'programming': 218, 'python': 219, 'pytorch': 220, 'question': 221, 'questions': 222, 'raining': 223, 'rate': 224, 'read': 225, 'reading': 226, 'ready': 227, 'recommend': 228, 'regression': 229, 'repeat': 230, 'reservation': 231, 'restaurant': 232, 'right': 233, 'round': 234, 'sad': 235, 'say': 236, 'script': 237, 'see': 238, 'show': 239, 'sick': 240, 'sit': 241, 'size': 242, 'slower': 243, 'smaller': 244, 'so': 245, 'soon': 246, 'sorry': 247, 'speak': 248, 'stable': 249, 'stand': 250, 'station': 251, 'stomach': 252, 'stop': 253, 'student': 254, 'studying': 255, 'sunday': 256, 'sunny': 257, 'sure': 258, 'system': 259, 'table': 260, 'take': 261, 'task': 262, 'teacher': 263, 'terrible': 264, 'thank': 265, 'that': 266, 'the': 267, 'there': 268, 'think': 269, 'thinking': 270, 'this': 271, 'time': 272, 'tired': 273, 'to': 274, 'today': 275, 'tomorrow': 276, 'too': 277, 'totally': 278, 'train': 279, 'training': 280, 'transfer': 281, 'transformers': 282, 'translation': 283, 'travel': 284, 'trip': 285, 'try': 286, 'trying': 287, 'tuesday': 288, 'turn': 289, 'tv': 290, 'two': 291, 'understand': 292, 'up': 293, 'urgent': 294, 'use': 295, 'useful': 296, 'vacation': 297, 'validation': 298, 'version': 299, 'very': 300, 'wait': 301, 'waiting': 302, 'want': 303, 'was': 304, 'watching': 305, 'water': 306, 'way': 307, 'we': 308, 'weather': 309, 'weekend': 310, 'welcome': 311, 'what': 312, \"what's\": 313, 'when': 314, 'where': 315, 'will': 316, 'window': 317, 'windy': 318, 'with': 319, 'wonderful': 320, 'word': 321, 'work': 322, 'working': 323, 'world': 324, 'worry': 325, 'would': 326, 'write': 327, 'year': 328, 'yes': 329, 'yesterday': 330, 'you': 331, \"you're\": 332, 'your': 333}\n",
            "PAD_IDX: 0, SOS_IDX: 1,EOS_IDX: 2, UNK_IDX: 3\n",
            "\n",
            "Target Vocabulary (FR)\n",
            "{'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3, 'a': 4, 'accord': 5, 'addition': 6, 'adresse': 7, 'affiner': 8, 'ai': 9, 'aide': 10, 'aider': 11, 'aime': 12, 'aimes': 13, 'ajuster': 14, 'algorithme': 15, 'aller': 16, 'allez': 17, 'allons': 18, 'ami': 19, 'anglais': 20, 'annee': 21, 'anniversaire': 22, 'appeler': 23, 'appetit': 24, 'apprendre': 25, 'apprends': 26, 'apprentissage': 27, 'apres-midi': 28, 'arretez': 29, 'arrive': 30, 'asseyez': 31, 'attends': 32, 'attention': 33, 'au': 34, 'aujourd': 35, 'automatique': 36, 'avec': 37, 'avez': 38, 'avis': 39, 'bagages': 40, 'banque': 41, 'beau': 42, 'beaucoup': 43, 'besoin': 44, 'bien': 45, 'bientot': 46, 'biere': 47, 'boire': 48, 'bon': 49, 'bonjour': 50, 'bonne': 51, 'bonsoir': 52, 'bout': 53, 'bureau': 54, 'bus': 55, 'c': 56, 'ca': 57, 'cafe': 58, 'canada': 59, 'caracteristiques': 60, 'carte': 61, 'ce': 62, 'cet': 63, 'cette': 64, 'chance': 65, 'chat': 66, 'chaud': 67, 'cher': 68, 'chien': 69, 'chose': 70, 'cinq': 71, 'classification': 72, 'code': 73, 'combien': 74, 'comment': 75, 'complexe': 76, 'comprends': 77, 'content': 78, 'controle': 79, 'cool': 80, 'coucher': 81, 'coute': 82, 'crucial': 83, 'cuisine': 84, 'cuisiner': 85, 'd': 86, 'dans': 87, 'de': 88, 'deboguons': 89, 'delicieux': 90, 'demain': 91, 'des': 92, 'descente': 93, 'desole': 94, 'deux': 95, 'devons': 96, 'difficile': 97, 'dimanche': 98, 'dire': 99, 'distance': 100, 'dit': 101, 'documenter': 102, 'dois': 103, 'donnees': 104, 'droite': 105, 'du': 106, 'e-mail': 107, 'eau': 108, 'ecoute': 109, 'ecoutez': 110, 'ecrivez': 111, 'efficace': 112, 'emmenez': 113, 'en': 114, 'ennuie': 115, 'entierement': 116, 'entrainement': 117, 'entrainons': 118, 'entree': 119, 'entrez': 120, 'especes': 121, 'espere': 122, 'essaie': 123, 'essayer': 124, 'essentiel': 125, 'est': 126, 'etait': 127, 'etes': 128, 'etudiant': 129, 'etudie': 130, 'eu': 131, 'evaluons': 132, 'excite': 133, 'excusez-moi': 134, 'exemple': 135, 'exercice': 136, 'expliquer': 137, 'expliquez': 138, 'facile': 139, 'faim': 140, 'fais': 141, 'faisons': 142, 'fait': 143, 'faites': 144, 'fatigue': 145, 'fenetre': 146, 'ferai': 147, 'ferme': 148, 'fermez': 149, 'fonctionne': 150, 'francais': 151, 'francaise': 152, 'france': 153, 'froid': 154, 'gare': 155, 'gauche': 156, 'gpu': 157, 'gradient': 158, 'grand': 159, 'grande': 160, 'heure': 161, 'heures': 162, 'hier': 163, 'hui': 164, 'hyperparametres': 165, 'ici': 166, 'il': 167, 'important': 168, 'importante': 169, 'information': 170, 'ingenierie': 171, 'inquiete': 172, 'interessant': 173, 'j': 174, 'je': 175, 'jeu': 176, 'jour': 177, 'journee': 178, 'joyeux': 179, 'juste': 180, 'l': 181, 'la': 182, 'laisse': 183, 'langage': 184, 'le': 185, 'lentement': 186, 'les': 187, 'levez': 188, 'lire': 189, 'lis': 190, 'livre': 191, 'loin': 192, 'm': 193, 'maintenant': 194, 'mal': 195, 'malade': 196, 'manger': 197, 'mardi': 198, 'mauvais': 199, 'me': 200, 'mecanisme': 201, 'medecin': 202, 'merci': 203, 'merveilleux': 204, 'mes': 205, 'mieux': 206, 'modele': 207, 'moi': 208, 'mon': 209, 'monde': 210, 'montrer': 211, 'mot': 212, 'musique': 213, 'n': 214, 'ne': 215, 'neuf': 216, 'neurones': 217, 'noel': 218, 'nom': 219, 'nous': 220, 'nuageux': 221, 'nuit': 222, 'numero': 223, 'occupe': 224, 'occupee': 225, 'on': 226, 'optimisation': 227, 'optimiser': 228, 'ou': 229, 'oui': 230, 'ouvert': 231, 'ouvre': 232, 'ouvrez': 233, 'par': 234, 'parle': 235, 'parlez': 236, 'part': 237, 'partir': 238, 'pas': 239, 'patienter': 240, 'pause': 241, 'payer': 242, 'pense': 243, 'performance': 244, 'petite': 245, 'plaisir': 246, 'plait': 247, 'plat': 248, 'pleut': 249, 'plus': 250, 'porte': 251, 'poste': 252, 'pour': 253, 'pouvez': 254, 'pratique': 255, 'pre-traitement': 256, 'precision': 257, 'predit': 258, 'prefere': 259, 'prends': 260, 'pres': 261, 'pret': 262, 'probleme': 263, 'production': 264, 'professeur': 265, 'programmation': 266, 'proximite': 267, 'puis': 268, 'python': 269, 'pytorch': 270, 'qu': 271, 'quand': 272, 'que': 273, 'quel': 274, 'quelle': 275, 'question': 276, 'questions': 277, 'quoi': 278, 'rapide': 279, 'ravi': 280, 'recommander': 281, 'reflechis': 282, 'regarde': 283, 'regardez': 284, 'regression': 285, 'rencontrer': 286, 'repeter': 287, 'reseau': 288, 'reservation': 289, 'restaurant': 290, 'retour': 291, 'retropropagation': 292, 'rien': 293, 's': 294, 'sais': 295, 'sante': 296, 'savoir': 297, 'script': 298, 'se': 299, 'sens': 300, 'simple': 301, 'soin': 302, 'soleil': 303, 'sommes': 304, 'sont': 305, 'sortie': 306, 'stable': 307, 'suis': 308, 'suivant': 309, 'sur': 310, 'surapprentissage': 311, 'systeme': 312, 't': 313, 'ta': 314, 'table': 315, 'tache': 316, 'taille': 317, 'tard': 318, 'taux': 319, 'tele': 320, 'telephone': 321, 'temps': 322, 'terrible': 323, 'tete': 324, 'tnl': 325, 'toi': 326, 'toilettes': 327, 'tomber': 328, 'tournez': 329, 'traduction': 330, 'train': 331, 'transfert': 332, 'transformers': 333, 'travail': 334, 'travaille': 335, 'tres': 336, 'triste': 337, 'trop': 338, 'trouve': 339, 'tu': 340, 'un': 341, 'une': 342, 'urgent': 343, 'utile': 344, 'utilisons': 345, 'va': 346, 'vacances': 347, 'vais': 348, 'validation': 349, 'venez': 350, 'vent': 351, 'ventre': 352, 'version': 353, 'veuillez': 354, 'veux': 355, 'vie': 356, 'viens': 357, 'votre': 358, 'voudrais': 359, 'voulez': 360, 'vous': 361, 'voyage': 362, 'voyager': 363, 'weekend': 364, 'y': 365}\n",
            "PAD_IDX: 0, SOS_IDX: 1EOS_IDX: 2, UNK_IDX: 3\n",
            "\n",
            "'hello world' -> [1, 129, 324, 2]\n",
            "'[1, 129, 324, 2]' -> 'hello world'\n",
            "\n",
            "'bonjour le monde' -> [1, 50, 185, 210, 2]\n",
            "'[1, 50, 185, 210, 2]' -> 'bonjour le monde'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding\n",
        "We need to pad so that different length sentences can be accepted by the RNN in batches. We use padding tokens to achieve this."
      ],
      "metadata": {
        "id": "9MHZzvHqGAhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch, src_tokenizer, tgt_tokenizer, device):\n",
        "  src_batch, tgt_batch = [], []\n",
        "  src_lens, tgt_lens = [], []\n",
        "  for src_sample, tgt_sample in batch:\n",
        "    src_indices = src_tokenizer.sentence_to_indices(src_sample)\n",
        "    tgt_indices = tgt_tokenizer.sentence_to_indices(tgt_sample)\n",
        "\n",
        "    src_batch.append(torch.tensor(src_indices, dtype=torch.long))\n",
        "    tgt_batch.append(torch.tensor(tgt_indices, dtype=torch.long))\n",
        "\n",
        "    src_lens.append(len(src_indices))\n",
        "    tgt_lens.append(len(tgt_indices))\n",
        "\n",
        "  src_padded = nn.utils.rnn.pad_sequence(src_batch, padding_value=src_tokenizer.PAD_IDX, batch_first=False)\n",
        "  tgt_padded = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=tgt_tokenizer.PAD_IDX, batch_first=False)\n",
        "\n",
        "  return src_padded.to(device), tgt_padded.to(device), torch.tensor(src_lens), torch.tensor(tgt_lens)"
      ],
      "metadata": {
        "id": "oh2Jr7IbGJlp"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a sample dataloader"
      ],
      "metadata": {
        "id": "4WygYgAhmajh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2\n",
        "def get_data_iterator(data, src_tokenizer, tgt_tokenizer, batch_size, device, shuffle=True):\n",
        "  if shuffle:\n",
        "    data_copy = list(data)\n",
        "    random.shuffle(data_copy)\n",
        "  else:\n",
        "    data_copy = data\n",
        "\n",
        "  for i in range(0, len(data_copy), batch_size):\n",
        "    batch = data_copy[i:i+batch_size]\n",
        "    yield collate_fn(batch, src_tokenizer, tgt_tokenizer, device)"
      ],
      "metadata": {
        "id": "fEFHGnySmcO-"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTesting data iterator:\")\n",
        "data_iter = get_data_iterator(train_data,src_tokenizer, tgt_tokenizer, BATCH_SIZE, device)\n",
        "for i, (src_batch, tgt_batch, src_lens, tgt_lens) in enumerate(data_iter):\n",
        "  print(f\"Batch {i+1}:\")\n",
        "  print(\"Source batch shape: \", src_batch.shape)\n",
        "  print(\"target batch shape: \", tgt_batch.shape)\n",
        "  print(\"Source lengths: \", src_lens)\n",
        "  print(\"Target lengths: \", tgt_lens)\n",
        "  print(\"Source batch (first example):\\n\", src_batch[:, 0])\n",
        "  print(\"Target batch (first example):\\n\", tgt_batch[:, 0])\n",
        "  if i == 0: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mECBFLTvn6KC",
        "outputId": "d7f78480-eb56-45c7-ae77-4bd035008515"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing data iterator:\n",
            "Batch 1:\n",
            "Source batch shape:  torch.Size([6, 2])\n",
            "target batch shape:  torch.Size([8, 2])\n",
            "Source lengths:  tensor([6, 6])\n",
            "Target lengths:  tensor([8, 8])\n",
            "Source batch (first example):\n",
            " tensor([  1, 137, 181,   4,  34,   2])\n",
            "Target batch (first example):\n",
            " tensor([  1, 174,   9,  44,  86, 342, 241,   2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder\n",
        "\n",
        "We'll write the encoder using an nn.GRU unit. The encoder unit has several stages to its learning:\n",
        "\n",
        "* Timestep states *h* are computed as h$_{t}$ = f(h$_{t-1}$, x$_{t}$).\n",
        "* The final state h$_{t}$ is the context variable.\n",
        "* The context variable is given by some mapping *m* such that c = m(h$_{1}$, h$_{2}$, ... , h$_{t}$)\n",
        "* Encoders may be bidirectional, such that h$_{t}$ is a function of h$_{t-1}$ and h$_{t+1}$\n",
        "\n",
        "We build the unidirectional encoder here.\n",
        "\n",
        "Note the dimensionality of the various matrices."
      ],
      "metadata": {
        "id": "9VbBms5l_zYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout_p):\n",
        "    super().__init__()\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_layers = n_layers\n",
        "    # Here is the interesting bit: input dim is the size of your vocabulary.\n",
        "    # emb_dim is the arbitrary learning layer - you can select a size and experiment.\n",
        "    # We therefore train the rnn to work at a token level\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim,\n",
        "                      hid_dim,\n",
        "                      n_layers,\n",
        "                      bidirectional=bidirectional_gru,\n",
        "                      dropout=dropout_p if n_layers > 1 else 0)\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self, src_seq):\n",
        "    embedded = self.dropout(self.embedding(src_seq))\n",
        "    outputs, hidden = self.rnn(embedded)\n",
        "    return outputs, hidden"
      ],
      "metadata": {
        "id": "9A57ZE1x_0np"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder takes the context variable from the encoder and creates its own hidden state. This hidden state is not only a function of the last hidden state, but also the previously decoded token.\n",
        "\n",
        "* s$_{t'}$ = g(s$_{t-1}$, y$_{t'-1}$, c)\n",
        "* y$_{t'}$ is a probability distribution of P(y$_{t'}$| y$_{t-1}$, ..., y$_{1}$, c) = softmax(s$_{t-1}$, y$_{t'-1}$, c).\n",
        "* In a sense, this means that the last N output tokens from the decoder influence the latest token and the current hidden state."
      ],
      "metadata": {
        "id": "A-pCsQ_QJpb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout_p):\n",
        "    super().__init__()\n",
        "    self.output_dim = output_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # Components\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim,\n",
        "                      hid_dim,\n",
        "                      n_layers,\n",
        "                      dropout=dropout_p if n_layers > 1 else 0)\n",
        "    self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self, input_token, hidden_state):\n",
        "    # turns [batch_size] into [1, batch_size]\n",
        "    input_token = input_token.unsqueeze(0)\n",
        "    embedded = self.dropout(self.embedding(input_token))\n",
        "\n",
        "    # Per token decoding\n",
        "    output, new_hidden_state = self.rnn(embedded, hidden_state)\n",
        "    prediction = self.fc_out(output.squeeze(0))\n",
        "\n",
        "    return prediction, new_hidden_state"
      ],
      "metadata": {
        "id": "8K5PmNiDN6yu"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq Implementation\n",
        "\n",
        "The Seq2Seq component implements and handles the encoder/decoder architecture."
      ],
      "metadata": {
        "id": "lY-91VeMfOcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_bidirectional_hidden(n_layers, batch_size, hid, enc_hidden, bridge):\n",
        "  enc_hidden = enc_hidden.view(n_layers, 2, batch_size, hid)\n",
        "  cat = torch.cat((enc_hidden[:, 0, :, :], enc_hidden[:, 1, :, :]), dim=2)\n",
        "  return torch.tanh(bridge(cat))\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "    self.bridge = nn.Linear(encoder.hid_dim * 2, decoder.hid_dim)\n",
        "\n",
        "    # Sanity check for dimensionality\n",
        "    assert encoder.hid_dim == decoder.hid_dim, \"hidden dims must be equal\"\n",
        "    assert encoder.n_layers == decoder.n_layers, \"layers must be equal\"\n",
        "\n",
        "  def _init_decoder_hidden(self, enc_hidden):\n",
        "    n_layers, batch_size, hid = self.encoder.n_layers, enc_hidden.size(1), self.encoder.hid_dim\n",
        "    return resize_bidirectional_hidden(n_layers, batch_size, hid, enc_hidden, self.bridge)\n",
        "\n",
        "  def forward(self, src_seq, tgt_seq, teacher_forcing_ratio=0.5):\n",
        "    batch_size = src_seq.shape[1]\n",
        "    tgt_len = tgt_seq.shape[0]\n",
        "    tgt_vocab_size = self.decoder.output_dim\n",
        "    outputs = torch.zeros(tgt_len, batch_size, tgt_vocab_size)\n",
        "\n",
        "    # Encode\n",
        "    enc_out, hidden = self.encoder(src_seq)\n",
        "    if (bidirectional_gru):\n",
        "      hidden = self._init_decoder_hidden(hidden)\n",
        "\n",
        "    # Decode\n",
        "    dec_in = tgt_seq[0, :]\n",
        "    for t in range(1, tgt_len):\n",
        "      dec_out, hidden = self.decoder(dec_in, hidden)\n",
        "      outputs[t] = dec_out\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "      top1 = dec_out.argmax(1)\n",
        "      dec_in = tgt_seq[t] if teacher_force else top1\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "-dy3R35PfXC_"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "QckUDH8kcOis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparams\n",
        "INPUT_DIM = src_tokenizer.n_count\n",
        "OUTPUT_DIM = tgt_tokenizer.n_count\n",
        "ENC_EMB_DIM = 128\n",
        "DEC_EMB_DIM = 128\n",
        "HID_DIM = 256\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "LEARNING_RATE = 0.001\n",
        "N_EPOCHS = 5\n",
        "CLIP = 1\n",
        "BIDIR_MODEL_NAME = \"language_enc_dec_bidir.pt\"\n",
        "UNIDIR_MODEL_NAME = \"language_enc_dec.pt\"\n",
        "MODEL_NAME = BIDIR_MODEL_NAME if bidirectional_gru else UNIDIR_MODEL_NAME"
      ],
      "metadata": {
        "id": "cquq3cUbcP-M"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# components\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)\n",
        "model_bidir = Seq2Seq(enc, dec, device)"
      ],
      "metadata": {
        "id": "LDs8YgZlweKi"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model_bidir):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwoiHd_0w1si",
        "outputId": "e6e980dc-4e7b-47e3-eb2e-5e0aaeadd715"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 2,781,806 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optim and learn\n",
        "optimizer = optim.Adam(model_bidir.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = tgt_tokenizer.PAD_IDX)"
      ],
      "metadata": {
        "id": "AmF7TUbzxEcT"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "brGpMZ6AxQ22"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    batch_n = 0\n",
        "    for i, (src, tgt, _, _) in enumerate(iterator): # src_lens, tgt_lens not used directly here\n",
        "        batch_n += 1\n",
        "        optimizer.zero_grad()\n",
        "        # output = [tgt_len, batch_size, output_vocab_size]\n",
        "        output = model(src, tgt)\n",
        "        # get vocab length for next step\n",
        "        output_dim = output.shape[-1]\n",
        "        # remove <sos> tag by enforcing [1:]\n",
        "        # turn [tgt_len, batch, vocab] into [(tgt_len-1 * batch), vocab] so it fits into loss\n",
        "        output_flat = output[1:].view(-1, output_dim)\n",
        "        # since we know the vocab, this doesn't have the V dimension [tgt_len-1, batch]\n",
        "        tgt_flat = tgt[1:].view(-1)\n",
        "        # now that they are equal dim, compute the loss between out and tgt\n",
        "        loss = criterion(output_flat, tgt_flat)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / batch_n\n",
        "\n",
        "def evaluate_epoch(model, iterator, criterion):\n",
        "  # the same setup, but we loop without adam use\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  batch_n = 0\n",
        "  with torch.no_grad():\n",
        "    for i, (src, tgt, _, _) in enumerate(iterator):\n",
        "      batch_n += 1\n",
        "      output = model(src, tgt, 0)\n",
        "      output_dim = output.shape[-1]\n",
        "      output_flat = output[1:].view(-1, output_dim)\n",
        "      tgt_flat = tgt[1:].view(-1)\n",
        "      loss = criterion(output_flat, tgt_flat)\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss / batch_n"
      ],
      "metadata": {
        "id": "XRLTMnN8yG2p"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Execute"
      ],
      "metadata": {
        "id": "LDWtJKaM2hfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start_time = time.time()\n",
        "  train_iter = get_data_iterator(train_data,\n",
        "                               src_tokenizer,\n",
        "                               tgt_tokenizer,\n",
        "                               BATCH_SIZE,\n",
        "                               device,\n",
        "                               shuffle=True)\n",
        "  valid_iter = get_data_iterator(valid_data,\n",
        "                               src_tokenizer,\n",
        "                               tgt_tokenizer,\n",
        "                               BATCH_SIZE,\n",
        "                               device,\n",
        "                               shuffle=False)\n",
        "  train_loss = train_epoch(model_bidir, train_iter, optimizer, criterion, CLIP)\n",
        "  valid_loss = evaluate_epoch(model_bidir, valid_iter, criterion)\n",
        "\n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "torch.save(model_bidir.state_dict(), MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PSyI3EJ2jNs",
        "outputId": "f1ec6a7f-0259-4b07-e2a1-7412e5b29da0"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch: 01 | Time: 0m 9s\n",
            "\tTrain Loss: 4.659 | Train PPL: 105.531\n",
            "\t Val. Loss: 4.530 |  Val. PPL:  92.789\n",
            "Epoch: 02 | Time: 0m 9s\n",
            "\tTrain Loss: 3.890 | Train PPL:  48.931\n",
            "\t Val. Loss: 4.623 |  Val. PPL: 101.823\n",
            "Epoch: 03 | Time: 0m 8s\n",
            "\tTrain Loss: 3.484 | Train PPL:  32.587\n",
            "\t Val. Loss: 4.659 |  Val. PPL: 105.556\n",
            "Epoch: 04 | Time: 0m 8s\n",
            "\tTrain Loss: 3.111 | Train PPL:  22.443\n",
            "\t Val. Loss: 4.803 |  Val. PPL: 121.906\n",
            "Epoch: 05 | Time: 0m 9s\n",
            "\tTrain Loss: 2.774 | Train PPL:  16.030\n",
            "\t Val. Loss: 4.897 |  Val. PPL: 133.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Model"
      ],
      "metadata": {
        "id": "inpbzvO6-Qzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_bidir.load_state_dict(torch.load(MODEL_NAME, weights_only=True))\n",
        "model_bidir.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V4cY_4e4Izk",
        "outputId": "c5dedf24-14ff-4cfc-9357-cf7595730c2d"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(334, 128)\n",
              "    (rnn): GRU(128, 256, num_layers=2, dropout=0.2, bidirectional=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(366, 128)\n",
              "    (rnn): GRU(128, 256, num_layers=2, dropout=0.2)\n",
              "    (fc_out): Linear(in_features=256, out_features=366, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (bridge): Linear(in_features=512, out_features=256, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, src_tokenizer, tgt_tokenizer, model, device, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = src_tokenizer.sentence_to_indices(sentence)\n",
        "    else:\n",
        "        tokens = sentence\n",
        "\n",
        "    # Shape: [src_len, 1]\n",
        "    src_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
        "        # Use model's bridge logic to reconcile bidirectional hidden state\n",
        "        if (bidirectional_gru):\n",
        "          hidden = model._init_decoder_hidden(hidden)\n",
        "\n",
        "    tgt_indices = [tgt_tokenizer.SOS_IDX]\n",
        "    for _ in range(max_len):\n",
        "        # Use the last generated token as input\n",
        "        tgt_tensor = torch.tensor([tgt_indices[-1]], dtype=torch.long).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model.decoder(tgt_tensor, hidden)\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "        tgt_indices.append(pred_token)\n",
        "\n",
        "        if pred_token == tgt_tokenizer.EOS_IDX:\n",
        "            break\n",
        "\n",
        "    translated_sentence = tgt_tokenizer.indices_to_sentence(tgt_indices)\n",
        "    return translated_sentence"
      ],
      "metadata": {
        "id": "47pMjo7a8mAD"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement BLEU to test accuracy"
      ],
      "metadata": {
        "id": "uv-7s6AO0lC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "\n",
        "def ngrams(seq: List[str], n: int) -> List[tuple[str, ...]]:\n",
        "  return [tuple(seq[i:i+n]) for i in range(len(seq) - n + 1)]\n",
        "\n",
        "def modified_precision(candidate: List[str],\n",
        "                       references: List[List[str]],\n",
        "                       n: int):\n",
        "  cand_ngrams = Counter(ngrams(candidate, n))\n",
        "  max_reference_counts = Counter()\n",
        "\n",
        "  for ref in references:\n",
        "    ref_counts = Counter(ngrams(ref, n))\n",
        "    for ngram, count in ref_counts.items():\n",
        "      max_reference_counts[ngram] = max(max_reference_counts[ngram], count)\n",
        "\n",
        "  clipped_counts = {ngram: min(count, max_reference_counts[ngram])\n",
        "    for ngram, count in cand_ngrams.items()}\n",
        "\n",
        "  numerator = sum(clipped_counts.values())\n",
        "  denominator = sum(cand_ngrams.values())\n",
        "\n",
        "  return numerator, denominator\n",
        "\n",
        "def brevity_penalty(c: int, r: int) -> float:\n",
        "  return 1.0 if c > r else math.exp(1 - r / c)\n",
        "\n",
        "def closest_ref_len(c: int, ref_lens: List[int]) -> int:\n",
        "  return min(ref_lens, key=lambda rl: (abs(rl - c), rl))\n",
        "\n",
        "def bleu(candidate: List[str],\n",
        "         references: List[List[str]],\n",
        "         max_n: int = 4) -> float:\n",
        "  weights = [1/max_n] * max_n\n",
        "  precisions = []\n",
        "\n",
        "  for n in range(1, max_n+1):\n",
        "    num, den = modified_precision(candidate, references, n)\n",
        "    if num == 0:\n",
        "      num, den = 1, 2\n",
        "    precisions.append((num, den))\n",
        "\n",
        "  geo_mean = math.exp(sum(w * math.log(num/den)\n",
        "    for (num, den), w in zip(precisions, weights)))\n",
        "\n",
        "  c = len(candidate)\n",
        "  r = closest_ref_len(c, [len(r) for r in references])\n",
        "  bp = brevity_penalty(c, r)\n",
        "  return bp * geo_mean\n",
        "\n",
        "def tokenize_sequence(sequence):\n",
        "  return sequence.lower().split(' ')"
      ],
      "metadata": {
        "id": "DZkPxmbxwQSq"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get BLEU candidates / references\n",
        "references_bidir = [tokenize_sequence(text_pair[1]) for text_pair in valid_data]\n",
        "candidates_bidir = [tokenize_sequence(translate_sentence(text_pair[0], src_tokenizer, tgt_tokenizer, model_bidir, device)) for text_pair in valid_data]\n",
        "print(references_bidir)\n",
        "print(candidates_bidir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLl4yNhd9XzS",
        "outputId": "e81e9676-b297-4c34-e39b-25a5d632820a"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['je', 'voudrais', 'savoir'], ['quels', 'sont', 'vos', 'loisirs'], ['j', 'ai', 'soif'], ['la', 'perte', 'd', 'entrainement', 'diminue'], ['il', 'semble', 'que'], ['j', 'apprends', 'le', 'francais'], ['j', 'aime', 'cette', 'chanson'], ['allez', 'tout', 'droit'], ['je', 'rentre', 'a', 'la', 'maison'], ['combien', 'de', 'temps', 'ca', 'prend'], ['je', 'me', 'reveille'], ['felicitations'], ['aujourd', 'hui', 'c', 'est', 'lundi'], ['merci'], ['les', 'resultats', 'sont', 'prometteurs'], ['ce', 'n', 'est', 'pas', 'assez'], ['a', 'plus', 'tard'], ['quel', 'est', 'votre', 'nom'], ['je', 'veux', 'un', 'billet', 'pour', 'paris'], ['nous', 'collectons', 'plus', 'de', 'donnees'], ['nous', 'avons', 'besoin', 'de', 'plus', 'de', 'donnees'], ['non', 'je', 'prefere', 'le', 'the'], ['non', 'je', 'ne', 'comprends', 'pas'], ['l', 'apprentissage', 'profond', 'est', 'interessant'], ['bonjour'], ['je', 'suis', 'perdu'], ['j', 'espere', 'que', 'non']]\n",
            "[['je', 'aime', 'voyager'], ['quel', 'est', 'votre'], ['je', 'suis'], ['la', 'est', 'modele', 'est'], ['c', 'etait'], ['je', 'suis', 'sens'], ['je', 'aime', 'voyager'], ['joyeux', 'vous'], ['je', 'suis'], ['comment', 'ca', 'le'], ['je', 'suis'], ['joyeux'], ['c', 'est', 'tres'], ['a', 'a', 'vous'], ['nous', 'devons', 'le', 'le'], ['c', 'est', 'un'], ['a'], ['quel', 'est', 'votre', 'est'], ['je', 'ai', 'besoin', 'd', 'aide'], ['nous', 'devons', 'le', 'le'], ['nous', 'devons', 'optimiser', 'le', 'code'], ['je', 'pense', 'que'], ['je', 'suis', 'sens'], ['l', 'fait', 'un'], ['bonne'], ['je', 'suis'], ['je', 'ai', 'besoin', 'd', 'aide']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_results = []\n",
        "for candidate in candidates_bidir:\n",
        "  bleu_results.append(bleu(candidate, references_bidir))\n",
        "\n",
        "print(bleu_results)\n",
        "sum_of_bleu = sum(bleu_results)\n",
        "len_of_bleu = len(bleu_results)\n",
        "print(f'The average bidir BLEU is: {sum_of_bleu / len_of_bleu}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOna1djQDWzd",
        "outputId": "9fb33634-c5a1-457a-9401-628840531dfe"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.537284965911771, 0.8408964152537145, 0.7071067811865476, 0.5, 0.5, 0.537284965911771, 0.537284965911771, 0.5, 0.7071067811865476, 0.537284965911771, 0.7071067811865476, 0.5, 0.537284965911771, 0.4518010018049224, 0.5, 0.5946035575013605, 0.5946035575013605, 0.5946035575013605, 0.5623413251903491, 0.5, 0.4728708045015879, 0.537284965911771, 0.537284965911771, 0.537284965911771, 0.5, 0.7071067811865476, 0.5623413251903491]\n",
            "The average bidir BLEU is: 0.5666951257957542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text_valid in valid_data:\n",
        "    translation = translate_sentence(text_valid[0], src_tokenizer, tgt_tokenizer, model, device)\n",
        "    print(f\"Original (EN): {text_valid}\")\n",
        "    # Find the ground truth if available\n",
        "    gt_fr = \"N/A\"\n",
        "    for en_s, fr_s in raw_data_pairs: # search in all raw data\n",
        "        if en_s == text_valid[0]:\n",
        "            gt_fr = fr_s\n",
        "            break\n",
        "    print(f\"Ground Truth (FR): {gt_fr}\")\n",
        "    print(f\"Translated (FR): {translation}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QaeIMp03wPfe",
        "outputId": "7deccced-8cc0-4a31-e72e-4861fc8e72a3"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original (EN): ('i would like to know', 'je voudrais savoir')\n",
            "Ground Truth (FR): je voudrais savoir\n",
            "Translated (FR): je aime voyager\n",
            "\n",
            "Original (EN): ('what are your hobbies', 'quels sont vos loisirs')\n",
            "Ground Truth (FR): quels sont vos loisirs\n",
            "Translated (FR): quel est votre\n",
            "\n",
            "Original (EN): ('i am thirsty', 'j ai soif')\n",
            "Ground Truth (FR): j ai soif\n",
            "Translated (FR): je suis\n",
            "\n",
            "Original (EN): ('the training loss is decreasing', 'la perte d entrainement diminue')\n",
            "Ground Truth (FR): la perte d entrainement diminue\n",
            "Translated (FR): la est modele est\n",
            "\n",
            "Original (EN): ('it seems that', 'il semble que')\n",
            "Ground Truth (FR): il semble que\n",
            "Translated (FR): c etait\n",
            "\n",
            "Original (EN): ('i am learning french', 'j apprends le francais')\n",
            "Ground Truth (FR): j apprends le francais\n",
            "Translated (FR): je suis sens\n",
            "\n",
            "Original (EN): ('i like this song', 'j aime cette chanson')\n",
            "Ground Truth (FR): j aime cette chanson\n",
            "Translated (FR): je aime voyager\n",
            "\n",
            "Original (EN): ('go straight ahead', 'allez tout droit')\n",
            "Ground Truth (FR): allez tout droit\n",
            "Translated (FR): joyeux vous\n",
            "\n",
            "Original (EN): ('i am going home', 'je rentre a la maison')\n",
            "Ground Truth (FR): je rentre a la maison\n",
            "Translated (FR): je suis\n",
            "\n",
            "Original (EN): ('how long does it take', 'combien de temps ca prend')\n",
            "Ground Truth (FR): combien de temps ca prend\n",
            "Translated (FR): comment ca le\n",
            "\n",
            "Original (EN): ('i am waking up', 'je me reveille')\n",
            "Ground Truth (FR): je me reveille\n",
            "Translated (FR): je suis\n",
            "\n",
            "Original (EN): ('congratulations', 'felicitations')\n",
            "Ground Truth (FR): felicitations\n",
            "Translated (FR): joyeux\n",
            "\n",
            "Original (EN): ('today is monday', 'aujourd hui c est lundi')\n",
            "Ground Truth (FR): aujourd hui c est lundi\n",
            "Translated (FR): c est tres\n",
            "\n",
            "Original (EN): ('thank you', 'merci')\n",
            "Ground Truth (FR): merci\n",
            "Translated (FR): a a vous\n",
            "\n",
            "Original (EN): ('the results are promising', 'les resultats sont prometteurs')\n",
            "Ground Truth (FR): les resultats sont prometteurs\n",
            "Translated (FR): nous devons le le\n",
            "\n",
            "Original (EN): ('this is not enough', 'ce n est pas assez')\n",
            "Ground Truth (FR): ce n est pas assez\n",
            "Translated (FR): c est un\n",
            "\n",
            "Original (EN): ('see you later', 'a plus tard')\n",
            "Ground Truth (FR): a plus tard\n",
            "Translated (FR): a\n",
            "\n",
            "Original (EN): ('what is your name', 'quel est votre nom')\n",
            "Ground Truth (FR): quel est votre nom\n",
            "Translated (FR): quel est votre est\n",
            "\n",
            "Original (EN): ('i want a ticket to paris', 'je veux un billet pour paris')\n",
            "Ground Truth (FR): je veux un billet pour paris\n",
            "Translated (FR): je ai besoin d aide\n",
            "\n",
            "Original (EN): ('we are collecting more data', 'nous collectons plus de donnees')\n",
            "Ground Truth (FR): nous collectons plus de donnees\n",
            "Translated (FR): nous devons le le\n",
            "\n",
            "Original (EN): ('we need more data', 'nous avons besoin de plus de donnees')\n",
            "Ground Truth (FR): nous avons besoin de plus de donnees\n",
            "Translated (FR): nous devons optimiser le code\n",
            "\n",
            "Original (EN): ('no i prefer tea', 'non je prefere le the')\n",
            "Ground Truth (FR): non je prefere le the\n",
            "Translated (FR): je pense que\n",
            "\n",
            "Original (EN): (\"no i don't understand\", 'non je ne comprends pas')\n",
            "Ground Truth (FR): non je ne comprends pas\n",
            "Translated (FR): je suis sens\n",
            "\n",
            "Original (EN): ('deep learning is interesting', 'l apprentissage profond est interessant')\n",
            "Ground Truth (FR): l apprentissage profond est interessant\n",
            "Translated (FR): l fait un\n",
            "\n",
            "Original (EN): ('good morning', 'bonjour')\n",
            "Ground Truth (FR): bonjour\n",
            "Translated (FR): bonne\n",
            "\n",
            "Original (EN): ('i am lost', 'je suis perdu')\n",
            "Ground Truth (FR): je suis perdu\n",
            "Translated (FR): je suis\n",
            "\n",
            "Original (EN): ('i hope not', 'j espere que non')\n",
            "Ground Truth (FR): j espere que non\n",
            "Translated (FR): je ai besoin d aide\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
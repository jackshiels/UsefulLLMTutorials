{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0neRFBDbmIZs"
      },
      "source": [
        "# Encoder-Decoder Models\n",
        "\n",
        "The following tutorial explores basic encoder-decoder models and walks through building one for machine translation. For more foundational understanding, see Chapter 2 of *Large Language Models: A Deep Dive*. You can find it for under $15 here: [purchase](https://link.springer.com/book/10.1007/978-3-031-65647-7)\n",
        "\n",
        "### How Encoder-Decoder Models Work\n",
        "\n",
        "Encoder-decoder models are designed to handle **variable-length inputs and outputs**, a challenge in many natural language processing tasks. Unlike standard neural networks with fixed input/output sizes (e.g., classifiers), encoder-decoder models dynamically generate sequences of arbitrary length by learning token-by-token outputs.\n",
        "\n",
        "The **encoder** transforms an input sequence into a fixed-size (or variable-length) vector representation, called a **context vector**. During training, the model learns to encode meaningful features of the input sequence into this representation.\n",
        "\n",
        "The **decoder** takes the context vector and generates the output sequence, one token at a time, typically using **autoregression** (feeding its previous output back in as input). This process relies on a hidden state that evolves as more tokens are generated.\n",
        "\n",
        "The architecture involves two sets of hidden states:\n",
        "- One in the encoder, which processes the input tokens.\n",
        "- One in the decoder, which generates output tokens based on both the encoder's context and previously generated tokens.\n",
        "\n",
        "The predictive capability of the decoder comes from probabilistically selecting the next token, based on the formula:\n",
        "\n",
        "$\\text{softmax}(s_{t-1}, y'_{t-1}, c)$\n",
        "\n",
        "Where:\n",
        "- $\\text s_{t-1}$ is the decoder's previous hidden state  \n",
        "- $\\text y'_{t-1}$ is the previously generated token  \n",
        "- $\\text c$ is the context vector from the encoder\n",
        "\n",
        "**Softmax** transforms a vector of logits (raw model scores) into probabilities that sum to 1, enabling the model to sample or choose the most likely next token.\n",
        "\n",
        "### What this Means / TL:DR\n",
        "\n",
        "TL:DR you can train a model to learn sequence probabilities such that it can output sequences that vary in length. Here are few examples of what you could do with such a network:\n",
        "\n",
        "- Translate one language to another (a very common. use case)\n",
        "- Extract data from a sentence, such as highlighting named entities (e.g., names, nouns, locations)\n",
        "- Predict time series data\n",
        "\n",
        "The crux of an ancoder decoder model is: we have a sequence, can we predict another sequence that depends on the data in this sequence?\n",
        "\n",
        "### What this Tutorial Does\n",
        "\n",
        "In this tutorial we'll be building an English to French translator model. We'll use a very small toy dataset, so don't expect accuracy. Remember to write it out yourself, line by line. We'll start by grabbing the data from huggingface.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVpDXhI7jFix"
      },
      "source": [
        "# Perform Standard Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cALWYbNEFjVV",
        "outputId": "e6880f86-46f7-4084-bf23-612e5924731a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "# Seed torch and random\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Import Hugging Face Datasets\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    HF_DATASETS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    HF_DATASETS_AVAILABLE = False\n",
        "    print(\"Hugging Face datasets library not found. Please install it: pip install datasets\")\n",
        "    print(\"Falling back to the toy dataset.\")\n",
        "\n",
        "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rq8MP7-rhsv"
      },
      "source": [
        "# Toy Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QExmKJ7yrjRq",
        "outputId": "0ea780e5-21d2-4d56-e960-31b685fe3717"
      },
      "outputs": [],
      "source": [
        "# Define the tags for our source (SRC) and target (TGT) sequences\n",
        "SRC_LANGUAGE = 'en'\n",
        "TGT_LANGUAGE = 'fr'\n",
        "\n",
        "MAX_DATASET_SIZE_TRAIN = 10000 # Keeping it small for quick demo\n",
        "MAX_DATASET_SIZE_VALID = 1000\n",
        "\n",
        "raw_data_pairs = []\n",
        "\n",
        "# Don't bother learning or recreating this function\n",
        "if HF_DATASETS_AVAILABLE:\n",
        "    try:\n",
        "        print(\"Attempting to load 'opus_books' dataset for en-fr...\")\n",
        "        hf_dataset = load_dataset(\"opus_books\", \"en-fr\", split='train')\n",
        "        print(f\"Successfully loaded 'opus_books' dataset. Original size: {len(hf_dataset)}\")\n",
        "\n",
        "        # Define the indices where we'll cut out the train and valid data\n",
        "        train_end_idx = 0\n",
        "        if MAX_DATASET_SIZE_TRAIN is not None:\n",
        "            train_end_idx = min(MAX_DATASET_SIZE_TRAIN, len(hf_dataset))\n",
        "            dataset_subset_train = hf_dataset.select(range(train_end_idx))\n",
        "        else:\n",
        "            dataset_subset_train = hf_dataset\n",
        "\n",
        "        # Validation starts where training ends\n",
        "        valid_start_idx = train_end_idx\n",
        "        valid_end_idx = valid_start_idx\n",
        "        if MAX_DATASET_SIZE_VALID is not None:\n",
        "            valid_end_idx = min(valid_start_idx + MAX_DATASET_SIZE_VALID, len(hf_dataset))\n",
        "            if valid_start_idx < valid_end_idx: # Ensure there's data left for validation\n",
        "                 dataset_subset_valid = hf_dataset.select(range(valid_start_idx, valid_end_idx))\n",
        "            else:\n",
        "                dataset_subset_valid = None\n",
        "                print(\"Not enough data for a separate validation set with current MAX_DATASET_SIZE settings.\")\n",
        "        else:\n",
        "            dataset_subset_valid = None\n",
        "\n",
        "        # Process training data\n",
        "        train_data_list = []\n",
        "        for example in dataset_subset_train:\n",
        "            src_text = example['translation'][SRC_LANGUAGE]\n",
        "            tgt_text = example['translation'][TGT_LANGUAGE]\n",
        "            if src_text and tgt_text:\n",
        "                train_data_list.append((src_text, tgt_text))\n",
        "\n",
        "        # Process validation data\n",
        "        valid_data_list = []\n",
        "        if dataset_subset_valid:\n",
        "            for example in dataset_subset_valid:\n",
        "                src_text = example['translation'][SRC_LANGUAGE]\n",
        "                tgt_text = example['translation'][TGT_LANGUAGE]\n",
        "                if src_text and tgt_text:\n",
        "                    valid_data_list.append((src_text, tgt_text))\n",
        "\n",
        "        # We apply some randomness to the data\n",
        "        random.shuffle(train_data_list)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset from Hugging Face: {e}\")\n",
        "        print(\"Falling back to the toy dataset.\")\n",
        "        HF_DATASETS_AVAILABLE = False\n",
        "\n",
        "print(f\"Total training examples: {len(train_data_list)}\")\n",
        "print(f\"Total validation examples: {len(valid_data_list)}\")\n",
        "\n",
        "if train_data_list:\n",
        "    print(\"\\nSample training data point:\")\n",
        "    print(f\"Source: {train_data_list[0][0]}\")\n",
        "    print(f\"Target: {train_data_list[0][1]}\")\n",
        "else:\n",
        "    print(\"Error: No training data available!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZiBElgSrvna"
      },
      "source": [
        "# Tokenizer\n",
        "\n",
        "What is a tokenizer? Tokenizers determine how to transform your input data into vector-ready data for the machine learning process. One way to tokenize a sentence is to simply assign unique numbers to each individual word. You could do this by counting up from 1 every time you encounter a new word and storing these values in a dictionary.\n",
        "\n",
        "Other, more complex and performant tokenizers exist. Some tokenizers will often reduce words into their 'lemmas' (base words) or 'stems' (base strings) to avoid producing overly large vocabularies. However, these tokenizer are becoming less common as models become larger.\n",
        "\n",
        "We build a simple tokenizer to introduce the concept. This tokenizer does not reduce to lemmas or stems on inputs. Instead, each unique word is given a numeric representation that counts upward as new words are added. This tokenizer will also be used to decode the model output back into french.\n",
        "\n",
        "### Token Types\n",
        "\n",
        "It is important to note *how* we deal with variable length sequences in this model. We already described how encoder-decoder models can handle variable length outputs, making them excellent for translation. However, we still need strategies around how to signal the start and end of a sentence, and how we ultimately fit these variable length sequences into the training data.\n",
        "\n",
        "We can do this with four tokens. These tokens are:\n",
        "\n",
        "- PAD: the $\\text <pad>$ token is added N times to the end of a sequence to prevent a sequence from being too short.\n",
        "- SOS: the Start of Sentence $\\text <sos>$ token signals a sequence has started.\n",
        "- EOS: the End of Sentence $\\text <eos>$ token signals a sequence has ended.\n",
        "-UNK: the unknown $\\text <unk>$ token is used for inputs that the tokenizer does not have in its vocabulary.\n",
        "\n",
        "Since our model ultimately has a maximum sequence length, we adjust the tokenizer to support a padding token, which we'll add to training data later on. For now, our tokenizer should support these four unique tokens, alongside all the token values for our training dataset.\n",
        "\n",
        "### TL:DR\n",
        "\n",
        "The tokenizer converts words to numbers, and we have some special tokens to handle padding (ensuring all inputs are the same length), start and ends of sentences (so that when we decode, we know when to stop), and an unknown token for any out-of-vocabulary inputs that are provided to our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wkhVYHUrw-6"
      },
      "outputs": [],
      "source": [
        "# Define the special tokens\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "SOS_TOKEN = \"<sos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "class CustomTokenizer:\n",
        "  def __init__(self, language_name):\n",
        "    self.language_name = language_name\n",
        "\n",
        "    # these two dictionaries are super important. We need them to get an index (token)\n",
        "    # from a word (word2index) and we need to later turn the token index back into\n",
        "    # a word again (index2word).\n",
        "    self.word2index = {}\n",
        "    self.index2word = {}\n",
        "    self.n_count = 0\n",
        "    self.word_counts = Counter()\n",
        "\n",
        "    # initialise the special tokens into the vocabulary\n",
        "    self.add_word(PAD_TOKEN)\n",
        "    self.add_word(SOS_TOKEN)\n",
        "    self.add_word(EOS_TOKEN)\n",
        "    self.add_word(UNK_TOKEN)\n",
        "\n",
        "    # add the numeric token values as class attributes for later\n",
        "    self.PAD_IDX = self.add_word(PAD_TOKEN)\n",
        "    self.SOS_IDX = self.add_word(SOS_TOKEN)\n",
        "    self.EOS_IDX = self.add_word(EOS_TOKEN)\n",
        "    self.UNK_IDX = self.add_word(UNK_TOKEN)\n",
        "\n",
        "  # a linear tokenizer (count -> index)\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_count\n",
        "      self.index2word[self.n_count] = word\n",
        "      self.n_count += 1\n",
        "    return self.word2index[word]\n",
        "\n",
        "  # clean the sentence before tokenization\n",
        "  def add_sentence(self, sentence):\n",
        "    cleaned_sentence = re.sub(r'[^a-z\\s\\']', '', sentence.lower())\n",
        "    for word in cleaned_sentence.lower().split(' '):\n",
        "      self.word_counts[word] += 1\n",
        "\n",
        "  def build_vocab(self, sentences):\n",
        "    # Build up a count for each word\n",
        "    for sentence in sentences:\n",
        "      self.add_sentence(sentence)\n",
        "\n",
        "    # Add each unique key (word) to the word2index / index2word dicts\n",
        "    for word in sorted(self.word_counts.keys()):\n",
        "      self.add_word(word)\n",
        "\n",
        "  # our outward facing methods getting both indices and sentences\n",
        "  def sentence_to_indices(self, sentence):\n",
        "    cleaned_sentence = re.sub(r'[^a-z\\s\\']', '', sentence.lower())\n",
        "    tokens = [SOS_TOKEN] + cleaned_sentence.lower().split(' ') + [EOS_TOKEN]\n",
        "    indices = [self.word2index.get(token, self.UNK_IDX) for token in tokens]\n",
        "    return indices\n",
        "\n",
        "  def indices_to_sentence(self, indices):\n",
        "    if hasattr(indices, 'tolist'):\n",
        "      indices = indices.tolist()\n",
        "    return ' '.join(self.index2word.get(index, UNK_TOKEN) for index in indices\n",
        "                    if index not in [self.SOS_IDX, self.EOS_IDX, self.PAD_IDX])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdxLLoTL8xN-"
      },
      "source": [
        "Create the tokenizers and input vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LV38c9080rP",
        "outputId": "76ab6322-4aa2-4005-f63e-123eea802add"
      },
      "outputs": [],
      "source": [
        "src_tokenizer = CustomTokenizer(SRC_LANGUAGE)\n",
        "tgt_tokenizer = CustomTokenizer(TGT_LANGUAGE)\n",
        "\n",
        "# Print a few pairs\n",
        "for pair in train_data_list[:3]:\n",
        "  print(pair[0])\n",
        "  print(pair[1])\n",
        "\n",
        "src_sentences = [pair[0] for pair in train_data_list]\n",
        "tgt_sentences = [pair[1] for pair in train_data_list]\n",
        "\n",
        "src_tokenizer.build_vocab(src_sentences)\n",
        "tgt_tokenizer.build_vocab(tgt_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjMJ73rz9aJR"
      },
      "source": [
        "Test their behaviour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkGlpW339bXF",
        "outputId": "1e34a4bd-a992-47e4-af14-527710ac3a31"
      },
      "outputs": [],
      "source": [
        "# Vocabulary\n",
        "print(\"\\nSource Vocabulary (EN):\")\n",
        "print(src_tokenizer.word2index)\n",
        "print(f\"PAD_IDX: {src_tokenizer.PAD_IDX}, SOS_IDX: {src_tokenizer.SOS_IDX},\"\n",
        "      f\"EOS_IDX: {src_tokenizer.EOS_IDX}, UNK_IDX: {src_tokenizer.UNK_IDX}\")\n",
        "\n",
        "print(f\"\\nTarget Vocabulary (FR)\")\n",
        "print(tgt_tokenizer.word2index)\n",
        "print(f\"PAD_IDX: {tgt_tokenizer.PAD_IDX}, SOS_IDX: {tgt_tokenizer.SOS_IDX}\"\n",
        "      f\"EOS_IDX: {tgt_tokenizer.EOS_IDX}, UNK_IDX: {tgt_tokenizer.UNK_IDX}\")\n",
        "\n",
        "# Test the tokenizer\n",
        "test_src_sent = \"the book\"\n",
        "test_src_indices = src_tokenizer.sentence_to_indices(test_src_sent)\n",
        "print(f\"\\n'{test_src_sent}' -> {test_src_indices}\")\n",
        "print(f\"'{test_src_indices}' -> '{src_tokenizer.indices_to_sentence(test_src_indices)}'\\n\")\n",
        "\n",
        "test_tgt_sent = \"le livre\"\n",
        "test_tgt_indices = tgt_tokenizer.sentence_to_indices(test_tgt_sent)\n",
        "print(f\"'{test_tgt_sent}' -> {test_tgt_indices}\")\n",
        "print(f\"'{test_tgt_indices}' -> '{tgt_tokenizer.indices_to_sentence(test_tgt_indices)}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MHZzvHqGAhg"
      },
      "source": [
        "### Padding\n",
        "As discussed above, we need to pad the training data to ensure each entry is of equal length. We do this with the collate_fn function, which we then apply to our data in the get_data_iterator function below that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh2Jr7IbGJlp"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, src_tokenizer, tgt_tokenizer, device):\n",
        "  src_batch, tgt_batch = [], []\n",
        "  src_lens, tgt_lens = [], []\n",
        "  for src_sample, tgt_sample in batch:\n",
        "    src_indices = src_tokenizer.sentence_to_indices(src_sample)\n",
        "    tgt_indices = tgt_tokenizer.sentence_to_indices(tgt_sample)\n",
        "\n",
        "    # take the indices of this batch and create a tensor\n",
        "    # (a matrix structure used in pytorch for training / inference)\n",
        "    src_batch.append(torch.tensor(src_indices, dtype=torch.long))\n",
        "    tgt_batch.append(torch.tensor(tgt_indices, dtype=torch.long))\n",
        "\n",
        "    src_lens.append(len(src_indices))\n",
        "    tgt_lens.append(len(tgt_indices))\n",
        "\n",
        "  # pad the tensors using a utility method from torch.nn. Note the padding_value\n",
        "  # batch_first=False is a complex topic, which we'll cover later on. Note that\n",
        "  # typically, tensor dimensions START with the batch B. Here we do not structure\n",
        "  # our tensors in this way.\n",
        "  src_padded = nn.utils.rnn.pad_sequence(src_batch, padding_value=src_tokenizer.PAD_IDX, batch_first=False)\n",
        "  tgt_padded = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=tgt_tokenizer.PAD_IDX, batch_first=False)\n",
        "  src_lens = torch.tensor(src_lens)\n",
        "  tgt_lens = torch.tensor(tgt_lens)\n",
        "\n",
        "  # return on the computing device. Tensors must generally be moved onto the hardware first\n",
        "  return src_padded.to(device), tgt_padded.to(device), src_lens.to(device), tgt_lens.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WygYgAhmajh"
      },
      "source": [
        "Create a sample dataloader. A dataloader is a function that iteratively returns the training and validation data during the training loop. Note the use of a yield pattern, which returns a padded batch of data each time the training loop will call it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEFHGnySmcO-"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "def get_data_iterator(data, src_tokenizer, tgt_tokenizer, batch_size, device, shuffle=True):\n",
        "  if shuffle:\n",
        "    data_copy = list(data)\n",
        "    random.shuffle(data_copy)\n",
        "  else:\n",
        "    data_copy = data\n",
        "\n",
        "  for i in range(0, len(data_copy), batch_size):\n",
        "    batch = data_copy[i:i+batch_size]\n",
        "    yield collate_fn(batch, src_tokenizer, tgt_tokenizer, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mECBFLTvn6KC",
        "outputId": "5d9a1f15-c719-4068-eded-61987e49d54f"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTesting data iterator:\")\n",
        "data_iter = get_data_iterator(train_data_list, src_tokenizer, tgt_tokenizer, BATCH_SIZE, device)\n",
        "for i, (src_batch, tgt_batch, src_lens, tgt_lens) in enumerate(data_iter):\n",
        "  print(f\"Batch {i+1}:\")\n",
        "  print(\"Source batch shape: \", src_batch.shape)\n",
        "  print(\"target batch shape: \", tgt_batch.shape)\n",
        "  print(\"Source lengths: \", src_lens)\n",
        "  print(\"Target lengths: \", tgt_lens)\n",
        "  print(\"Source batch (first example):\\n\", src_batch[:, 0])\n",
        "  print(\"Target batch (first example):\\n\", tgt_batch[:, 0])\n",
        "  if i == 0: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLSSlGp204K9"
      },
      "source": [
        "# Building the Model Components\n",
        "\n",
        "The following four code blocks build the components that make up this model. We start by defining a standard Encoder block. We'll use nn.GRU to perform the hidden state and context training. Gated recurrent unit (GRU) involves some fairly complex mathematics, which we'll implement manually at a later point. You can read more about it here: https://en.wikipedia.org/wiki/Gated_recurrent_unit\n",
        "\n",
        "The second component we'll write is the attention mechanism. Typically, a normal Encoder-Decoder (sometimes called Seq2Seq) model will just rely on its own context and hidden states to perform training. However, we can improve the accuracy of the model by implementing attention. Attention in this instance is the measure of how well the encoder output aligns with the decoder hidden state. The results from this measure are then added to the context vector from the encoder to feed into the decoder module. More information is provided below.\n",
        "\n",
        "Lastly, the decoder performs its sequential output with the attention-scored context from above. These three components compose the Seq2Seq class, which is the final code block. Try to pay attention to the tensor matrix shapes - how do they mutate and change as they pass through the model components. Comments indicate each mutation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VbBms5l_zYc"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "We'll write the encoder using an `nn.GRU` unit. The encoder computes hidden states over time using:\n",
        "\n",
        "- Hidden states $h_t$ are computed as $h_t = f(h_{t-1}, x_t)$.\n",
        "- The context vector $c$ is derived from the hidden states: either as the final state $h_T$, or as a function $c = m(h_1, h_2, ..., h_T)$.\n",
        "- Encoders may be bidirectional, meaning $h_t$ combines forward and backward passes: information from both past ($h_{t-1}$) and future ($h_{t+1}$).\n",
        "\n",
        "### How Does the GRU Work?\n",
        "\n",
        "The Gated Recurrent Unit (GRU) uses gating mechanisms to control how information flows through its hidden state:\n",
        "\n",
        "- **Update gate ($z_t$):** controls how much of the past state $h_{t-1}$ to retain.\n",
        "- **Reset gate ($r_t$):** determines how much of the past to forget when generating new candidate states.\n",
        "- **Candidate hidden state ($\\tilde{h}_t$):** proposed new hidden state, computed using $r_t$ and a tanh activation.\n",
        "- **Final hidden state ($h_t$):** a weighted combination of $\\tilde{h}_t$ and $h_{t-1}$, using $z_t$.\n",
        "\n",
        "Each gate computes outputs of shape $[B, H]$, taking as input the embedding $x_t$ of shape $[B, E]$ and the previous hidden state $h_{t-1}$. These mechanisms allow the model to retain or forget information dynamically over time, improving learning over long sequences.\n",
        "\n",
        "### TL;DR\n",
        "\n",
        "**TL;DR**: GRUs are a compact and efficient way to learn how input embeddings evolve into hidden representations over time. These hidden states then feed into the next stage: the attention-based decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A57ZE1x_0np"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout_p):\n",
        "    super().__init__()\n",
        "\n",
        "    # This is arbitrary, you can make it as big or small as you like (but test results)\n",
        "    self.hid_dim = hid_dim\n",
        "    # also arbitrary. 4 is a good start\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # Here is the interesting bit: input dim is the size of your vocabulary.\n",
        "    # emb_dim is the arbitrary learning layer - you can select a size and experiment.\n",
        "    # We therefore train the recurrent unit to work at a token level\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim,\n",
        "                      hid_dim,\n",
        "                      n_layers,\n",
        "                      bidirectional=True,\n",
        "                      dropout=dropout_p if n_layers > 1 else 0)\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self, src_seq):\n",
        "    embedded = self.dropout(self.embedding(src_seq))\n",
        "\n",
        "    # we get outputs, which is the hidden state at every token step, and the hidden,\n",
        "    # which is the final state of the encoder.\n",
        "    outputs, hidden = self.rnn(embedded)\n",
        "    return outputs, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZbkoyEU16bt"
      },
      "source": [
        "### Attention Module\n",
        "\n",
        "We implement an optional attention mechanism that computes attention weights based on the relationship between the current decoder state and the encoder's hidden states. These weights determine how much focus the decoder should place on each part of the input sequence at each timestep and provides contextual information about each token in relation to the others.\n",
        "\n",
        "The resulting attention context vector captures relevant information from the encoder and is combined with the decoder's input or hidden state. This enriched representation is then passed into the embedding or decoding layer, allowing the model to dynamically incorporate context from the entire input sequence during decoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_TNW8d32dmr"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "    # enc_hid_dim_effective is the final hidden layer dimension, e.g. H * 2 for bidir\n",
        "    # dec_hid_dim is the decoder hidden dimensions\n",
        "    self.attn = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)\n",
        "    self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "  def forward(self, decoder_hidden_top_layer, encoder_outputs):\n",
        "    # decoder_hidden_top_layer = [batch_size, dec_hid_dim] as we take current only\n",
        "    # encoder outputs = [src_len, batch_size, enc_hid_dim]\n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "    hidden_repeated = decoder_hidden_top_layer.unsqueeze(1).repeat(1, src_len, 1)\n",
        "    encoder_outputs_permuted = encoder_outputs.permute(1, 0, 2)\n",
        "    energy = torch.tanh(self.attn(torch.cat((hidden_repeated, encoder_outputs_permuted), dim=2)))\n",
        "    # This doesn't damage attention [batch, src_len, 1] because it just removes the dim\n",
        "    attention_scores = self.v(energy).squeeze(2)\n",
        "    return F.softmax(attention_scores, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-pCsQ_QJpb7"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder takes the context vector from the encoder and generates its own hidden states to produce the output sequence. Each decoder hidden state depends not only on the previous decoder state but also on the previously generated token and the context vector:\n",
        "\n",
        "- $s_{t'} = g(s_{t'-1}, y_{t'-1}, c)$\n",
        "- The output at timestep $t'$, $y_{t'}$, is computed as a probability distribution:  \n",
        "  $P(y_{t'} \\mid y_1, \\dots, y_{t'-1}, c) = \\text{softmax}(s_{t'-1}, y_{t'-1}, c)$\n",
        "\n",
        "This means that each predicted token is influenced by all previous tokens as well as the encoder's context, allowing the model to maintain sequential coherence.\n",
        "\n",
        "When using attention, the decoder accesses all encoder hidden states at every timestep. This enables the model to dynamically focus on relevant parts of the input sequence, providing richer contextual understanding and improving output quality during training and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K5PmNiDN6yu"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, dec_hid_dim, n_layers, dropout_p, enc_hid_dim):\n",
        "    super().__init__()\n",
        "    self.output_dim = output_dim\n",
        "    self.hid_dim = dec_hid_dim\n",
        "    self.n_layers = n_layers\n",
        "    self.enc_hid_dim = enc_hid_dim # This is HID_DIM * 2\n",
        "\n",
        "    # Components\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.attention = Attention(self.enc_hid_dim, self.hid_dim)\n",
        "    rnn_input_size = emb_dim + self.enc_hid_dim\n",
        "    self.rnn = nn.GRU(rnn_input_size,\n",
        "                          dec_hid_dim,\n",
        "                          n_layers,\n",
        "                          dropout=dropout_p if n_layers > 1 else 0)\n",
        "\n",
        "    self.fc_out = nn.Linear(dec_hid_dim + enc_hid_dim + emb_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self, input_token, hidden_state, encoder_outputs):\n",
        "      # ... your forward pass is correct and does not need to change ...\n",
        "      input_token = input_token.unsqueeze(0)\n",
        "      embedded = self.dropout(self.embedding(input_token))\n",
        "\n",
        "      decoder_top_hidden = hidden_state[-1]\n",
        "      a = self.attention(decoder_top_hidden, encoder_outputs)\n",
        "      a = a.unsqueeze(1)\n",
        "\n",
        "      encoder_outputs_permutated = encoder_outputs.permute(1, 0, 2)\n",
        "      context = torch.bmm(a, encoder_outputs_permutated)\n",
        "      context = context.permute(1, 0, 2)\n",
        "\n",
        "      # Now this concatenation correctly matches the GRU's input size\n",
        "      rnn_input = torch.cat((embedded, context), dim=2)\n",
        "      output, new_hidden_state = self.rnn(rnn_input, hidden_state)\n",
        "\n",
        "      # Recombine the inputs into the fc_out layer\n",
        "      combined = torch.cat((output.squeeze(0), context.squeeze(0), embedded.squeeze(0)), dim=1)\n",
        "      prediction = self.fc_out(combined)\n",
        "\n",
        "      return prediction, new_hidden_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY-91VeMfOcM"
      },
      "source": [
        "### Seq2Seq Implementation\n",
        "\n",
        "The `Seq2Seq` module manages the full encoder–decoder architecture. It contains:\n",
        "\n",
        "- An **encoder** that processes the input sequence.\n",
        "- A **decoder** that generates the output sequence.\n",
        "- A **bridge** (a linear layer) that transforms the bidirectional hidden states from the encoder into a format suitable for initializing the decoder.\n",
        "\n",
        "Since the encoder is bidirectional and the decoder is unidirectional, the hidden states must be reshaped and projected to match the decoder's expected dimensions. The bridge performs this transformation using concatenation and a learned linear projection.\n",
        "\n",
        "During decoding, the model generates tokens step by step. At each timestep, it predicts the next token by computing a probability distribution over the target vocabulary using a softmax layer. The predicted outputs for each timestep are collected and returned as the full output sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dy3R35PfXC_"
      },
      "outputs": [],
      "source": [
        "def resize_bidirectional_hidden(n_layers, batch_size, hid, enc_hidden, bridge):\n",
        "  enc_hidden = enc_hidden.view(n_layers, 2, batch_size, hid)\n",
        "  cat = torch.cat((enc_hidden[:, 0, :, :], enc_hidden[:, 1, :, :]), dim=2)\n",
        "  return torch.tanh(bridge(cat))\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "    enc_bridge_input_dim = self.encoder.hid_dim * 2\n",
        "    self.bridge = nn.Linear(enc_bridge_input_dim, self.decoder.hid_dim)\n",
        "\n",
        "    # Sanity check for dimensionality\n",
        "    assert encoder.hid_dim == decoder.hid_dim, \"hidden dims must be equal\"\n",
        "    assert encoder.n_layers == decoder.n_layers, \"layers must be equal\"\n",
        "\n",
        "  def _init_decoder_hidden(self, enc_hidden):\n",
        "    n_layers, batch_size, hid = self.encoder.n_layers, enc_hidden.size(1), self.encoder.hid_dim\n",
        "    return resize_bidirectional_hidden(n_layers, batch_size, hid, enc_hidden, self.bridge)\n",
        "\n",
        "  def forward(self, src_seq, tgt_seq, teacher_forcing_ratio=0.5):\n",
        "    batch_size = src_seq.shape[1]\n",
        "    # Encode\n",
        "    enc_out, hidden = self.encoder(src_seq)\n",
        "    hidden = self._init_decoder_hidden(hidden)\n",
        "\n",
        "    # Decode\n",
        "    tgt_len = tgt_seq.shape[0]\n",
        "    batch_size = tgt_seq.shape[1]\n",
        "    tgt_vocab_size = self.decoder.output_dim\n",
        "\n",
        "    outputs = torch.zeros(tgt_len, batch_size, tgt_vocab_size).to(self.device)\n",
        "\n",
        "    dec_in = tgt_seq[0, :]\n",
        "    for t in range(1, tgt_len):\n",
        "      dec_out, hidden = self.decoder(dec_in, hidden, enc_out)\n",
        "      outputs[t] = dec_out\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "      top1 = dec_out.argmax(1)\n",
        "      dec_in = tgt_seq[t] if teacher_force else top1\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QckUDH8kcOis"
      },
      "source": [
        "# Training\n",
        "\n",
        "Now, we perform the standard training process. By now, this format should be familiar to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cquq3cUbcP-M"
      },
      "outputs": [],
      "source": [
        "# hyperparams\n",
        "INPUT_DIM = src_tokenizer.n_count\n",
        "OUTPUT_DIM = tgt_tokenizer.n_count\n",
        "ENC_EMB_DIM = 128\n",
        "DEC_EMB_DIM = 128\n",
        "HID_DIM = 128\n",
        "EFFECTIVE_ENC_HID_DIM = HID_DIM * 2\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "LEARNING_RATE = 0.001\n",
        "N_EPOCHS = 5\n",
        "CLIP = 1\n",
        "BIDIR_MODEL_NAME = \"language_enc_dec_bidir_attn.pt\"\n",
        "UNIDIR_MODEL_NAME = \"language_enc_dec.pt\"\n",
        "MODEL_NAME = BIDIR_MODEL_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDs8YgZlweKi"
      },
      "outputs": [],
      "source": [
        "# components\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, EFFECTIVE_ENC_HID_DIM).to(device)\n",
        "model_bidir = Seq2Seq(enc, dec, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwoiHd_0w1si",
        "outputId": "91356e22-2cca-4e53-c4fd-d8716ccb8824"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model_bidir):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmF7TUbzxEcT"
      },
      "outputs": [],
      "source": [
        "# optim and learn\n",
        "optimizer = optim.Adam(model_bidir.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = tgt_tokenizer.PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brGpMZ6AxQ22"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRLTMnN8yG2p"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    batch_n = 0\n",
        "    for i, (src, tgt, _, _) in enumerate(iterator): # src_lens, tgt_lens not used directly here\n",
        "        batch_n += 1\n",
        "        optimizer.zero_grad()\n",
        "        # output = [tgt_len, batch_size, output_vocab_size]\n",
        "        output = model(src, tgt)\n",
        "        # get vocab length for next step\n",
        "        output_dim = output.shape[-1]\n",
        "        # remove <sos> tag by enforcing [1:]\n",
        "        # turn [tgt_len, batch, vocab] into [(tgt_len-1 * batch), vocab] so it fits into loss\n",
        "        output_flat = output[1:].view(-1, output_dim)\n",
        "        # since we know the vocab, this doesn't have the V dimension [tgt_len-1, batch]\n",
        "        tgt_flat = tgt[1:].view(-1)\n",
        "        # now that they are equal dim, compute the loss between out and tgt\n",
        "        loss = criterion(output_flat, tgt_flat)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / batch_n\n",
        "\n",
        "def evaluate_epoch(model, iterator, criterion):\n",
        "  # the same setup, but we loop without adam use\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  batch_n = 0\n",
        "  with torch.no_grad():\n",
        "    for i, (src, tgt, _, _) in enumerate(iterator):\n",
        "      batch_n += 1\n",
        "      output = model(src, tgt, 0)\n",
        "      output_dim = output.shape[-1]\n",
        "      output_flat = output[1:].view(-1, output_dim)\n",
        "      tgt_flat = tgt[1:].view(-1)\n",
        "      loss = criterion(output_flat, tgt_flat)\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss / batch_n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDWtJKaM2hfr"
      },
      "source": [
        "### Train Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8PSyI3EJ2jNs",
        "outputId": "325d648c-2194-4f7d-9345-95a5b2857acd"
      },
      "outputs": [],
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start_time = time.time()\n",
        "  train_iter = get_data_iterator(train_data_list,\n",
        "                               src_tokenizer,\n",
        "                               tgt_tokenizer,\n",
        "                               BATCH_SIZE,\n",
        "                               device,\n",
        "                               shuffle=True)\n",
        "  valid_iter = get_data_iterator(valid_data_list,\n",
        "                               src_tokenizer,\n",
        "                               tgt_tokenizer,\n",
        "                               BATCH_SIZE,\n",
        "                               device,\n",
        "                               shuffle=False)\n",
        "  train_loss = train_epoch(model_bidir, train_iter, optimizer, criterion, CLIP)\n",
        "  valid_loss = evaluate_epoch(model_bidir, valid_iter, criterion)\n",
        "\n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "torch.save(model_bidir.state_dict(), MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inpbzvO6-Qzs"
      },
      "source": [
        "### Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V4cY_4e4Izk"
      },
      "outputs": [],
      "source": [
        "model_bidir.load_state_dict(torch.load(MODEL_NAME, weights_only=True))\n",
        "model_bidir.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvbxTxaHDb_r"
      },
      "source": [
        "### Beam Search Decoding\n",
        "\n",
        "The `beam_search_translate` function implements beam search for sequence generation in a trained Seq2Seq model. It performs the following steps:\n",
        "\n",
        "- **Tokenization and Encoding**: The input sentence is tokenized and encoded using the source tokenizer and encoder. The encoder outputs and hidden state are used to initialize the decoder.\n",
        "\n",
        "- **Beam Initialization**: A beam is initialized with the start-of-sequence (`<sos>`) token and the decoder's initial hidden state. The algorithm maintains two lists: `active_beams` (currently growing sequences) and `completed` (finished sequences ending in `<eos>`).\n",
        "\n",
        "- **Step-by-Step Decoding**: At each timestep:\n",
        "  - Active beams are expanded by generating the log-probabilities of the next token.\n",
        "  - The top `k` candidates (beam width) are selected based on their cumulative scores.\n",
        "  - Early `<eos>` tokens are discouraged by masking them out until a minimum number of decoding steps.\n",
        "\n",
        "- **Length Penalty**: During beam ranking, a length normalization penalty is applied to favor more fluent outputs without disproportionately penalizing longer sequences.\n",
        "\n",
        "- **Completion and Output**: Once the search terminates (via `<eos>` or `max_len`), the completed sequences are scored and the best one is selected. Start (`<sos>`) and end (`<eos>`) tokens are removed, and the remaining token indices are converted back into a sentence using the target tokenizer.\n",
        "\n",
        "Beam search improves decoding quality over greedy approaches by exploring multiple hypotheses at each timestep and selecting the best overall sequence based on both likelihood and sequence length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47pMjo7a8mAD"
      },
      "outputs": [],
      "source": [
        "class BeamHypothesis:\n",
        "  def __init__(self, tokens, log_prob, hidden_state):\n",
        "    self.tokens = tokens\n",
        "    self.log_prob = log_prob\n",
        "    self.hidden_state = hidden_state\n",
        "\n",
        "  def extend(self, token_idx, log_prob_token, new_hidden_state):\n",
        "    return BeamHypothesis(\n",
        "        self.tokens + [token_idx],\n",
        "        self.log_prob + log_prob_token,\n",
        "        new_hidden_state\n",
        "    )\n",
        "\n",
        "  @property\n",
        "  def latest_token(self):\n",
        "    return self.tokens[-1]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens)\n",
        "\n",
        "  def __lt__(self, other):\n",
        "    return self.log_prob < other.log_prob\n",
        "\n",
        "  def normalized_score(self, alpha=0.3):\n",
        "    length = len(self.tokens)\n",
        "    lp = ((5 + length) ** alpha) / ((5 + 1) ** alpha)\n",
        "    return self.log_prob / lp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xko9aBRK_tH"
      },
      "source": [
        "Build the search function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpzalzzeLC67"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def beam_search_translate(sentence,\n",
        "                          src_tokenizer,\n",
        "                          tgt_tokenizer,\n",
        "                          model,\n",
        "                          device,\n",
        "                          beam_width=3,\n",
        "                          max_len=50,\n",
        "                          len_penalty_alpha=0.6,\n",
        "                          min_target_len=4):\n",
        "    model.eval()\n",
        "\n",
        "    # 1) Tokenize & batchify\n",
        "    if isinstance(sentence, str):\n",
        "        src_indices = src_tokenizer.sentence_to_indices(sentence)\n",
        "    else:\n",
        "        src_indices = sentence\n",
        "    src_tensor = torch.tensor(src_indices, dtype=torch.long) \\\n",
        "                     .unsqueeze(1).to(device)  # [src_len, 1]\n",
        "\n",
        "    # 2) Encode\n",
        "    with torch.no_grad():\n",
        "        enc_outputs, hidden = model.encoder(src_tensor)\n",
        "        decoder_hidden = model._init_decoder_hidden(hidden)\n",
        "\n",
        "    # 3) Initialize beams\n",
        "    initial_beam = BeamHypothesis(\n",
        "        tokens=[tgt_tokenizer.SOS_IDX],\n",
        "        log_prob=0.0,\n",
        "        hidden_state=decoder_hidden\n",
        "    )\n",
        "    active_beams   = [initial_beam]\n",
        "    completed      = []\n",
        "\n",
        "    def clone_hidden(h):\n",
        "        if isinstance(h, tuple):            # LSTM case\n",
        "            return tuple(t.clone() for t in h)\n",
        "        return h.clone()                    # GRU case\n",
        "\n",
        "    # 4) Beam search\n",
        "    for step in range(max_len):\n",
        "        if not active_beams:\n",
        "            break\n",
        "\n",
        "        candidates = []\n",
        "        next_beams = []\n",
        "\n",
        "        # move any EOS-ended beams to completed\n",
        "        for beam in active_beams:\n",
        "            if beam.latest_token == tgt_tokenizer.EOS_IDX:\n",
        "                completed.append(beam)\n",
        "            else:\n",
        "                next_beams.append(beam)\n",
        "        active_beams = next_beams\n",
        "        if not active_beams:\n",
        "            break\n",
        "\n",
        "        # expand each active beam\n",
        "        for beam in active_beams:\n",
        "            inp = torch.tensor([beam.latest_token], dtype=torch.long).to(device)  # [batch=1]\n",
        "            with torch.no_grad():\n",
        "                output, new_hidden = model.decoder(inp, beam.hidden_state, enc_outputs)\n",
        "            # output: [batch=1, vocab_size]\n",
        "            logits    = output.squeeze(0)                     # → [vocab_size]\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            # forbid <eos> too early\n",
        "            if step < min_target_len:\n",
        "                log_probs[tgt_tokenizer.EOS_IDX] = -float('inf')\n",
        "\n",
        "            topk_lp, topk_idx = torch.topk(log_probs, beam_width)\n",
        "            print([\n",
        "                (tgt_tokenizer.index2word[i.item()], round(p.item(), 4))\n",
        "                for i, p in zip(topk_idx, topk_lp)\n",
        "            ])\n",
        "            for lp, idx in zip(topk_lp.tolist(), topk_idx.tolist()):\n",
        "                # **positional args** to match your extend() signature:\n",
        "                new_beam = beam.extend(idx, lp, clone_hidden(new_hidden))\n",
        "                candidates.append(new_beam)\n",
        "\n",
        "        if not candidates:\n",
        "            break\n",
        "\n",
        "        # keep best K hypotheses\n",
        "        active_beams = sorted(\n",
        "            candidates,\n",
        "            key=lambda b: b.normalized_score(len_penalty_alpha),\n",
        "            reverse=True\n",
        "        )[:beam_width]\n",
        "\n",
        "    # gather finals\n",
        "    completed.extend(active_beams)\n",
        "    if not completed:\n",
        "        return \"error\"\n",
        "\n",
        "    # pick best\n",
        "    best = max(\n",
        "        completed,\n",
        "        key=lambda b: b.normalized_score(len_penalty_alpha)\n",
        "    )\n",
        "\n",
        "    # strip SOS/EOS\n",
        "    toks = best.tokens\n",
        "    if toks and toks[0] == tgt_tokenizer.SOS_IDX:\n",
        "        toks = toks[1:]\n",
        "    if toks and toks[-1] == tgt_tokenizer.EOS_IDX:\n",
        "        toks = toks[:-1]\n",
        "\n",
        "    return tgt_tokenizer.indices_to_sentence(toks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv-7s6AO0lC-"
      },
      "source": [
        "### BLEU Score Overview\n",
        "\n",
        "This implementation calculates the BLEU score, which measures how closely a generated sentence matches one or more reference sentences. It's not necessary to implement this yourself, as there are libraries to do so. However, I kept it here for interest's sake.\n",
        "\n",
        "#### Core Concepts\n",
        "\n",
        "- **N-grams**: Sub-sequences of `n` words used to compare local word patterns.\n",
        "- **Modified Precision**: Counts matching n-grams between candidate and references, clipped to avoid overcounting.\n",
        "- **Brevity Penalty**: Penalizes candidates that are too short compared to references.\n",
        "- **Geometric Mean**: Combines n-gram precisions into a single score.\n",
        "\n",
        "#### BLEU Function\n",
        "\n",
        "The `bleu()` function:\n",
        "1. Computes modified precision for 1- to 4-grams.\n",
        "2. Applies smoothing if no matches exist.\n",
        "3. Calculates the geometric mean of precisions.\n",
        "4. Multiplies by the brevity penalty.\n",
        "\n",
        "Returns a score between 0 (no match) and 1 (perfect match)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZkPxmbxwQSq"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "\n",
        "def ngrams(seq: List[str], n: int) -> List[tuple[str, ...]]:\n",
        "  return [tuple(seq[i:i+n]) for i in range(len(seq) - n + 1)]\n",
        "\n",
        "def modified_precision(candidate: List[str],\n",
        "                       references: List[List[str]],\n",
        "                       n: int):\n",
        "  cand_ngrams = Counter(ngrams(candidate, n))\n",
        "  max_reference_counts = Counter()\n",
        "\n",
        "  for ref in references:\n",
        "    ref_counts = Counter(ngrams(ref, n))\n",
        "    for ngram, count in ref_counts.items():\n",
        "      max_reference_counts[ngram] = max(max_reference_counts[ngram], count)\n",
        "\n",
        "  clipped_counts = {ngram: min(count, max_reference_counts[ngram])\n",
        "    for ngram, count in cand_ngrams.items()}\n",
        "\n",
        "  numerator = sum(clipped_counts.values())\n",
        "  denominator = sum(cand_ngrams.values())\n",
        "\n",
        "  return numerator, denominator\n",
        "\n",
        "def brevity_penalty(c: int, r: int) -> float:\n",
        "  return 1.0 if c > r else math.exp(1 - r / c)\n",
        "\n",
        "def closest_ref_len(c: int, ref_lens: List[int]) -> int:\n",
        "  return min(ref_lens, key=lambda rl: (abs(rl - c), rl))\n",
        "\n",
        "def bleu(candidate: List[str],\n",
        "         references: List[List[str]],\n",
        "         max_n: int = 4) -> float:\n",
        "  weights = [1/max_n] * max_n\n",
        "  precisions = []\n",
        "\n",
        "  for n in range(1, max_n+1):\n",
        "    num, den = modified_precision(candidate, references, n)\n",
        "    if num == 0:\n",
        "      num, den = 1, 2\n",
        "    precisions.append((num, den))\n",
        "\n",
        "  geo_mean = math.exp(sum(w * math.log(num/den)\n",
        "    for (num, den), w in zip(precisions, weights)))\n",
        "\n",
        "  c = len(candidate)\n",
        "  r = closest_ref_len(c, [len(r) for r in references])\n",
        "  bp = brevity_penalty(c, r)\n",
        "  return bp * geo_mean\n",
        "\n",
        "def tokenize_sequence(sequence):\n",
        "  return sequence.lower().split(' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P_nJ3tO1aYW"
      },
      "source": [
        "# Last Step: Run It!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLl4yNhd9XzS"
      },
      "outputs": [],
      "source": [
        "# get BLEU candidates / references\n",
        "references_bidir = [tokenize_sequence(text_pair[1]) for text_pair in valid_data_list]\n",
        "candidates_bidir = [tokenize_sequence(beam_search_translate(text_pair[0], src_tokenizer, tgt_tokenizer, model_bidir, device, beam_width=5, max_len=100)) for text_pair in valid_data_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOna1djQDWzd"
      },
      "outputs": [],
      "source": [
        "bleu_results = []\n",
        "for candidate in candidates_bidir:\n",
        "  bleu_results.append(bleu(candidate, references_bidir))\n",
        "\n",
        "print(bleu_results)\n",
        "sum_of_bleu = sum(bleu_results)\n",
        "len_of_bleu = len(bleu_results)\n",
        "print(f'The average bidir BLEU is: {sum_of_bleu / len_of_bleu}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QaeIMp03wPfe"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Sample Translations from Validation Set ---\")\n",
        "for i, (src_text, tgt_text) in enumerate(valid_data_list[:10]): # Show first 10\n",
        "    translation = beam_search_translate(src_text, src_tokenizer, tgt_tokenizer, model_bidir, device)\n",
        "    print(f\"Original (EN):     {src_text}\")\n",
        "    print(f\"Ground Truth (FR): {tgt_text}\")\n",
        "    print(f\"Translated (FR):   {translation}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.12.0",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
